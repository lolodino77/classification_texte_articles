    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list. ( _ **Update
2022-11-26:**_ I’ve added the actual distribution of score configurations for
the last 3 world cups. Would be interesting if someone could do it for the
entire history of the world cup.)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

Score configuration | No. of permutations | WC 2018 | WC 2014 | WC 2010  
---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  
7, 5, 4, 0 | 24 (3.3%) |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  
6, 6, 4, 1 | 24 (3.3%) |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  
5, 5, 4, 1 | 24 (3.3%) | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  
  
The code I used to produce the table above is below:

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-in-optimization%2F&signup_flow=account)

Reblogged this on [Statistical Methods, Paradigms and
Application](https://ibiloyeacresearch.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/) and commented:  
Interesting and Really good explanation here:

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [secant
equation](https://statisticaloddsandends.wordpress.com/tag/secant-equation/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-in-optimization/ "Permalink to Quasi-Newton methods
in optimization").

Quasi-Newton methods include SR1, DFP, BFGS and L-BFGS, which I hope to
describe in future posts.

![\\begin{aligned} \\nabla^2 f\(x_k\) p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1}
&= x_k + t_k p_k.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cnabla%5E2+f%28x_k%29+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
according to some procedure so that the secant equation (and other conditions)
are satisfied.

That is, we (i) compute the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
at
![x_k](https://s0.wp.com/latex.php?latex=x_k&bg=ffffff&fg=333333&s=0&c=20201002),
(ii) solve for
![p_k](https://s0.wp.com/latex.php?latex=p_k&bg=ffffff&fg=333333&s=0&c=20201002),
then (iii) update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
according to the second line.

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This last equation is known as the _**secant equation**_ , and is sometimes
written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Note that this equation involves
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations in ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, so there is a lot of freedom in how to choose the
![B_k](https://s0.wp.com/latex.php?latex=B_k&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
On top of satisfying the secant equation, we often require 3 other conditions:

A big drawback of Newton’s method is that computing the Hessian can be very
expensive. _**Quasi-Newton methods**_ address this problem: update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
with

![\\begin{aligned} B_k p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1} &= x_k + t_k
p_k,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_k+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

where ![f : \\mathbb{R}^n \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f+%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is twice differentiable and ![\\text{dom}\(f\) =
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Ctext%7Bdom%7D%28f%29+%3D+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**Newton’s
method**_](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is
a second-order descent method for finding the minimum. Starting at some
initial point, at the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration we update the candidate solution with the formula

where ![\\nabla
f](https://s0.wp.com/latex.php?latex=%5Cnabla+f&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\\nabla^2
f](https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+f&bg=ffffff&fg=333333&s=0&c=20201002)
are the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
respectively, and
![t_k](https://s0.wp.com/latex.php?latex=t_k&bg=ffffff&fg=333333&s=0&c=20201002)
is a step size chosen appropriately (e.g. with [backtracking line
search](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-line-
search-and-backtracking-line-search/)). Another way to write the update is

This is known as the _**SR1 update**_. SR1 is simple to understand and
implement, but has two shortcomings. First, the denominator in the update for
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
![\(y - Bs\)^\\top
s](https://s0.wp.com/latex.php?latex=%28y+-+Bs%29%5E%5Ctop+s&bg=ffffff&fg=333333&s=0&c=20201002),
might be approximately zero, causing numerical issues. Second, it does not
preserve positive definiteness.

which implies that
![u](https://s0.wp.com/latex.php?latex=u&bg=ffffff&fg=333333&s=0&c=20201002)
must be a multiple of ![y -
Bs](https://s0.wp.com/latex.php?latex=y+-+Bs&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this into the equation and solving for
![a](https://s0.wp.com/latex.php?latex=a&bg=ffffff&fg=333333&s=0&c=20201002),
we obtain the update

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [SR1](https://statisticaloddsandends.wordpress.com/tag/sr1/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-sr1/ "Permalink to Quasi-Newton methods: SR1").

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Because this is a system of
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations with ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, there are many ways to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-sr1%2F&signup_flow=account)

One way to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
easily is to assume that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix, i.e.

Recall from the quasi-Newton updates that we need to solve the equation ![B p
= - \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=B+p+%3D+-+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002)
for
![p](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=333333&s=0&c=20201002),
which means that we need to compute the inverse ![H =
B^{-1}](https://s0.wp.com/latex.php?latex=H+%3D+B%5E%7B-1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to get ![p = - H \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=p+%3D+-+H+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002).
Because of the [Sherman-Morrison-Woodbury
formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we
can use the update to
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
to derive an update for
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002):

for some ![a \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![u \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this update into the secant equation and rearranging, we get

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list. ( _ **Update
2022-11-26:**_ I’ve added the actual distribution of score configurations for
the last 3 world cups. Would be interesting if someone could do it for the
entire history of the world cup.)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

Score configuration | No. of permutations | WC 2018 | WC 2014 | WC 2010  
---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  
7, 5, 4, 0 | 24 (3.3%) |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  
6, 6, 4, 1 | 24 (3.3%) |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  
5, 5, 4, 1 | 24 (3.3%) | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  
  
The code I used to produce the table above is below:

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

where ![\\nabla
f](https://s0.wp.com/latex.php?latex=%5Cnabla+f&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\\nabla^2
f](https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+f&bg=ffffff&fg=333333&s=0&c=20201002)
are the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
respectively, and
![t_k](https://s0.wp.com/latex.php?latex=t_k&bg=ffffff&fg=333333&s=0&c=20201002)
is a step size chosen appropriately (e.g. with [backtracking line
search](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-line-
search-and-backtracking-line-search/)). Another way to write the update is

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [secant
equation](https://statisticaloddsandends.wordpress.com/tag/secant-equation/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-in-optimization/ "Permalink to Quasi-Newton methods
in optimization").

![\\begin{aligned} B_k p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1} &= x_k + t_k
p_k,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_k+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

A big drawback of Newton’s method is that computing the Hessian can be very
expensive. _**Quasi-Newton methods**_ address this problem: update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
with

That is, we (i) compute the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
at
![x_k](https://s0.wp.com/latex.php?latex=x_k&bg=ffffff&fg=333333&s=0&c=20201002),
(ii) solve for
![p_k](https://s0.wp.com/latex.php?latex=p_k&bg=ffffff&fg=333333&s=0&c=20201002),
then (iii) update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
according to the second line.

where ![f : \\mathbb{R}^n \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f+%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is twice differentiable and ![\\text{dom}\(f\) =
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Ctext%7Bdom%7D%28f%29+%3D+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**Newton’s
method**_](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is
a second-order descent method for finding the minimum. Starting at some
initial point, at the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration we update the candidate solution with the formula

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This last equation is known as the _**secant equation**_ , and is sometimes
written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Note that this equation involves
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations in ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, so there is a lot of freedom in how to choose the
![B_k](https://s0.wp.com/latex.php?latex=B_k&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
On top of satisfying the secant equation, we often require 3 other conditions:

![\\begin{aligned} \\nabla^2 f\(x_k\) p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1}
&= x_k + t_k p_k.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cnabla%5E2+f%28x_k%29+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Reblogged this on [Statistical Methods, Paradigms and
Application](https://ibiloyeacresearch.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/) and commented:  
Interesting and Really good explanation here:

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-in-optimization%2F&signup_flow=account)

Quasi-Newton methods include SR1, DFP, BFGS and L-BFGS, which I hope to
describe in future posts.

Update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
according to some procedure so that the secant equation (and other conditions)
are satisfied.

for some ![a \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![u \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this update into the secant equation and rearranging, we get

This is known as the _**SR1 update**_. SR1 is simple to understand and
implement, but has two shortcomings. First, the denominator in the update for
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
![\(y - Bs\)^\\top
s](https://s0.wp.com/latex.php?latex=%28y+-+Bs%29%5E%5Ctop+s&bg=ffffff&fg=333333&s=0&c=20201002),
might be approximately zero, causing numerical issues. Second, it does not
preserve positive definiteness.

which implies that
![u](https://s0.wp.com/latex.php?latex=u&bg=ffffff&fg=333333&s=0&c=20201002)
must be a multiple of ![y -
Bs](https://s0.wp.com/latex.php?latex=y+-+Bs&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this into the equation and solving for
![a](https://s0.wp.com/latex.php?latex=a&bg=ffffff&fg=333333&s=0&c=20201002),
we obtain the update

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Because this is a system of
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations with ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, there are many ways to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).

Recall from the quasi-Newton updates that we need to solve the equation ![B p
= - \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=B+p+%3D+-+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002)
for
![p](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=333333&s=0&c=20201002),
which means that we need to compute the inverse ![H =
B^{-1}](https://s0.wp.com/latex.php?latex=H+%3D+B%5E%7B-1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to get ![p = - H \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=p+%3D+-+H+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002).
Because of the [Sherman-Morrison-Woodbury
formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we
can use the update to
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
to derive an update for
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002):

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-sr1%2F&signup_flow=account)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [SR1](https://statisticaloddsandends.wordpress.com/tag/sr1/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-sr1/ "Permalink to Quasi-Newton methods: SR1").

One way to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
easily is to assume that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix, i.e.

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

Score configuration | No. of permutations | WC 2018 | WC 2014 | WC 2010  
---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  
7, 5, 4, 0 | 24 (3.3%) |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  
6, 6, 4, 1 | 24 (3.3%) |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  
5, 5, 4, 1 | 24 (3.3%) | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  
  
The code I used to produce the table above is below:

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list. ( _ **Update
2022-11-26:**_ I’ve added the actual distribution of score configurations for
the last 3 world cups. Would be interesting if someone could do it for the
entire history of the world cup.)

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

Update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
according to some procedure so that the secant equation (and other conditions)
are satisfied.

Reblogged this on [Statistical Methods, Paradigms and
Application](https://ibiloyeacresearch.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/) and commented:  
Interesting and Really good explanation here:

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-in-optimization%2F&signup_flow=account)

That is, we (i) compute the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
at
![x_k](https://s0.wp.com/latex.php?latex=x_k&bg=ffffff&fg=333333&s=0&c=20201002),
(ii) solve for
![p_k](https://s0.wp.com/latex.php?latex=p_k&bg=ffffff&fg=333333&s=0&c=20201002),
then (iii) update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
according to the second line.

![\\begin{aligned} \\nabla^2 f\(x_k\) p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1}
&= x_k + t_k p_k.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cnabla%5E2+f%28x_k%29+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [secant
equation](https://statisticaloddsandends.wordpress.com/tag/secant-equation/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-in-optimization/ "Permalink to Quasi-Newton methods
in optimization").

where ![f : \\mathbb{R}^n \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f+%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is twice differentiable and ![\\text{dom}\(f\) =
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Ctext%7Bdom%7D%28f%29+%3D+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**Newton’s
method**_](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is
a second-order descent method for finding the minimum. Starting at some
initial point, at the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration we update the candidate solution with the formula

A big drawback of Newton’s method is that computing the Hessian can be very
expensive. _**Quasi-Newton methods**_ address this problem: update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
with

Quasi-Newton methods include SR1, DFP, BFGS and L-BFGS, which I hope to
describe in future posts.

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This last equation is known as the _**secant equation**_ , and is sometimes
written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Note that this equation involves
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations in ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, so there is a lot of freedom in how to choose the
![B_k](https://s0.wp.com/latex.php?latex=B_k&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
On top of satisfying the secant equation, we often require 3 other conditions:

where ![\\nabla
f](https://s0.wp.com/latex.php?latex=%5Cnabla+f&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\\nabla^2
f](https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+f&bg=ffffff&fg=333333&s=0&c=20201002)
are the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
respectively, and
![t_k](https://s0.wp.com/latex.php?latex=t_k&bg=ffffff&fg=333333&s=0&c=20201002)
is a step size chosen appropriately (e.g. with [backtracking line
search](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-line-
search-and-backtracking-line-search/)). Another way to write the update is

![\\begin{aligned} B_k p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1} &= x_k + t_k
p_k,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_k+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

which implies that
![u](https://s0.wp.com/latex.php?latex=u&bg=ffffff&fg=333333&s=0&c=20201002)
must be a multiple of ![y -
Bs](https://s0.wp.com/latex.php?latex=y+-+Bs&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this into the equation and solving for
![a](https://s0.wp.com/latex.php?latex=a&bg=ffffff&fg=333333&s=0&c=20201002),
we obtain the update

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [SR1](https://statisticaloddsandends.wordpress.com/tag/sr1/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-sr1/ "Permalink to Quasi-Newton methods: SR1").

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Because this is a system of
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations with ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, there are many ways to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).

for some ![a \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![u \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this update into the secant equation and rearranging, we get

Recall from the quasi-Newton updates that we need to solve the equation ![B p
= - \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=B+p+%3D+-+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002)
for
![p](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=333333&s=0&c=20201002),
which means that we need to compute the inverse ![H =
B^{-1}](https://s0.wp.com/latex.php?latex=H+%3D+B%5E%7B-1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to get ![p = - H \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=p+%3D+-+H+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002).
Because of the [Sherman-Morrison-Woodbury
formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we
can use the update to
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
to derive an update for
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002):

This is known as the _**SR1 update**_. SR1 is simple to understand and
implement, but has two shortcomings. First, the denominator in the update for
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
![\(y - Bs\)^\\top
s](https://s0.wp.com/latex.php?latex=%28y+-+Bs%29%5E%5Ctop+s&bg=ffffff&fg=333333&s=0&c=20201002),
might be approximately zero, causing numerical issues. Second, it does not
preserve positive definiteness.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-sr1%2F&signup_flow=account)

One way to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
easily is to assume that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix, i.e.

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
according to some procedure so that the secant equation (and other conditions)
are satisfied.

![\\begin{aligned} \\nabla^2 f\(x_k\) p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1}
&= x_k + t_k p_k.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cnabla%5E2+f%28x_k%29+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [secant
equation](https://statisticaloddsandends.wordpress.com/tag/secant-equation/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-in-optimization/ "Permalink to Quasi-Newton methods
in optimization").

That is, we (i) compute the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
at
![x_k](https://s0.wp.com/latex.php?latex=x_k&bg=ffffff&fg=333333&s=0&c=20201002),
(ii) solve for
![p_k](https://s0.wp.com/latex.php?latex=p_k&bg=ffffff&fg=333333&s=0&c=20201002),
then (iii) update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
according to the second line.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-in-optimization%2F&signup_flow=account)

where ![\\nabla
f](https://s0.wp.com/latex.php?latex=%5Cnabla+f&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\\nabla^2
f](https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+f&bg=ffffff&fg=333333&s=0&c=20201002)
are the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
respectively, and
![t_k](https://s0.wp.com/latex.php?latex=t_k&bg=ffffff&fg=333333&s=0&c=20201002)
is a step size chosen appropriately (e.g. with [backtracking line
search](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-line-
search-and-backtracking-line-search/)). Another way to write the update is

where ![f : \\mathbb{R}^n \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f+%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is twice differentiable and ![\\text{dom}\(f\) =
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Ctext%7Bdom%7D%28f%29+%3D+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**Newton’s
method**_](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is
a second-order descent method for finding the minimum. Starting at some
initial point, at the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration we update the candidate solution with the formula

A big drawback of Newton’s method is that computing the Hessian can be very
expensive. _**Quasi-Newton methods**_ address this problem: update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
with

Reblogged this on [Statistical Methods, Paradigms and
Application](https://ibiloyeacresearch.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/) and commented:  
Interesting and Really good explanation here:

![\\begin{aligned} B_k p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1} &= x_k + t_k
p_k,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_k+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This last equation is known as the _**secant equation**_ , and is sometimes
written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Note that this equation involves
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations in ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, so there is a lot of freedom in how to choose the
![B_k](https://s0.wp.com/latex.php?latex=B_k&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
On top of satisfying the secant equation, we often require 3 other conditions:

Quasi-Newton methods include SR1, DFP, BFGS and L-BFGS, which I hope to
describe in future posts.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-sr1%2F&signup_flow=account)

for some ![a \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![u \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this update into the secant equation and rearranging, we get

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [SR1](https://statisticaloddsandends.wordpress.com/tag/sr1/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-sr1/ "Permalink to Quasi-Newton methods: SR1").

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Because this is a system of
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations with ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, there are many ways to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).

Recall from the quasi-Newton updates that we need to solve the equation ![B p
= - \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=B+p+%3D+-+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002)
for
![p](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=333333&s=0&c=20201002),
which means that we need to compute the inverse ![H =
B^{-1}](https://s0.wp.com/latex.php?latex=H+%3D+B%5E%7B-1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to get ![p = - H \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=p+%3D+-+H+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002).
Because of the [Sherman-Morrison-Woodbury
formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we
can use the update to
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
to derive an update for
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002):

This is known as the _**SR1 update**_. SR1 is simple to understand and
implement, but has two shortcomings. First, the denominator in the update for
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
![\(y - Bs\)^\\top
s](https://s0.wp.com/latex.php?latex=%28y+-+Bs%29%5E%5Ctop+s&bg=ffffff&fg=333333&s=0&c=20201002),
might be approximately zero, causing numerical issues. Second, it does not
preserve positive definiteness.

which implies that
![u](https://s0.wp.com/latex.php?latex=u&bg=ffffff&fg=333333&s=0&c=20201002)
must be a multiple of ![y -
Bs](https://s0.wp.com/latex.php?latex=y+-+Bs&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this into the equation and solving for
![a](https://s0.wp.com/latex.php?latex=a&bg=ffffff&fg=333333&s=0&c=20201002),
we obtain the update

One way to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
easily is to assume that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix, i.e.

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-in-optimization%2F&signup_flow=account)

![\\begin{aligned} \\nabla^2 f\(x_k\) p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1}
&= x_k + t_k p_k.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cnabla%5E2+f%28x_k%29+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Quasi-Newton methods include SR1, DFP, BFGS and L-BFGS, which I hope to
describe in future posts.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [secant
equation](https://statisticaloddsandends.wordpress.com/tag/secant-equation/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-in-optimization/ "Permalink to Quasi-Newton methods
in optimization").

where ![f : \\mathbb{R}^n \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f+%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is twice differentiable and ![\\text{dom}\(f\) =
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Ctext%7Bdom%7D%28f%29+%3D+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**Newton’s
method**_](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is
a second-order descent method for finding the minimum. Starting at some
initial point, at the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration we update the candidate solution with the formula

Update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
according to some procedure so that the secant equation (and other conditions)
are satisfied.

![\\begin{aligned} B_k p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1} &= x_k + t_k
p_k,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_k+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

That is, we (i) compute the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
at
![x_k](https://s0.wp.com/latex.php?latex=x_k&bg=ffffff&fg=333333&s=0&c=20201002),
(ii) solve for
![p_k](https://s0.wp.com/latex.php?latex=p_k&bg=ffffff&fg=333333&s=0&c=20201002),
then (iii) update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
according to the second line.

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This last equation is known as the _**secant equation**_ , and is sometimes
written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Note that this equation involves
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations in ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, so there is a lot of freedom in how to choose the
![B_k](https://s0.wp.com/latex.php?latex=B_k&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
On top of satisfying the secant equation, we often require 3 other conditions:

Reblogged this on [Statistical Methods, Paradigms and
Application](https://ibiloyeacresearch.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/) and commented:  
Interesting and Really good explanation here:

where ![\\nabla
f](https://s0.wp.com/latex.php?latex=%5Cnabla+f&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\\nabla^2
f](https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+f&bg=ffffff&fg=333333&s=0&c=20201002)
are the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
respectively, and
![t_k](https://s0.wp.com/latex.php?latex=t_k&bg=ffffff&fg=333333&s=0&c=20201002)
is a step size chosen appropriately (e.g. with [backtracking line
search](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-line-
search-and-backtracking-line-search/)). Another way to write the update is

A big drawback of Newton’s method is that computing the Hessian can be very
expensive. _**Quasi-Newton methods**_ address this problem: update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
with

One way to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
easily is to assume that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix, i.e.

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Because this is a system of
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations with ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, there are many ways to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-sr1%2F&signup_flow=account)

This is known as the _**SR1 update**_. SR1 is simple to understand and
implement, but has two shortcomings. First, the denominator in the update for
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
![\(y - Bs\)^\\top
s](https://s0.wp.com/latex.php?latex=%28y+-+Bs%29%5E%5Ctop+s&bg=ffffff&fg=333333&s=0&c=20201002),
might be approximately zero, causing numerical issues. Second, it does not
preserve positive definiteness.

Recall from the quasi-Newton updates that we need to solve the equation ![B p
= - \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=B+p+%3D+-+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002)
for
![p](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=333333&s=0&c=20201002),
which means that we need to compute the inverse ![H =
B^{-1}](https://s0.wp.com/latex.php?latex=H+%3D+B%5E%7B-1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to get ![p = - H \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=p+%3D+-+H+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002).
Because of the [Sherman-Morrison-Woodbury
formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we
can use the update to
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
to derive an update for
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002):

which implies that
![u](https://s0.wp.com/latex.php?latex=u&bg=ffffff&fg=333333&s=0&c=20201002)
must be a multiple of ![y -
Bs](https://s0.wp.com/latex.php?latex=y+-+Bs&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this into the equation and solving for
![a](https://s0.wp.com/latex.php?latex=a&bg=ffffff&fg=333333&s=0&c=20201002),
we obtain the update

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [SR1](https://statisticaloddsandends.wordpress.com/tag/sr1/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-sr1/ "Permalink to Quasi-Newton methods: SR1").

for some ![a \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![u \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this update into the secant equation and rearranging, we get

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

![\\begin{aligned} \\nabla^2 f\(x_k\) p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1}
&= x_k + t_k p_k.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cnabla%5E2+f%28x_k%29+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This last equation is known as the _**secant equation**_ , and is sometimes
written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Note that this equation involves
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations in ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, so there is a lot of freedom in how to choose the
![B_k](https://s0.wp.com/latex.php?latex=B_k&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
On top of satisfying the secant equation, we often require 3 other conditions:

where ![\\nabla
f](https://s0.wp.com/latex.php?latex=%5Cnabla+f&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\\nabla^2
f](https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+f&bg=ffffff&fg=333333&s=0&c=20201002)
are the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
respectively, and
![t_k](https://s0.wp.com/latex.php?latex=t_k&bg=ffffff&fg=333333&s=0&c=20201002)
is a step size chosen appropriately (e.g. with [backtracking line
search](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-line-
search-and-backtracking-line-search/)). Another way to write the update is

That is, we (i) compute the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
at
![x_k](https://s0.wp.com/latex.php?latex=x_k&bg=ffffff&fg=333333&s=0&c=20201002),
(ii) solve for
![p_k](https://s0.wp.com/latex.php?latex=p_k&bg=ffffff&fg=333333&s=0&c=20201002),
then (iii) update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
according to the second line.

A big drawback of Newton’s method is that computing the Hessian can be very
expensive. _**Quasi-Newton methods**_ address this problem: update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
with

![\\begin{aligned} B_k p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1} &= x_k + t_k
p_k,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_k+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Quasi-Newton methods include SR1, DFP, BFGS and L-BFGS, which I hope to
describe in future posts.

Update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
according to some procedure so that the secant equation (and other conditions)
are satisfied.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [secant
equation](https://statisticaloddsandends.wordpress.com/tag/secant-equation/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-in-optimization/ "Permalink to Quasi-Newton methods
in optimization").

Reblogged this on [Statistical Methods, Paradigms and
Application](https://ibiloyeacresearch.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/) and commented:  
Interesting and Really good explanation here:

where ![f : \\mathbb{R}^n \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f+%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is twice differentiable and ![\\text{dom}\(f\) =
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Ctext%7Bdom%7D%28f%29+%3D+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**Newton’s
method**_](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is
a second-order descent method for finding the minimum. Starting at some
initial point, at the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration we update the candidate solution with the formula

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-in-optimization%2F&signup_flow=account)

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Because this is a system of
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations with ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, there are many ways to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).

Recall from the quasi-Newton updates that we need to solve the equation ![B p
= - \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=B+p+%3D+-+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002)
for
![p](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=333333&s=0&c=20201002),
which means that we need to compute the inverse ![H =
B^{-1}](https://s0.wp.com/latex.php?latex=H+%3D+B%5E%7B-1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to get ![p = - H \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=p+%3D+-+H+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002).
Because of the [Sherman-Morrison-Woodbury
formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we
can use the update to
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
to derive an update for
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002):

This is known as the _**SR1 update**_. SR1 is simple to understand and
implement, but has two shortcomings. First, the denominator in the update for
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
![\(y - Bs\)^\\top
s](https://s0.wp.com/latex.php?latex=%28y+-+Bs%29%5E%5Ctop+s&bg=ffffff&fg=333333&s=0&c=20201002),
might be approximately zero, causing numerical issues. Second, it does not
preserve positive definiteness.

One way to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
easily is to assume that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix, i.e.

for some ![a \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![u \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this update into the secant equation and rearranging, we get

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-sr1%2F&signup_flow=account)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [SR1](https://statisticaloddsandends.wordpress.com/tag/sr1/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-sr1/ "Permalink to Quasi-Newton methods: SR1").

which implies that
![u](https://s0.wp.com/latex.php?latex=u&bg=ffffff&fg=333333&s=0&c=20201002)
must be a multiple of ![y -
Bs](https://s0.wp.com/latex.php?latex=y+-+Bs&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this into the equation and solving for
![a](https://s0.wp.com/latex.php?latex=a&bg=ffffff&fg=333333&s=0&c=20201002),
we obtain the update

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

Update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
according to some procedure so that the secant equation (and other conditions)
are satisfied.

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This last equation is known as the _**secant equation**_ , and is sometimes
written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Note that this equation involves
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations in ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, so there is a lot of freedom in how to choose the
![B_k](https://s0.wp.com/latex.php?latex=B_k&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
On top of satisfying the secant equation, we often require 3 other conditions:

That is, we (i) compute the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
at
![x_k](https://s0.wp.com/latex.php?latex=x_k&bg=ffffff&fg=333333&s=0&c=20201002),
(ii) solve for
![p_k](https://s0.wp.com/latex.php?latex=p_k&bg=ffffff&fg=333333&s=0&c=20201002),
then (iii) update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
according to the second line.

![\\begin{aligned} B_k p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1} &= x_k + t_k
p_k,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_k+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [secant
equation](https://statisticaloddsandends.wordpress.com/tag/secant-equation/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-in-optimization/ "Permalink to Quasi-Newton methods
in optimization").

![\\begin{aligned} \\nabla^2 f\(x_k\) p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1}
&= x_k + t_k p_k.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cnabla%5E2+f%28x_k%29+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

where ![\\nabla
f](https://s0.wp.com/latex.php?latex=%5Cnabla+f&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\\nabla^2
f](https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+f&bg=ffffff&fg=333333&s=0&c=20201002)
are the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
respectively, and
![t_k](https://s0.wp.com/latex.php?latex=t_k&bg=ffffff&fg=333333&s=0&c=20201002)
is a step size chosen appropriately (e.g. with [backtracking line
search](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-line-
search-and-backtracking-line-search/)). Another way to write the update is

Reblogged this on [Statistical Methods, Paradigms and
Application](https://ibiloyeacresearch.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/) and commented:  
Interesting and Really good explanation here:

where ![f : \\mathbb{R}^n \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f+%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is twice differentiable and ![\\text{dom}\(f\) =
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Ctext%7Bdom%7D%28f%29+%3D+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**Newton’s
method**_](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is
a second-order descent method for finding the minimum. Starting at some
initial point, at the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration we update the candidate solution with the formula

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-in-optimization%2F&signup_flow=account)

Quasi-Newton methods include SR1, DFP, BFGS and L-BFGS, which I hope to
describe in future posts.

A big drawback of Newton’s method is that computing the Hessian can be very
expensive. _**Quasi-Newton methods**_ address this problem: update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
with

This is known as the _**SR1 update**_. SR1 is simple to understand and
implement, but has two shortcomings. First, the denominator in the update for
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
![\(y - Bs\)^\\top
s](https://s0.wp.com/latex.php?latex=%28y+-+Bs%29%5E%5Ctop+s&bg=ffffff&fg=333333&s=0&c=20201002),
might be approximately zero, causing numerical issues. Second, it does not
preserve positive definiteness.

for some ![a \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![u \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this update into the secant equation and rearranging, we get

One way to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
easily is to assume that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix, i.e.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-sr1%2F&signup_flow=account)

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Because this is a system of
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations with ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, there are many ways to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).

which implies that
![u](https://s0.wp.com/latex.php?latex=u&bg=ffffff&fg=333333&s=0&c=20201002)
must be a multiple of ![y -
Bs](https://s0.wp.com/latex.php?latex=y+-+Bs&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this into the equation and solving for
![a](https://s0.wp.com/latex.php?latex=a&bg=ffffff&fg=333333&s=0&c=20201002),
we obtain the update

Recall from the quasi-Newton updates that we need to solve the equation ![B p
= - \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=B+p+%3D+-+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002)
for
![p](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=333333&s=0&c=20201002),
which means that we need to compute the inverse ![H =
B^{-1}](https://s0.wp.com/latex.php?latex=H+%3D+B%5E%7B-1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to get ![p = - H \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=p+%3D+-+H+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002).
Because of the [Sherman-Morrison-Woodbury
formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we
can use the update to
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
to derive an update for
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002):

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [SR1](https://statisticaloddsandends.wordpress.com/tag/sr1/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-sr1/ "Permalink to Quasi-Newton methods: SR1").

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-in-optimization%2F&signup_flow=account)

A big drawback of Newton’s method is that computing the Hessian can be very
expensive. _**Quasi-Newton methods**_ address this problem: update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
with

where ![f : \\mathbb{R}^n \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f+%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is twice differentiable and ![\\text{dom}\(f\) =
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Ctext%7Bdom%7D%28f%29+%3D+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**Newton’s
method**_](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is
a second-order descent method for finding the minimum. Starting at some
initial point, at the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration we update the candidate solution with the formula

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This last equation is known as the _**secant equation**_ , and is sometimes
written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Note that this equation involves
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations in ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, so there is a lot of freedom in how to choose the
![B_k](https://s0.wp.com/latex.php?latex=B_k&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
On top of satisfying the secant equation, we often require 3 other conditions:

![\\begin{aligned} \\nabla^2 f\(x_k\) p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1}
&= x_k + t_k p_k.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cnabla%5E2+f%28x_k%29+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [secant
equation](https://statisticaloddsandends.wordpress.com/tag/secant-equation/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-in-optimization/ "Permalink to Quasi-Newton methods
in optimization").

Quasi-Newton methods include SR1, DFP, BFGS and L-BFGS, which I hope to
describe in future posts.

where ![\\nabla
f](https://s0.wp.com/latex.php?latex=%5Cnabla+f&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\\nabla^2
f](https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+f&bg=ffffff&fg=333333&s=0&c=20201002)
are the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
respectively, and
![t_k](https://s0.wp.com/latex.php?latex=t_k&bg=ffffff&fg=333333&s=0&c=20201002)
is a step size chosen appropriately (e.g. with [backtracking line
search](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-line-
search-and-backtracking-line-search/)). Another way to write the update is

![\\begin{aligned} B_k p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1} &= x_k + t_k
p_k,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_k+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

That is, we (i) compute the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
at
![x_k](https://s0.wp.com/latex.php?latex=x_k&bg=ffffff&fg=333333&s=0&c=20201002),
(ii) solve for
![p_k](https://s0.wp.com/latex.php?latex=p_k&bg=ffffff&fg=333333&s=0&c=20201002),
then (iii) update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
according to the second line.

Reblogged this on [Statistical Methods, Paradigms and
Application](https://ibiloyeacresearch.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/) and commented:  
Interesting and Really good explanation here:

Update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
according to some procedure so that the secant equation (and other conditions)
are satisfied.

for some ![a \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![u \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this update into the secant equation and rearranging, we get

which implies that
![u](https://s0.wp.com/latex.php?latex=u&bg=ffffff&fg=333333&s=0&c=20201002)
must be a multiple of ![y -
Bs](https://s0.wp.com/latex.php?latex=y+-+Bs&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this into the equation and solving for
![a](https://s0.wp.com/latex.php?latex=a&bg=ffffff&fg=333333&s=0&c=20201002),
we obtain the update

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Because this is a system of
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations with ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, there are many ways to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).

Recall from the quasi-Newton updates that we need to solve the equation ![B p
= - \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=B+p+%3D+-+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002)
for
![p](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=333333&s=0&c=20201002),
which means that we need to compute the inverse ![H =
B^{-1}](https://s0.wp.com/latex.php?latex=H+%3D+B%5E%7B-1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to get ![p = - H \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=p+%3D+-+H+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002).
Because of the [Sherman-Morrison-Woodbury
formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we
can use the update to
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
to derive an update for
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002):

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [SR1](https://statisticaloddsandends.wordpress.com/tag/sr1/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-sr1/ "Permalink to Quasi-Newton methods: SR1").

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-sr1%2F&signup_flow=account)

One way to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
easily is to assume that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix, i.e.

This is known as the _**SR1 update**_. SR1 is simple to understand and
implement, but has two shortcomings. First, the denominator in the update for
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
![\(y - Bs\)^\\top
s](https://s0.wp.com/latex.php?latex=%28y+-+Bs%29%5E%5Ctop+s&bg=ffffff&fg=333333&s=0&c=20201002),
might be approximately zero, causing numerical issues. Second, it does not
preserve positive definiteness.

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

Reblogged this on [Statistical Methods, Paradigms and
Application](https://ibiloyeacresearch.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/) and commented:  
Interesting and Really good explanation here:

![\\begin{aligned} \\nabla^2 f\(x_k\) p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1}
&= x_k + t_k p_k.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cnabla%5E2+f%28x_k%29+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

A big drawback of Newton’s method is that computing the Hessian can be very
expensive. _**Quasi-Newton methods**_ address this problem: update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
with

where ![\\nabla
f](https://s0.wp.com/latex.php?latex=%5Cnabla+f&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\\nabla^2
f](https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+f&bg=ffffff&fg=333333&s=0&c=20201002)
are the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
respectively, and
![t_k](https://s0.wp.com/latex.php?latex=t_k&bg=ffffff&fg=333333&s=0&c=20201002)
is a step size chosen appropriately (e.g. with [backtracking line
search](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-line-
search-and-backtracking-line-search/)). Another way to write the update is

That is, we (i) compute the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
at
![x_k](https://s0.wp.com/latex.php?latex=x_k&bg=ffffff&fg=333333&s=0&c=20201002),
(ii) solve for
![p_k](https://s0.wp.com/latex.php?latex=p_k&bg=ffffff&fg=333333&s=0&c=20201002),
then (iii) update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
according to the second line.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [secant
equation](https://statisticaloddsandends.wordpress.com/tag/secant-equation/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-in-optimization/ "Permalink to Quasi-Newton methods
in optimization").

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This last equation is known as the _**secant equation**_ , and is sometimes
written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Note that this equation involves
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations in ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, so there is a lot of freedom in how to choose the
![B_k](https://s0.wp.com/latex.php?latex=B_k&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
On top of satisfying the secant equation, we often require 3 other conditions:

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-in-optimization%2F&signup_flow=account)

Quasi-Newton methods include SR1, DFP, BFGS and L-BFGS, which I hope to
describe in future posts.

Update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
according to some procedure so that the secant equation (and other conditions)
are satisfied.

![\\begin{aligned} B_k p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1} &= x_k + t_k
p_k,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_k+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

where ![f : \\mathbb{R}^n \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f+%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is twice differentiable and ![\\text{dom}\(f\) =
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Ctext%7Bdom%7D%28f%29+%3D+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**Newton’s
method**_](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is
a second-order descent method for finding the minimum. Starting at some
initial point, at the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration we update the candidate solution with the formula

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Because this is a system of
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations with ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, there are many ways to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).

One way to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
easily is to assume that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix, i.e.

for some ![a \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![u \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this update into the secant equation and rearranging, we get

This is known as the _**SR1 update**_. SR1 is simple to understand and
implement, but has two shortcomings. First, the denominator in the update for
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
![\(y - Bs\)^\\top
s](https://s0.wp.com/latex.php?latex=%28y+-+Bs%29%5E%5Ctop+s&bg=ffffff&fg=333333&s=0&c=20201002),
might be approximately zero, causing numerical issues. Second, it does not
preserve positive definiteness.

Recall from the quasi-Newton updates that we need to solve the equation ![B p
= - \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=B+p+%3D+-+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002)
for
![p](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=333333&s=0&c=20201002),
which means that we need to compute the inverse ![H =
B^{-1}](https://s0.wp.com/latex.php?latex=H+%3D+B%5E%7B-1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to get ![p = - H \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=p+%3D+-+H+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002).
Because of the [Sherman-Morrison-Woodbury
formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we
can use the update to
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
to derive an update for
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002):

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-sr1%2F&signup_flow=account)

which implies that
![u](https://s0.wp.com/latex.php?latex=u&bg=ffffff&fg=333333&s=0&c=20201002)
must be a multiple of ![y -
Bs](https://s0.wp.com/latex.php?latex=y+-+Bs&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this into the equation and solving for
![a](https://s0.wp.com/latex.php?latex=a&bg=ffffff&fg=333333&s=0&c=20201002),
we obtain the update

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [SR1](https://statisticaloddsandends.wordpress.com/tag/sr1/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-sr1/ "Permalink to Quasi-Newton methods: SR1").

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

That is, we (i) compute the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
at
![x_k](https://s0.wp.com/latex.php?latex=x_k&bg=ffffff&fg=333333&s=0&c=20201002),
(ii) solve for
![p_k](https://s0.wp.com/latex.php?latex=p_k&bg=ffffff&fg=333333&s=0&c=20201002),
then (iii) update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
according to the second line.

A big drawback of Newton’s method is that computing the Hessian can be very
expensive. _**Quasi-Newton methods**_ address this problem: update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
with

where ![f : \\mathbb{R}^n \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f+%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is twice differentiable and ![\\text{dom}\(f\) =
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Ctext%7Bdom%7D%28f%29+%3D+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**Newton’s
method**_](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is
a second-order descent method for finding the minimum. Starting at some
initial point, at the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration we update the candidate solution with the formula

Reblogged this on [Statistical Methods, Paradigms and
Application](https://ibiloyeacresearch.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/) and commented:  
Interesting and Really good explanation here:

![\\begin{aligned} B_k p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1} &= x_k + t_k
p_k,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_k+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

![\\begin{aligned} \\nabla^2 f\(x_k\) p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1}
&= x_k + t_k p_k.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cnabla%5E2+f%28x_k%29+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [secant
equation](https://statisticaloddsandends.wordpress.com/tag/secant-equation/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-in-optimization/ "Permalink to Quasi-Newton methods
in optimization").

Quasi-Newton methods include SR1, DFP, BFGS and L-BFGS, which I hope to
describe in future posts.

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This last equation is known as the _**secant equation**_ , and is sometimes
written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Note that this equation involves
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations in ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, so there is a lot of freedom in how to choose the
![B_k](https://s0.wp.com/latex.php?latex=B_k&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
On top of satisfying the secant equation, we often require 3 other conditions:

where ![\\nabla
f](https://s0.wp.com/latex.php?latex=%5Cnabla+f&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\\nabla^2
f](https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+f&bg=ffffff&fg=333333&s=0&c=20201002)
are the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
respectively, and
![t_k](https://s0.wp.com/latex.php?latex=t_k&bg=ffffff&fg=333333&s=0&c=20201002)
is a step size chosen appropriately (e.g. with [backtracking line
search](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-line-
search-and-backtracking-line-search/)). Another way to write the update is

Update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
according to some procedure so that the secant equation (and other conditions)
are satisfied.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-in-optimization%2F&signup_flow=account)

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Because this is a system of
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations with ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, there are many ways to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).

for some ![a \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![u \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this update into the secant equation and rearranging, we get

Recall from the quasi-Newton updates that we need to solve the equation ![B p
= - \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=B+p+%3D+-+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002)
for
![p](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=333333&s=0&c=20201002),
which means that we need to compute the inverse ![H =
B^{-1}](https://s0.wp.com/latex.php?latex=H+%3D+B%5E%7B-1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to get ![p = - H \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=p+%3D+-+H+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002).
Because of the [Sherman-Morrison-Woodbury
formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we
can use the update to
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
to derive an update for
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002):

One way to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
easily is to assume that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix, i.e.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-sr1%2F&signup_flow=account)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [SR1](https://statisticaloddsandends.wordpress.com/tag/sr1/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-sr1/ "Permalink to Quasi-Newton methods: SR1").

which implies that
![u](https://s0.wp.com/latex.php?latex=u&bg=ffffff&fg=333333&s=0&c=20201002)
must be a multiple of ![y -
Bs](https://s0.wp.com/latex.php?latex=y+-+Bs&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this into the equation and solving for
![a](https://s0.wp.com/latex.php?latex=a&bg=ffffff&fg=333333&s=0&c=20201002),
we obtain the update

This is known as the _**SR1 update**_. SR1 is simple to understand and
implement, but has two shortcomings. First, the denominator in the update for
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
![\(y - Bs\)^\\top
s](https://s0.wp.com/latex.php?latex=%28y+-+Bs%29%5E%5Ctop+s&bg=ffffff&fg=333333&s=0&c=20201002),
might be approximately zero, causing numerical issues. Second, it does not
preserve positive definiteness.

[![](https://statisticaloddsandends.files.wordpress.com/2022/11/backtracking-
line-
search.png?w=584&h=327)](https://statisticaloddsandends.files.wordpress.com/2022/11/backtracking-
line-search.png)This is the same picture as that for exact line search, except
we’ve added the line ![y = f\(x\) + \\alpha t \\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=y+%3D+f%28x%29+%2B+%5Calpha+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002).
This line always has negative slope: because ![\\Delta
x](https://s0.wp.com/latex.php?latex=%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002)
is a descent direction, we must have ![\\nabla f\(x\)^\\top \\Delta x <
0](https://s0.wp.com/latex.php?latex=%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x+%3C+0&bg=ffffff&fg=333333&s=0&c=20201002).
The stopping condition, ![f\(x + t\\Delta x\) \\leq f\(x\) + \\alpha t \\nabla
f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=f%28x+%2B+t%5CDelta+x%29+%5Cleq+f%28x%29+%2B+%5Calpha+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002),
holds for the blue segment ![\[0,
t_0\]](https://s0.wp.com/latex.php?latex=%5B0%2C+t_0%5D&bg=ffffff&fg=333333&s=0&c=20201002).
The operation ![t \\leftarrow \\beta
t](https://s0.wp.com/latex.php?latex=t+%5Cleftarrow+%5Cbeta+t&bg=ffffff&fg=333333&s=0&c=20201002)
makes the step size smaller and smaller until
![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=333333&s=0&c=20201002)
falls in the blue segment, and by the way it’s set up, we know that the
objective function will have a strictly smaller value.

( _ **Note:**_ The last diagram is taken from Reference 1, and the other
diagrams are this base + my annotations.)

While ![f\(x + t\\Delta x\) > f\(x\) + \\alpha t \\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=f%28x+%2B+t%5CDelta+x%29+%3E+f%28x%29+%2B+%5Calpha+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002),
set ![t \\leftarrow \\beta
t](https://s0.wp.com/latex.php?latex=t+%5Cleftarrow+%5Cbeta+t&bg=ffffff&fg=333333&s=0&c=20201002).

Since exact line search requires solving a minimization problem (albeit in one
dimension), it can be expensively unless
![t^*](https://s0.wp.com/latex.php?latex=t%5E%2A&bg=ffffff&fg=333333&s=0&c=20201002)
can be computed efficiently (or even analytically).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F19%2Fexact-line-search-and-backtracking-line-search%2F&signup_flow=account)

There are several ways to choose a descent direction (see [this previous
post](https://statisticaloddsandends.wordpress.com/2021/04/29/what-is-
steepest-descent/) on steepest descent for some examples). This post is about
two ways to choose the step size. This step is called a _**line search**_
because the step size determines where along the line ![\\{x + t \\Delta x : t
\\geq 0
\\}](https://s0.wp.com/latex.php?latex=%5C%7Bx+%2B+t+%5CDelta+x+%3A+t+%5Cgeq+0+%5C%7D&bg=ffffff&fg=333333&s=0&c=20201002)
the next iterate will be.

Assume we are trying to minimize some convex function ![f: \\mathbb{R}^n
\\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
which is differentiable. One way to do this is to use a _**descent method**_.
A general descent method can be described as follows: Given a starting point
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
in the domain of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002),
iterate over the following 3 steps until some stopping criterion is satisfied:

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [backtracking line
search](https://statisticaloddsandends.wordpress.com/tag/backtracking-line-
search/), [convex](https://statisticaloddsandends.wordpress.com/tag/convex/),
[exact line search](https://statisticaloddsandends.wordpress.com/tag/exact-
line-search/), [line
search](https://statisticaloddsandends.wordpress.com/tag/line-search/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-
line-search-and-backtracking-line-search/ "Permalink to Exact line search and
backtracking line search").

_**Backtracking line search**_ uses a heuristic to quickly determine a step
size which decreases the objective function value by a certain amount. The
user needs to set two parameters, ![0 < \\alpha <
0.5](https://s0.wp.com/latex.php?latex=0+%3C+%5Calpha+%3C+0.5&bg=ffffff&fg=333333&s=0&c=20201002)
and ![0 < \\beta <
1](https://s0.wp.com/latex.php?latex=0+%3C+%5Cbeta+%3C+1&bg=ffffff&fg=333333&s=0&c=20201002).
In each iteration, after the descent direction ![\\Delta
x](https://s0.wp.com/latex.php?latex=%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002)
has been chosen, the step size is determine by the following:

_**Exact line search**_ finds
![t^\\star](https://s0.wp.com/latex.php?latex=t%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002)
such that ![f\(x + t \\Delta
x\)](https://s0.wp.com/latex.php?latex=f%28x+%2B+t+%5CDelta+x%29&bg=ffffff&fg=333333&s=0&c=20201002)
attains its minimum value at ![t =
t^\\star](https://s0.wp.com/latex.php?latex=t+%3D+t%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002),
where we let
![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=333333&s=0&c=20201002)
range over non-negative values. The diagram below demonstrates this:

In other words, we keep reducing the step size by the same multiplicative
factor until the ![f\(x + t\\Delta x\) \\leq f\(x\) + \\alpha t \\nabla
f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=f%28x+%2B+t%5CDelta+x%29+%5Cleq+f%28x%29+%2B+%5Calpha+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002).

How do we know that
![t_0](https://s0.wp.com/latex.php?latex=t_0&bg=ffffff&fg=333333&s=0&c=20201002)
is always ![>
0](https://s0.wp.com/latex.php?latex=%3E+0&bg=ffffff&fg=333333&s=0&c=20201002)
(i.e. the blue segment exists)? It’s because of how we chose the term
![\\alpha t \\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=%5Calpha+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002).
It can be shown that the derivative of ![f\(x + t \\Delta
x\)](https://s0.wp.com/latex.php?latex=f%28x+%2B+t+%5CDelta+x%29&bg=ffffff&fg=333333&s=0&c=20201002)
w.r.t.
![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=333333&s=0&c=20201002)
at the point ![t =
0](https://s0.wp.com/latex.php?latex=t+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002)
is ![\\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002).
Hence, the line ![y = f\(x\) + t \\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=y+%3D+f%28x%29+%2B+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002)
is tangent to ![y = f\(x + t \\nabla
x\)](https://s0.wp.com/latex.php?latex=y+%3D+f%28x+%2B+t+%5Cnabla+x%29&bg=ffffff&fg=333333&s=0&c=20201002)
at ![t =
0](https://s0.wp.com/latex.php?latex=t+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002).
Multiplying the term ![t \\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002)
by some ![0 < \\alpha <
1](https://s0.wp.com/latex.php?latex=0+%3C+%5Calpha+%3C+1&bg=ffffff&fg=333333&s=0&c=20201002)
ensures that the stopping condition will hold for some non-trivial segment
![\[0,
t_0\]](https://s0.wp.com/latex.php?latex=%5B0%2C+t_0%5D&bg=ffffff&fg=333333&s=0&c=20201002).

[![](https://statisticaloddsandends.files.wordpress.com/2022/11/exact-line-
search.png?w=584&h=327)](https://statisticaloddsandends.files.wordpress.com/2022/11/exact-
line-search.png)The curve is the value of the function along the ray ![x + t
\\Delta
x](https://s0.wp.com/latex.php?latex=x+%2B+t+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002)
with ![t \\geq
0](https://s0.wp.com/latex.php?latex=t+%5Cgeq+0&bg=ffffff&fg=333333&s=0&c=20201002).
![t^*](https://s0.wp.com/latex.php?latex=t%5E%2A&bg=ffffff&fg=333333&s=0&c=20201002)
is the point where this function is minimized. It should be clear from the
description that this choice will result in smaller values of the function in
each iteration.

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

![\\begin{aligned} \\nabla^2 f\(x_k\) p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1}
&= x_k + t_k p_k.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cnabla%5E2+f%28x_k%29+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [secant
equation](https://statisticaloddsandends.wordpress.com/tag/secant-equation/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-in-optimization/ "Permalink to Quasi-Newton methods
in optimization").

Quasi-Newton methods include SR1, DFP, BFGS and L-BFGS, which I hope to
describe in future posts.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-in-optimization%2F&signup_flow=account)

Reblogged this on [Statistical Methods, Paradigms and
Application](https://ibiloyeacresearch.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/) and commented:  
Interesting and Really good explanation here:

A big drawback of Newton’s method is that computing the Hessian can be very
expensive. _**Quasi-Newton methods**_ address this problem: update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
with

where ![f : \\mathbb{R}^n \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f+%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is twice differentiable and ![\\text{dom}\(f\) =
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Ctext%7Bdom%7D%28f%29+%3D+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**Newton’s
method**_](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is
a second-order descent method for finding the minimum. Starting at some
initial point, at the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration we update the candidate solution with the formula

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This last equation is known as the _**secant equation**_ , and is sometimes
written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Note that this equation involves
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations in ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, so there is a lot of freedom in how to choose the
![B_k](https://s0.wp.com/latex.php?latex=B_k&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
On top of satisfying the secant equation, we often require 3 other conditions:

![\\begin{aligned} B_k p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1} &= x_k + t_k
p_k,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_k+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
according to some procedure so that the secant equation (and other conditions)
are satisfied.

where ![\\nabla
f](https://s0.wp.com/latex.php?latex=%5Cnabla+f&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\\nabla^2
f](https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+f&bg=ffffff&fg=333333&s=0&c=20201002)
are the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
respectively, and
![t_k](https://s0.wp.com/latex.php?latex=t_k&bg=ffffff&fg=333333&s=0&c=20201002)
is a step size chosen appropriately (e.g. with [backtracking line
search](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-line-
search-and-backtracking-line-search/)). Another way to write the update is

That is, we (i) compute the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
at
![x_k](https://s0.wp.com/latex.php?latex=x_k&bg=ffffff&fg=333333&s=0&c=20201002),
(ii) solve for
![p_k](https://s0.wp.com/latex.php?latex=p_k&bg=ffffff&fg=333333&s=0&c=20201002),
then (iii) update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
according to the second line.

This is known as the _**SR1 update**_. SR1 is simple to understand and
implement, but has two shortcomings. First, the denominator in the update for
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
![\(y - Bs\)^\\top
s](https://s0.wp.com/latex.php?latex=%28y+-+Bs%29%5E%5Ctop+s&bg=ffffff&fg=333333&s=0&c=20201002),
might be approximately zero, causing numerical issues. Second, it does not
preserve positive definiteness.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [SR1](https://statisticaloddsandends.wordpress.com/tag/sr1/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-sr1/ "Permalink to Quasi-Newton methods: SR1").

which implies that
![u](https://s0.wp.com/latex.php?latex=u&bg=ffffff&fg=333333&s=0&c=20201002)
must be a multiple of ![y -
Bs](https://s0.wp.com/latex.php?latex=y+-+Bs&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this into the equation and solving for
![a](https://s0.wp.com/latex.php?latex=a&bg=ffffff&fg=333333&s=0&c=20201002),
we obtain the update

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Because this is a system of
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations with ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, there are many ways to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).

One way to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
easily is to assume that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix, i.e.

for some ![a \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![u \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this update into the secant equation and rearranging, we get

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-sr1%2F&signup_flow=account)

Recall from the quasi-Newton updates that we need to solve the equation ![B p
= - \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=B+p+%3D+-+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002)
for
![p](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=333333&s=0&c=20201002),
which means that we need to compute the inverse ![H =
B^{-1}](https://s0.wp.com/latex.php?latex=H+%3D+B%5E%7B-1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to get ![p = - H \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=p+%3D+-+H+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002).
Because of the [Sherman-Morrison-Woodbury
formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we
can use the update to
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
to derive an update for
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002):

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

That is, we (i) compute the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
at
![x_k](https://s0.wp.com/latex.php?latex=x_k&bg=ffffff&fg=333333&s=0&c=20201002),
(ii) solve for
![p_k](https://s0.wp.com/latex.php?latex=p_k&bg=ffffff&fg=333333&s=0&c=20201002),
then (iii) update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
according to the second line.

where ![f : \\mathbb{R}^n \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f+%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is twice differentiable and ![\\text{dom}\(f\) =
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Ctext%7Bdom%7D%28f%29+%3D+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**Newton’s
method**_](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is
a second-order descent method for finding the minimum. Starting at some
initial point, at the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration we update the candidate solution with the formula

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-in-optimization%2F&signup_flow=account)

![\\begin{aligned} \\nabla^2 f\(x_k\) p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1}
&= x_k + t_k p_k.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cnabla%5E2+f%28x_k%29+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

![\\begin{aligned} B_k p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1} &= x_k + t_k
p_k,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_k+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

where ![\\nabla
f](https://s0.wp.com/latex.php?latex=%5Cnabla+f&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\\nabla^2
f](https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+f&bg=ffffff&fg=333333&s=0&c=20201002)
are the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
respectively, and
![t_k](https://s0.wp.com/latex.php?latex=t_k&bg=ffffff&fg=333333&s=0&c=20201002)
is a step size chosen appropriately (e.g. with [backtracking line
search](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-line-
search-and-backtracking-line-search/)). Another way to write the update is

A big drawback of Newton’s method is that computing the Hessian can be very
expensive. _**Quasi-Newton methods**_ address this problem: update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
with

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This last equation is known as the _**secant equation**_ , and is sometimes
written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Note that this equation involves
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations in ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, so there is a lot of freedom in how to choose the
![B_k](https://s0.wp.com/latex.php?latex=B_k&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
On top of satisfying the secant equation, we often require 3 other conditions:

Quasi-Newton methods include SR1, DFP, BFGS and L-BFGS, which I hope to
describe in future posts.

Reblogged this on [Statistical Methods, Paradigms and
Application](https://ibiloyeacresearch.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/) and commented:  
Interesting and Really good explanation here:

Update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
according to some procedure so that the secant equation (and other conditions)
are satisfied.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [secant
equation](https://statisticaloddsandends.wordpress.com/tag/secant-equation/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-in-optimization/ "Permalink to Quasi-Newton methods
in optimization").

for some ![a \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![u \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this update into the secant equation and rearranging, we get

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [SR1](https://statisticaloddsandends.wordpress.com/tag/sr1/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-sr1/ "Permalink to Quasi-Newton methods: SR1").

This is known as the _**SR1 update**_. SR1 is simple to understand and
implement, but has two shortcomings. First, the denominator in the update for
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
![\(y - Bs\)^\\top
s](https://s0.wp.com/latex.php?latex=%28y+-+Bs%29%5E%5Ctop+s&bg=ffffff&fg=333333&s=0&c=20201002),
might be approximately zero, causing numerical issues. Second, it does not
preserve positive definiteness.

Recall from the quasi-Newton updates that we need to solve the equation ![B p
= - \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=B+p+%3D+-+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002)
for
![p](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=333333&s=0&c=20201002),
which means that we need to compute the inverse ![H =
B^{-1}](https://s0.wp.com/latex.php?latex=H+%3D+B%5E%7B-1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to get ![p = - H \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=p+%3D+-+H+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002).
Because of the [Sherman-Morrison-Woodbury
formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we
can use the update to
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
to derive an update for
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002):

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Because this is a system of
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations with ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, there are many ways to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).

which implies that
![u](https://s0.wp.com/latex.php?latex=u&bg=ffffff&fg=333333&s=0&c=20201002)
must be a multiple of ![y -
Bs](https://s0.wp.com/latex.php?latex=y+-+Bs&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this into the equation and solving for
![a](https://s0.wp.com/latex.php?latex=a&bg=ffffff&fg=333333&s=0&c=20201002),
we obtain the update

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-sr1%2F&signup_flow=account)

One way to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
easily is to assume that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix, i.e.

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

Quasi-Newton methods include SR1, DFP, BFGS and L-BFGS, which I hope to
describe in future posts.

where ![f : \\mathbb{R}^n \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f+%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is twice differentiable and ![\\text{dom}\(f\) =
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Ctext%7Bdom%7D%28f%29+%3D+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**Newton’s
method**_](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is
a second-order descent method for finding the minimum. Starting at some
initial point, at the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration we update the candidate solution with the formula

Reblogged this on [Statistical Methods, Paradigms and
Application](https://ibiloyeacresearch.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/) and commented:  
Interesting and Really good explanation here:

![\\begin{aligned} \\nabla^2 f\(x_k\) p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1}
&= x_k + t_k p_k.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cnabla%5E2+f%28x_k%29+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

A big drawback of Newton’s method is that computing the Hessian can be very
expensive. _**Quasi-Newton methods**_ address this problem: update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
with

![\\begin{aligned} B_k p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1} &= x_k + t_k
p_k,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_k+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

That is, we (i) compute the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
at
![x_k](https://s0.wp.com/latex.php?latex=x_k&bg=ffffff&fg=333333&s=0&c=20201002),
(ii) solve for
![p_k](https://s0.wp.com/latex.php?latex=p_k&bg=ffffff&fg=333333&s=0&c=20201002),
then (iii) update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
according to the second line.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [secant
equation](https://statisticaloddsandends.wordpress.com/tag/secant-equation/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-in-optimization/ "Permalink to Quasi-Newton methods
in optimization").

where ![\\nabla
f](https://s0.wp.com/latex.php?latex=%5Cnabla+f&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\\nabla^2
f](https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+f&bg=ffffff&fg=333333&s=0&c=20201002)
are the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
respectively, and
![t_k](https://s0.wp.com/latex.php?latex=t_k&bg=ffffff&fg=333333&s=0&c=20201002)
is a step size chosen appropriately (e.g. with [backtracking line
search](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-line-
search-and-backtracking-line-search/)). Another way to write the update is

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-in-optimization%2F&signup_flow=account)

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This last equation is known as the _**secant equation**_ , and is sometimes
written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Note that this equation involves
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations in ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, so there is a lot of freedom in how to choose the
![B_k](https://s0.wp.com/latex.php?latex=B_k&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
On top of satisfying the secant equation, we often require 3 other conditions:

Update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
according to some procedure so that the secant equation (and other conditions)
are satisfied.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [SR1](https://statisticaloddsandends.wordpress.com/tag/sr1/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-sr1/ "Permalink to Quasi-Newton methods: SR1").

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Because this is a system of
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations with ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, there are many ways to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).

Recall from the quasi-Newton updates that we need to solve the equation ![B p
= - \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=B+p+%3D+-+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002)
for
![p](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=333333&s=0&c=20201002),
which means that we need to compute the inverse ![H =
B^{-1}](https://s0.wp.com/latex.php?latex=H+%3D+B%5E%7B-1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to get ![p = - H \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=p+%3D+-+H+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002).
Because of the [Sherman-Morrison-Woodbury
formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we
can use the update to
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
to derive an update for
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002):

for some ![a \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![u \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this update into the secant equation and rearranging, we get

which implies that
![u](https://s0.wp.com/latex.php?latex=u&bg=ffffff&fg=333333&s=0&c=20201002)
must be a multiple of ![y -
Bs](https://s0.wp.com/latex.php?latex=y+-+Bs&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this into the equation and solving for
![a](https://s0.wp.com/latex.php?latex=a&bg=ffffff&fg=333333&s=0&c=20201002),
we obtain the update

This is known as the _**SR1 update**_. SR1 is simple to understand and
implement, but has two shortcomings. First, the denominator in the update for
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
![\(y - Bs\)^\\top
s](https://s0.wp.com/latex.php?latex=%28y+-+Bs%29%5E%5Ctop+s&bg=ffffff&fg=333333&s=0&c=20201002),
might be approximately zero, causing numerical issues. Second, it does not
preserve positive definiteness.

One way to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
easily is to assume that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix, i.e.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-sr1%2F&signup_flow=account)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This last equation is known as the _**secant equation**_ , and is sometimes
written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Note that this equation involves
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations in ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, so there is a lot of freedom in how to choose the
![B_k](https://s0.wp.com/latex.php?latex=B_k&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
On top of satisfying the secant equation, we often require 3 other conditions:

![\\begin{aligned} \\nabla^2 f\(x_k\) p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1}
&= x_k + t_k p_k.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cnabla%5E2+f%28x_k%29+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Reblogged this on [Statistical Methods, Paradigms and
Application](https://ibiloyeacresearch.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/) and commented:  
Interesting and Really good explanation here:

![\\begin{aligned} B_k p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1} &= x_k + t_k
p_k,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_k+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

That is, we (i) compute the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
at
![x_k](https://s0.wp.com/latex.php?latex=x_k&bg=ffffff&fg=333333&s=0&c=20201002),
(ii) solve for
![p_k](https://s0.wp.com/latex.php?latex=p_k&bg=ffffff&fg=333333&s=0&c=20201002),
then (iii) update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
according to the second line.

A big drawback of Newton’s method is that computing the Hessian can be very
expensive. _**Quasi-Newton methods**_ address this problem: update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
with

where ![\\nabla
f](https://s0.wp.com/latex.php?latex=%5Cnabla+f&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\\nabla^2
f](https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+f&bg=ffffff&fg=333333&s=0&c=20201002)
are the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
respectively, and
![t_k](https://s0.wp.com/latex.php?latex=t_k&bg=ffffff&fg=333333&s=0&c=20201002)
is a step size chosen appropriately (e.g. with [backtracking line
search](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-line-
search-and-backtracking-line-search/)). Another way to write the update is

Update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
according to some procedure so that the secant equation (and other conditions)
are satisfied.

Quasi-Newton methods include SR1, DFP, BFGS and L-BFGS, which I hope to
describe in future posts.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [secant
equation](https://statisticaloddsandends.wordpress.com/tag/secant-equation/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-in-optimization/ "Permalink to Quasi-Newton methods
in optimization").

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-in-optimization%2F&signup_flow=account)

where ![f : \\mathbb{R}^n \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f+%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is twice differentiable and ![\\text{dom}\(f\) =
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Ctext%7Bdom%7D%28f%29+%3D+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**Newton’s
method**_](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is
a second-order descent method for finding the minimum. Starting at some
initial point, at the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration we update the candidate solution with the formula

This is known as the _**SR1 update**_. SR1 is simple to understand and
implement, but has two shortcomings. First, the denominator in the update for
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
![\(y - Bs\)^\\top
s](https://s0.wp.com/latex.php?latex=%28y+-+Bs%29%5E%5Ctop+s&bg=ffffff&fg=333333&s=0&c=20201002),
might be approximately zero, causing numerical issues. Second, it does not
preserve positive definiteness.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [SR1](https://statisticaloddsandends.wordpress.com/tag/sr1/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-sr1/ "Permalink to Quasi-Newton methods: SR1").

One way to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
easily is to assume that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix, i.e.

Recall from the quasi-Newton updates that we need to solve the equation ![B p
= - \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=B+p+%3D+-+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002)
for
![p](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=333333&s=0&c=20201002),
which means that we need to compute the inverse ![H =
B^{-1}](https://s0.wp.com/latex.php?latex=H+%3D+B%5E%7B-1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to get ![p = - H \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=p+%3D+-+H+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002).
Because of the [Sherman-Morrison-Woodbury
formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we
can use the update to
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
to derive an update for
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002):

which implies that
![u](https://s0.wp.com/latex.php?latex=u&bg=ffffff&fg=333333&s=0&c=20201002)
must be a multiple of ![y -
Bs](https://s0.wp.com/latex.php?latex=y+-+Bs&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this into the equation and solving for
![a](https://s0.wp.com/latex.php?latex=a&bg=ffffff&fg=333333&s=0&c=20201002),
we obtain the update

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Because this is a system of
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations with ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, there are many ways to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).

for some ![a \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![u \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this update into the secant equation and rearranging, we get

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-sr1%2F&signup_flow=account)

![\\begin{aligned} \\text{proj}_{C\(P\)}\(\\hat{x}\) &= \\hat{x} +
\\text{proj}_{C\(P - \\hat{x}\)}\(0\), \\\\  \\text{proj}_{C\(P -
\\hat{x}\)}\(0\) &= \\text{proj}_{C\(P\)}\(\\hat{x}\) - \\hat{x}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext%7Bproj%7D_%7BC%28P%29%7D%28%5Chat%7Bx%7D%29+%26%3D+%5Chat%7Bx%7D+%2B+%5Ctext%7Bproj%7D_%7BC%28P+-+%5Chat%7Bx%7D%29%7D%280%29%2C+%5C%5C++%5Ctext%7Bproj%7D_%7BC%28P+-+%5Chat%7Bx%7D%29%7D%280%29+%26%3D+%5Ctext%7Bproj%7D_%7BC%28P%29%7D%28%5Chat%7Bx%7D%29+-+%5Chat%7Bx%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

![\\begin{aligned} \(x - \\hat{x}\)^\\top \(p_j - \\hat{x}\) &\\geq \\|x -
\\hat{x}\\|^2 \\quad \\text{for all } j = 1, \\dots, m, \\\\  \(x -
\\hat{x}\)^\\top p_j &\\geq \(x - \\hat{x}\)^\\top x \\quad \\text{for all } j
= 1, \\dots, m.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%28x+-+%5Chat%7Bx%7D%29%5E%5Ctop+%28p_j+-+%5Chat%7Bx%7D%29+%26%5Cgeq+%5C%7Cx+-+%5Chat%7Bx%7D%5C%7C%5E2+%5Cquad+%5Ctext%7Bfor+all+%7D+j+%3D+1%2C+%5Cdots%2C+m%2C+%5C%5C++%28x+-+%5Chat%7Bx%7D%29%5E%5Ctop+p_j+%26%5Cgeq+%28x+-+%5Chat%7Bx%7D%29%5E%5Ctop+x+%5Cquad+%5Ctext%7Bfor+all+%7D+j+%3D+1%2C+%5Cdots%2C+m.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

The diagram below demonstrates this in two dimensions. In both figures, the
candidate for being the nearest point,
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002),
is ![X =
P_1](https://s0.wp.com/latex.php?latex=X+%3D+P_1&bg=ffffff&fg=333333&s=0&c=20201002).
![P_1](https://s0.wp.com/latex.php?latex=P_1&bg=ffffff&fg=333333&s=0&c=20201002)
is the nearest point to the origin in the left figure but not the right
figure. The angles that the optimality condition looks at are the angles
between the black arrow and the colored arrows: they are also marked by little
arcs. We see that in the left figure, both the angles are ![\\leq
90^\\circ](https://s0.wp.com/latex.php?latex=%5Cleq+90%5E%5Ccirc&bg=ffffff&fg=333333&s=0&c=20201002).
In the right figure, the yellow angle is ![>
90^\\circ](https://s0.wp.com/latex.php?latex=%3E+90%5E%5Ccirc&bg=ffffff&fg=333333&s=0&c=20201002).
It corresponds to a vertex of the polytope that is on the same side of the
hyperplane (dotted line) as the origin, and hence some point on the line
![XP_3](https://s0.wp.com/latex.php?latex=XP_3&bg=ffffff&fg=333333&s=0&c=20201002)
is going to be closer to the origin than
![X](https://s0.wp.com/latex.php?latex=X&bg=ffffff&fg=333333&s=0&c=20201002).

( _ **Note:**_ I learned of the iff condition for general
![\\hat{x}](https://s0.wp.com/latex.php?latex=%5Chat%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002)
from Reference 2.)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[hyperplane](https://statisticaloddsandends.wordpress.com/tag/hyperplane/),
[polytope](https://statisticaloddsandends.wordpress.com/tag/polytope/),
[projection](https://statisticaloddsandends.wordpress.com/tag/projection/),
[wolfe](https://statisticaloddsandends.wordpress.com/tag/wolfe/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/12/10/optimality-
condition-for-euclidean-projection-onto-a-convex-polytope/ "Permalink to
Optimality condition for Euclidean projection onto a convex polytope").

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

Let ![P - \\hat{x} = \\{ p_1 - \\hat{x}, \\dots, p_m - \\hat{x}
\\}](https://s0.wp.com/latex.php?latex=P+-+%5Chat%7Bx%7D+%3D+%5C%7B+p_1+-+%5Chat%7Bx%7D%2C+%5Cdots%2C+p_m+-+%5Chat%7Bx%7D+%5C%7D&bg=ffffff&fg=333333&s=0&c=20201002).
Projecting
![\\hat{x}](https://s0.wp.com/latex.php?latex=%5Chat%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002)
onto
![C\(P\)](https://s0.wp.com/latex.php?latex=C%28P%29&bg=ffffff&fg=333333&s=0&c=20201002)
is equivalent to projecting
![0](https://s0.wp.com/latex.php?latex=0&bg=ffffff&fg=333333&s=0&c=20201002)
onto ![C\(P -
\\hat{x}\)](https://s0.wp.com/latex.php?latex=C%28P+-+%5Chat%7Bx%7D%29&bg=ffffff&fg=333333&s=0&c=20201002)
and then adding
![\\hat{x}](https://s0.wp.com/latex.php?latex=%5Chat%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to it:

Let ![P = \\{ p_1, \\dots, p_m
\\}](https://s0.wp.com/latex.php?latex=P+%3D+%5C%7B+p_1%2C+%5Cdots%2C+p_m+%5C%7D&bg=ffffff&fg=333333&s=0&c=20201002)
be a set of points in
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002),
and let
![C\(P\)](https://s0.wp.com/latex.php?latex=C%28P%29&bg=ffffff&fg=333333&s=0&c=20201002)
denote the [convex hull](https://en.wikipedia.org/wiki/Convex_hull) of
![P](https://s0.wp.com/latex.php?latex=P&bg=ffffff&fg=333333&s=0&c=20201002).
We are often interested in finding the point in
![C\(P\)](https://s0.wp.com/latex.php?latex=C%28P%29&bg=ffffff&fg=333333&s=0&c=20201002)
that is nearest to some reference point
![\\hat{x}](https://s0.wp.com/latex.php?latex=%5Chat%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002),
known as the Euclidean projection onto
![C\(P\)](https://s0.wp.com/latex.php?latex=C%28P%29&bg=ffffff&fg=333333&s=0&c=20201002).
Let’s denote this point by
![\\text{proj}_{C\(P\)}\(\\hat{x}\)](https://s0.wp.com/latex.php?latex=%5Ctext%7Bproj%7D_%7BC%28P%29%7D%28%5Chat%7Bx%7D%29&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F12%2F10%2Foptimality-condition-for-euclidean-projection-onto-a-convex-polytope%2F&signup_flow=account)

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

Rearranging the condition above gives us ![x^\\top \(p_j - x\) \\geq
0](https://s0.wp.com/latex.php?latex=x%5E%5Ctop+%28p_j+-+x%29+%5Cgeq+0&bg=ffffff&fg=333333&s=0&c=20201002).
Fixing
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002),
note that ![x^\\top \(y - x\) =
0](https://s0.wp.com/latex.php?latex=x%5E%5Ctop+%28y+-+x%29+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002)
represents the hyperplane passing through
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
which is orthogonal to the line segment going from
![0](https://s0.wp.com/latex.php?latex=0&bg=ffffff&fg=333333&s=0&c=20201002)
to
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002).
Hence, the condition ![x^\\top \(p_j - x\) \\geq
0](https://s0.wp.com/latex.php?latex=x%5E%5Ctop+%28p_j+-+x%29+%5Cgeq+0&bg=ffffff&fg=333333&s=0&c=20201002)
means that the directed line segment from
![0](https://s0.wp.com/latex.php?latex=0&bg=ffffff&fg=333333&s=0&c=20201002)
to
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
and the directed line segment from
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
to
![p_j](https://s0.wp.com/latex.php?latex=p_j&bg=ffffff&fg=333333&s=0&c=20201002)
make an angle that is ![\\leq
90^\\circ](https://s0.wp.com/latex.php?latex=%5Cleq+90%5E%5Ccirc&bg=ffffff&fg=333333&s=0&c=20201002).
Geometrically, this means that
![p_j](https://s0.wp.com/latex.php?latex=p_j&bg=ffffff&fg=333333&s=0&c=20201002)
and
![0](https://s0.wp.com/latex.php?latex=0&bg=ffffff&fg=333333&s=0&c=20201002)
lie on different sides of the hyperplane ![x^\\top \(y - x\) =
0](https://s0.wp.com/latex.php?latex=x%5E%5Ctop+%28y+-+x%29+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002),
and so
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
is nearer to
![0](https://s0.wp.com/latex.php?latex=0&bg=ffffff&fg=333333&s=0&c=20201002)
than
![p_j](https://s0.wp.com/latex.php?latex=p_j&bg=ffffff&fg=333333&s=0&c=20201002).

> **Theorem.** Let
> ![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
> be a point in
> ![C\(P\)](https://s0.wp.com/latex.php?latex=C%28P%29&bg=ffffff&fg=333333&s=0&c=20201002).
> ![x =
> \\text{proj}_{C\(P\)}\(0\)](https://s0.wp.com/latex.php?latex=x+%3D+%5Ctext%7Bproj%7D_%7BC%28P%29%7D%280%29&bg=ffffff&fg=333333&s=0&c=20201002)
> if and only if
>
> ![\\begin{aligned} x^\\top p_j \\geq \\| x \\|^2 \\quad \\text{for all } j =
> 1, \\dots, m.
> \\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+x%5E%5Ctop+p_j+%5Cgeq+%5C%7C+x+%5C%7C%5E2+%5Cquad+%5Ctext%7Bfor+all+%7D+j+%3D+1%2C+%5Cdots%2C+m.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Wolfe (1976) (Reference 1) provides an optimality condition for
![\\text{proj}_{C\(P\)}\(\\hat{x}\)](https://s0.wp.com/latex.php?latex=%5Ctext%7Bproj%7D_%7BC%28P%29%7D%28%5Chat%7Bx%7D%29&bg=ffffff&fg=333333&s=0&c=20201002)
when the reference point is the origin, i.e. ![\\hat{x} =
0](https://s0.wp.com/latex.php?latex=%5Chat%7Bx%7D+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002).

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-in-optimization%2F&signup_flow=account)

A big drawback of Newton’s method is that computing the Hessian can be very
expensive. _**Quasi-Newton methods**_ address this problem: update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
with

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This last equation is known as the _**secant equation**_ , and is sometimes
written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Note that this equation involves
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations in ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, so there is a lot of freedom in how to choose the
![B_k](https://s0.wp.com/latex.php?latex=B_k&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
On top of satisfying the secant equation, we often require 3 other conditions:

That is, we (i) compute the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
at
![x_k](https://s0.wp.com/latex.php?latex=x_k&bg=ffffff&fg=333333&s=0&c=20201002),
(ii) solve for
![p_k](https://s0.wp.com/latex.php?latex=p_k&bg=ffffff&fg=333333&s=0&c=20201002),
then (iii) update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
according to the second line.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [secant
equation](https://statisticaloddsandends.wordpress.com/tag/secant-equation/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-in-optimization/ "Permalink to Quasi-Newton methods
in optimization").

![\\begin{aligned} B_k p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1} &= x_k + t_k
p_k,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_k+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

where ![f : \\mathbb{R}^n \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f+%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is twice differentiable and ![\\text{dom}\(f\) =
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Ctext%7Bdom%7D%28f%29+%3D+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**Newton’s
method**_](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is
a second-order descent method for finding the minimum. Starting at some
initial point, at the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration we update the candidate solution with the formula

Quasi-Newton methods include SR1, DFP, BFGS and L-BFGS, which I hope to
describe in future posts.

![\\begin{aligned} \\nabla^2 f\(x_k\) p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1}
&= x_k + t_k p_k.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cnabla%5E2+f%28x_k%29+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
according to some procedure so that the secant equation (and other conditions)
are satisfied.

Reblogged this on [Statistical Methods, Paradigms and
Application](https://ibiloyeacresearch.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/) and commented:  
Interesting and Really good explanation here:

where ![\\nabla
f](https://s0.wp.com/latex.php?latex=%5Cnabla+f&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\\nabla^2
f](https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+f&bg=ffffff&fg=333333&s=0&c=20201002)
are the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
respectively, and
![t_k](https://s0.wp.com/latex.php?latex=t_k&bg=ffffff&fg=333333&s=0&c=20201002)
is a step size chosen appropriately (e.g. with [backtracking line
search](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-line-
search-and-backtracking-line-search/)). Another way to write the update is

Recall from the quasi-Newton updates that we need to solve the equation ![B p
= - \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=B+p+%3D+-+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002)
for
![p](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=333333&s=0&c=20201002),
which means that we need to compute the inverse ![H =
B^{-1}](https://s0.wp.com/latex.php?latex=H+%3D+B%5E%7B-1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to get ![p = - H \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=p+%3D+-+H+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002).
Because of the [Sherman-Morrison-Woodbury
formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we
can use the update to
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
to derive an update for
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002):

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Because this is a system of
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations with ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, there are many ways to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).

One way to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
easily is to assume that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix, i.e.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [SR1](https://statisticaloddsandends.wordpress.com/tag/sr1/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-sr1/ "Permalink to Quasi-Newton methods: SR1").

This is known as the _**SR1 update**_. SR1 is simple to understand and
implement, but has two shortcomings. First, the denominator in the update for
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
![\(y - Bs\)^\\top
s](https://s0.wp.com/latex.php?latex=%28y+-+Bs%29%5E%5Ctop+s&bg=ffffff&fg=333333&s=0&c=20201002),
might be approximately zero, causing numerical issues. Second, it does not
preserve positive definiteness.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-sr1%2F&signup_flow=account)

which implies that
![u](https://s0.wp.com/latex.php?latex=u&bg=ffffff&fg=333333&s=0&c=20201002)
must be a multiple of ![y -
Bs](https://s0.wp.com/latex.php?latex=y+-+Bs&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this into the equation and solving for
![a](https://s0.wp.com/latex.php?latex=a&bg=ffffff&fg=333333&s=0&c=20201002),
we obtain the update

for some ![a \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![u \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this update into the secant equation and rearranging, we get

[![](https://statisticaloddsandends.files.wordpress.com/2022/11/backtracking-
line-
search.png?w=584&h=327)](https://statisticaloddsandends.files.wordpress.com/2022/11/backtracking-
line-search.png)This is the same picture as that for exact line search, except
we’ve added the line ![y = f\(x\) + \\alpha t \\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=y+%3D+f%28x%29+%2B+%5Calpha+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002).
This line always has negative slope: because ![\\Delta
x](https://s0.wp.com/latex.php?latex=%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002)
is a descent direction, we must have ![\\nabla f\(x\)^\\top \\Delta x <
0](https://s0.wp.com/latex.php?latex=%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x+%3C+0&bg=ffffff&fg=333333&s=0&c=20201002).
The stopping condition, ![f\(x + t\\Delta x\) \\leq f\(x\) + \\alpha t \\nabla
f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=f%28x+%2B+t%5CDelta+x%29+%5Cleq+f%28x%29+%2B+%5Calpha+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002),
holds for the blue segment ![\[0,
t_0\]](https://s0.wp.com/latex.php?latex=%5B0%2C+t_0%5D&bg=ffffff&fg=333333&s=0&c=20201002).
The operation ![t \\leftarrow \\beta
t](https://s0.wp.com/latex.php?latex=t+%5Cleftarrow+%5Cbeta+t&bg=ffffff&fg=333333&s=0&c=20201002)
makes the step size smaller and smaller until
![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=333333&s=0&c=20201002)
falls in the blue segment, and by the way it’s set up, we know that the
objective function will have a strictly smaller value.

In other words, we keep reducing the step size by the same multiplicative
factor until the ![f\(x + t\\Delta x\) \\leq f\(x\) + \\alpha t \\nabla
f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=f%28x+%2B+t%5CDelta+x%29+%5Cleq+f%28x%29+%2B+%5Calpha+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002).

( _ **Note:**_ The last diagram is taken from Reference 1, and the other
diagrams are this base + my annotations.)

While ![f\(x + t\\Delta x\) > f\(x\) + \\alpha t \\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=f%28x+%2B+t%5CDelta+x%29+%3E+f%28x%29+%2B+%5Calpha+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002),
set ![t \\leftarrow \\beta
t](https://s0.wp.com/latex.php?latex=t+%5Cleftarrow+%5Cbeta+t&bg=ffffff&fg=333333&s=0&c=20201002).

There are several ways to choose a descent direction (see [this previous
post](https://statisticaloddsandends.wordpress.com/2021/04/29/what-is-
steepest-descent/) on steepest descent for some examples). This post is about
two ways to choose the step size. This step is called a _**line search**_
because the step size determines where along the line ![\\{x + t \\Delta x : t
\\geq 0
\\}](https://s0.wp.com/latex.php?latex=%5C%7Bx+%2B+t+%5CDelta+x+%3A+t+%5Cgeq+0+%5C%7D&bg=ffffff&fg=333333&s=0&c=20201002)
the next iterate will be.

Since exact line search requires solving a minimization problem (albeit in one
dimension), it can be expensively unless
![t^*](https://s0.wp.com/latex.php?latex=t%5E%2A&bg=ffffff&fg=333333&s=0&c=20201002)
can be computed efficiently (or even analytically).

How do we know that
![t_0](https://s0.wp.com/latex.php?latex=t_0&bg=ffffff&fg=333333&s=0&c=20201002)
is always ![>
0](https://s0.wp.com/latex.php?latex=%3E+0&bg=ffffff&fg=333333&s=0&c=20201002)
(i.e. the blue segment exists)? It’s because of how we chose the term
![\\alpha t \\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=%5Calpha+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002).
It can be shown that the derivative of ![f\(x + t \\Delta
x\)](https://s0.wp.com/latex.php?latex=f%28x+%2B+t+%5CDelta+x%29&bg=ffffff&fg=333333&s=0&c=20201002)
w.r.t.
![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=333333&s=0&c=20201002)
at the point ![t =
0](https://s0.wp.com/latex.php?latex=t+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002)
is ![\\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002).
Hence, the line ![y = f\(x\) + t \\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=y+%3D+f%28x%29+%2B+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002)
is tangent to ![y = f\(x + t \\nabla
x\)](https://s0.wp.com/latex.php?latex=y+%3D+f%28x+%2B+t+%5Cnabla+x%29&bg=ffffff&fg=333333&s=0&c=20201002)
at ![t =
0](https://s0.wp.com/latex.php?latex=t+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002).
Multiplying the term ![t \\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002)
by some ![0 < \\alpha <
1](https://s0.wp.com/latex.php?latex=0+%3C+%5Calpha+%3C+1&bg=ffffff&fg=333333&s=0&c=20201002)
ensures that the stopping condition will hold for some non-trivial segment
![\[0,
t_0\]](https://s0.wp.com/latex.php?latex=%5B0%2C+t_0%5D&bg=ffffff&fg=333333&s=0&c=20201002).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [backtracking line
search](https://statisticaloddsandends.wordpress.com/tag/backtracking-line-
search/), [convex](https://statisticaloddsandends.wordpress.com/tag/convex/),
[exact line search](https://statisticaloddsandends.wordpress.com/tag/exact-
line-search/), [line
search](https://statisticaloddsandends.wordpress.com/tag/line-search/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-
line-search-and-backtracking-line-search/ "Permalink to Exact line search and
backtracking line search").

_**Backtracking line search**_ uses a heuristic to quickly determine a step
size which decreases the objective function value by a certain amount. The
user needs to set two parameters, ![0 < \\alpha <
0.5](https://s0.wp.com/latex.php?latex=0+%3C+%5Calpha+%3C+0.5&bg=ffffff&fg=333333&s=0&c=20201002)
and ![0 < \\beta <
1](https://s0.wp.com/latex.php?latex=0+%3C+%5Cbeta+%3C+1&bg=ffffff&fg=333333&s=0&c=20201002).
In each iteration, after the descent direction ![\\Delta
x](https://s0.wp.com/latex.php?latex=%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002)
has been chosen, the step size is determine by the following:

[![](https://statisticaloddsandends.files.wordpress.com/2022/11/exact-line-
search.png?w=584&h=327)](https://statisticaloddsandends.files.wordpress.com/2022/11/exact-
line-search.png)The curve is the value of the function along the ray ![x + t
\\Delta
x](https://s0.wp.com/latex.php?latex=x+%2B+t+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002)
with ![t \\geq
0](https://s0.wp.com/latex.php?latex=t+%5Cgeq+0&bg=ffffff&fg=333333&s=0&c=20201002).
![t^*](https://s0.wp.com/latex.php?latex=t%5E%2A&bg=ffffff&fg=333333&s=0&c=20201002)
is the point where this function is minimized. It should be clear from the
description that this choice will result in smaller values of the function in
each iteration.

_**Exact line search**_ finds
![t^\\star](https://s0.wp.com/latex.php?latex=t%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002)
such that ![f\(x + t \\Delta
x\)](https://s0.wp.com/latex.php?latex=f%28x+%2B+t+%5CDelta+x%29&bg=ffffff&fg=333333&s=0&c=20201002)
attains its minimum value at ![t =
t^\\star](https://s0.wp.com/latex.php?latex=t+%3D+t%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002),
where we let
![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=333333&s=0&c=20201002)
range over non-negative values. The diagram below demonstrates this:

Assume we are trying to minimize some convex function ![f: \\mathbb{R}^n
\\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
which is differentiable. One way to do this is to use a _**descent method**_.
A general descent method can be described as follows: Given a starting point
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
in the domain of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002),
iterate over the following 3 steps until some stopping criterion is satisfied:

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F19%2Fexact-line-search-and-backtracking-line-search%2F&signup_flow=account)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [affine
hull](https://statisticaloddsandends.wordpress.com/tag/affine-hull/),
[convex](https://statisticaloddsandends.wordpress.com/tag/convex/), [convex
hull](https://statisticaloddsandends.wordpress.com/tag/convex-hull/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/18/affine-
hull-vs-convex-hull/ "Permalink to Affine hull vs. convex hull").

Let’s have a look at a few examples. If
![S](https://s0.wp.com/latex.php?latex=S&bg=ffffff&fg=333333&s=0&c=20201002)
consists of two points, the convex hull is the line segment joining them
(including the endpoints) while the affine hull is the entire line through
these two points:

The [_**affine hull**_](https://en.wikipedia.org/wiki/Affine_hull) and
[_**convex hull**_](https://en.wikipedia.org/wiki/Convex_hull) are closely
related concepts. Let
![S](https://s0.wp.com/latex.php?latex=S&bg=ffffff&fg=333333&s=0&c=20201002)
be a set in
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
The affine hull of
![S](https://s0.wp.com/latex.php?latex=S&bg=ffffff&fg=333333&s=0&c=20201002)
is the set of all _affine combinations_ of elements of
![S](https://s0.wp.com/latex.php?latex=S&bg=ffffff&fg=333333&s=0&c=20201002):

[![](https://statisticaloddsandends.files.wordpress.com/2022/11/2-2d.png?w=584&h=316)](https://statisticaloddsandends.files.wordpress.com/2022/11/2-2d.png)The
final illustration below shows the affine and convex hulls of 3 non-collinear
points in
![\\mathbb{R}^3](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E3&bg=ffffff&fg=333333&s=0&c=20201002).
They are both two-dimensional sets living in 3-dimensional space. Notice also
how the affine hull need not pass through the origin (whereas all subspaces of
![\\mathbb{R}^3](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E3&bg=ffffff&fg=333333&s=0&c=20201002)
must pass through it).

![\\begin{aligned} \\text{aff}\(S\) = \\left\\{ \\sum_{i=1}^k a_i x_i : k > 0,
x_i \\in S, a_i \\in \\mathbb{R}, \\sum_{i=1}^k a_i = 1 \\right\\}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext%7Baff%7D%28S%29+%3D+%5Cleft%5C%7B+%5Csum_%7Bi%3D1%7D%5Ek+a_i+x_i+%3A+k+%3E+0%2C+x_i+%5Cin+S%2C+a_i+%5Cin+%5Cmathbb%7BR%7D%2C+%5Csum_%7Bi%3D1%7D%5Ek+a_i+%3D+1+%5Cright%5C%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F18%2Faffine-hull-vs-convex-hull%2F&signup_flow=account)

[![](https://statisticaloddsandends.files.wordpress.com/2022/11/1-2d.png?w=584&h=297)](https://statisticaloddsandends.files.wordpress.com/2022/11/1-2d.png)For
3 non-collinear points in two dimensions, the convex hull is the triangle with
these 3 points as vertices while the affine hull is the entire plane
![\\mathbb{R}^2](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E2&bg=ffffff&fg=333333&s=0&c=20201002):

Putting the definitions side by side, we see that the only difference is that
for the convex hull, the weights in the linear combination (the
![a_i](https://s0.wp.com/latex.php?latex=a_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s)
have an additional restriction of being non-negative. The only restriction on
the
![a_i](https://s0.wp.com/latex.php?latex=a_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
for combinations in the affine hull is that they sum to 1. This also means
that the affine hull always contains the convex hull.

![\\begin{aligned} \\text{conv}\(S\) = \\left\\{ \\sum_{i=1}^k a_i x_i : k >
0, x_i \\in S, a_i \\in \\mathbb{R}, a_i \\geq 0, \\sum_{i=1}^k a_i = 1
\\right\\}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext%7Bconv%7D%28S%29+%3D+%5Cleft%5C%7B+%5Csum_%7Bi%3D1%7D%5Ek+a_i+x_i+%3A+k+%3E+0%2C+x_i+%5Cin+S%2C+a_i+%5Cin+%5Cmathbb%7BR%7D%2C+a_i+%5Cgeq+0%2C+%5Csum_%7Bi%3D1%7D%5Ek+a_i+%3D+1+%5Cright%5C%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

The convex hull of
![S](https://s0.wp.com/latex.php?latex=S&bg=ffffff&fg=333333&s=0&c=20201002)
is the set of all _convex combinations_ of the elements of
![S](https://s0.wp.com/latex.php?latex=S&bg=ffffff&fg=333333&s=0&c=20201002):

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2020%2F04%2F07%2Fgit-references%2F&signup_flow=account)

**[How to merge a branch into master.](https://stackabuse.com/git-merge-
branch-into-master/)** This deals with the simple case when there are no merge
conflicts.

**[How to add tags to certain commits.](https://git-scm.com/book/en/v2/Git-
Basics-Tagging)** Helpful for keeping track of versions of the repo.

( _ **Note:**_ This post will be updated periodically as I find other useful
idioms that I keep forgetting.)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [git](https://statisticaloddsandends.wordpress.com/tag/git/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2020/04/07/git-
references/ "Permalink to Git references").

[**How to push changes on a local branch to the remote version of the
branch.**](https://github.com/Kunena/Kunena-Forum/wiki/Create-a-new-branch-
with-git-and-manage-branches) This includes how to delete local and remote
branches.

_**Why am I not simply posting the git commands?**_ That’s because the correct
git command depends a lot on the context. By linking to the full reference,
you can check if the situation the reference is addressing matches yours
before applying its solution.

[**How to amend a commit
message.**](https://help.github.com/en/github/committing-changes-to-your-
project/changing-a-commit-message) This reference also deals with the case
where you want to amend messages for older commits, or if you want to amend
messages for commits that have been pushed to a remote repository.

[**How to see html on Github as a rendered HTML
page:**](https://stackoverflow.com/questions/8446218/how-to-see-an-html-page-
on-github-as-a-normal-rendered-html-page-to-see-preview) Go to
<https://htmlpreview.github.io/> and put in the Github html.

[**_Git_**](https://git-scm.com/) is an essential distributed version control
system for managing programming projects (and in my case, maintaining R
packages). While essential, I find it quite difficult to remember common git
idioms for simple operations. This post lists references that I personally
find useful: hopefully some of you might find them useful too.

[**How to “squash” multiple commits into a single
commit.**](https://www.internalpointers.com/post/squash-commits-into-one-git)
This is especially useful for removing small, intermediate checkpoints that
help during development but clutter up the commit history. See [this
link](https://thoughtbot.com/blog/git-interactive-rebase-squash-amend-
rewriting-history) for more interactive rebase options.

_(Sufficiency)_ For a convex optimization problem (i.e.
![f_i](https://s0.wp.com/latex.php?latex=f_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
convex and
![h_i](https://s0.wp.com/latex.php?latex=h_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
affine), if the KKT conditions hold for some ![\(\\tilde{x},
\\tilde{\\lambda},
\\tilde{\\nu}\)](https://s0.wp.com/latex.php?latex=%28%5Ctilde%7Bx%7D%2C+%5Ctilde%7B%5Clambda%7D%2C+%5Ctilde%7B%5Cnu%7D%29&bg=ffffff&fg=333333&s=0&c=20201002),
then
![\\tilde{x}](https://s0.wp.com/latex.php?latex=%5Ctilde%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\(\\tilde{\\lambda},
\\tilde{\\nu}\)](https://s0.wp.com/latex.php?latex=%28%5Ctilde%7B%5Clambda%7D%2C+%5Ctilde%7B%5Cnu%7D%29&bg=ffffff&fg=333333&s=0&c=20201002)
are primal and dual optimal, and there is zero duality gap.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F15%2Fwhat-are-the-kkt-conditions%2F&signup_flow=account)

with the variable ![x \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Assume that the
![f_i](https://s0.wp.com/latex.php?latex=f_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
and
![h_i](https://s0.wp.com/latex.php?latex=h_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
are differentiable. (At this point, we are not assuming anything about their
convexity.)

![\\begin{aligned} \\text{minimize} \\quad& f_0\(x\) \\\\  \\text{subject to}
\\quad& f_i\(x\) \\leq 0, \\quad i = 1, \\dots, m, \\\\  & h_i\(x\) = 0,
\\quad i = 1, \\dots, p,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext%7Bminimize%7D+%5Cquad%26+f_0%28x%29+%5C%5C++%5Ctext%7Bsubject+to%7D+%5Cquad%26+f_i%28x%29+%5Cleq+0%2C+%5Cquad+i+%3D+1%2C+%5Cdots%2C+m%2C+%5C%5C++%26+h_i%28x%29+%3D+0%2C+%5Cquad+i+%3D+1%2C+%5Cdots%2C+p%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [convex](https://statisticaloddsandends.wordpress.com/tag/convex/),
[kkt conditions](https://statisticaloddsandends.wordpress.com/tag/kkt-
conditions/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[strong duality](https://statisticaloddsandends.wordpress.com/tag/strong-
duality/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/15/what-are-
the-kkt-conditions/ "Permalink to What are the KKT conditions?").

_Primal feasibility :_ ![f_i\(x^\\star\) \\leq
0](https://s0.wp.com/latex.php?latex=f_i%28x%5E%5Cstar%29+%5Cleq+0&bg=ffffff&fg=333333&s=0&c=20201002)
for ![i = 1, \\dots,
m](https://s0.wp.com/latex.php?latex=i+%3D+1%2C+%5Cdots%2C+m&bg=ffffff&fg=333333&s=0&c=20201002),
and ![h_i\(x^\\star\) =
0](https://s0.wp.com/latex.php?latex=h_i%28x%5E%5Cstar%29+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002)
for ![i = 1, \\dots,
p](https://s0.wp.com/latex.php?latex=i+%3D+1%2C+%5Cdots%2C+p&bg=ffffff&fg=333333&s=0&c=20201002).

_**Note:**_ There are more general versions of the KKT conditions; see
Reference 2 for details.

Here are 3 statements (theorems, if you will) involving the KKT conditions
which highlight their importance:

Let
![x^\\star](https://s0.wp.com/latex.php?latex=x%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\(\\lambda^\\star,
\\nu^\\star\)](https://s0.wp.com/latex.php?latex=%28%5Clambda%5E%5Cstar%2C+%5Cnu%5E%5Cstar%29&bg=ffffff&fg=333333&s=0&c=20201002)
be the primal and dual optimal points respectively (i.e. points where the
primal and dual problems are optimized). The _**Karush-Kuhn-Tucker (KKT)**_
conditions refer to the following set of 4 assumptions:

![\\begin{aligned} L\(x, \\lambda, \\nu\) = f_0\(x\) + \\sum_{i=1}^m
\\lambda_i f_i\(x\) + \\sum_{i=1}^p \\nu_i h_i\(x\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+L%28x%2C+%5Clambda%2C+%5Cnu%29+%3D+f_0%28x%29+%2B+%5Csum_%7Bi%3D1%7D%5Em+%5Clambda_i+f_i%28x%29+%2B+%5Csum_%7Bi%3D1%7D%5Ep+%5Cnu_i+h_i%28x%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

_(Necessity)_ For any optimization problem for which [strong
duality](https://statisticaloddsandends.wordpress.com/2022/11/12/lagrange-
dual-weak-duality-and-strong-duality/) holds, any pair of primal and dual
optimal points must satisfy the KKT conditions.

_Stationarity :_ ![\\partial_x L\(x^\\star, \\lambda^\\star, \\nu^\\star\) =
0](https://s0.wp.com/latex.php?latex=%5Cpartial_x+L%28x%5E%5Cstar%2C+%5Clambda%5E%5Cstar%2C+%5Cnu%5E%5Cstar%29+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002),
i.e. ![\\nabla f_0\(x^\\star\) + \\displaystyle\\sum_{i=1}^m \\lambda_i^\\star
\\nabla f_i\(x^\\star\) + \\displaystyle\\sum_{i=1}^p \\nu_i^\\star \\nabla
h_i\(x^\\star\) =
0](https://s0.wp.com/latex.php?latex=%5Cnabla+f_0%28x%5E%5Cstar%29+%2B+%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5Em+%5Clambda_i%5E%5Cstar+%5Cnabla+f_i%28x%5E%5Cstar%29+%2B+%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5Ep+%5Cnu_i%5E%5Cstar+%5Cnabla+h_i%28x%5E%5Cstar%29+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002).

_(Necessity & sufficiency)_ For a convex optimization problem that satisfies
[Slater’s
condition](https://statisticaloddsandends.wordpress.com/2022/11/12/lagrange-
dual-weak-duality-and-strong-duality/), then
![\\tilde{x}](https://s0.wp.com/latex.php?latex=%5Ctilde%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is primal optimal if and only if there exists ![\(\\tilde{\\lambda},
\\tilde{\\nu}\)](https://s0.wp.com/latex.php?latex=%28%5Ctilde%7B%5Clambda%7D%2C+%5Ctilde%7B%5Cnu%7D%29&bg=ffffff&fg=333333&s=0&c=20201002)
such that the KKT conditions are satisfied.

where the inequality above is understood to be element-wise. Let
![d^\\star](https://s0.wp.com/latex.php?latex=d%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002)
denote the optimal value of this problem.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F12%2Flagrange-dual-weak-duality-and-strong-duality%2F&signup_flow=account)

The _**Lagrange dual function**_ is the function ![g: \\mathbb{R}^m \\times
\\mathbb{R}^p \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=g%3A+%5Cmathbb%7BR%7D%5Em+%5Ctimes+%5Cmathbb%7BR%7D%5Ep+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
defined as the minimum value of the Lagrangian over
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002):

The Lagrange dual problem is _always_ a convex optimization problem, because
the objective being maximized is concave (see [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/11/the-lagrange-
dual-function-is-always-concave/) for a proof) and the constraint is convex.
One reason the dual problem is of critical importance is that it provides a
lower bound on
![p^\\star](https://s0.wp.com/latex.php?latex=p%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002)
(it is the best lower bound that can be obtained from the Lagrange dual
function) and in some cases, gives the value of
![p^\\star](https://s0.wp.com/latex.php?latex=p%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002).

Since the above holds for any ![\\lambda \\geq
0](https://s0.wp.com/latex.php?latex=%5Clambda+%5Cgeq+0&bg=ffffff&fg=333333&s=0&c=20201002)
and any
![\\nu](https://s0.wp.com/latex.php?latex=%5Cnu&bg=ffffff&fg=333333&s=0&c=20201002),
maximizing over these variables gives us ![f_0\(\\tilde{x}\) \\geq
d^\\star](https://s0.wp.com/latex.php?latex=f_0%28%5Ctilde%7Bx%7D%29+%5Cgeq+d%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002).
Since this holds for all feasible
![\\tilde{x}](https://s0.wp.com/latex.php?latex=%5Ctilde%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002),
minimizing over all feasible
![\\tilde{x}](https://s0.wp.com/latex.php?latex=%5Ctilde%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002)
gives us ![p^\\star \\geq
d^\\star](https://s0.wp.com/latex.php?latex=p%5E%5Cstar+%5Cgeq+d%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002).

_**Slater’s theorem**_ says that strongly duality holds if Slater’s condition
holds and the problem is convex. A proof of this can be found in Section 5.3.2
of Reference 1; see Section 2.1.3 of the same reference for more details on
the concept of relative interior.

![\\begin{aligned} f_0\(\\tilde{x}\) &\\geq f_0\(\\tilde{x}\) + \\sum_{i=1}^m
\\lambda_i f_i\(\\tilde{x}\) + \\sum_{i=1}^p \\nu_i h_i\(\\tilde{x}\) \\\\
&\\geq \\inf_{x \\in \\mathcal{D}} \\left\\{ f_0\(x\) + \\sum_{i=1}^m
\\lambda_i f_i\(x\) + \\sum_{i=1}^p \\nu_i h_i\(x\) \\right\\} \\\\  &=
g\(\\lambda, \\nu\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+f_0%28%5Ctilde%7Bx%7D%29+%26%5Cgeq+f_0%28%5Ctilde%7Bx%7D%29+%2B+%5Csum_%7Bi%3D1%7D%5Em+%5Clambda_i+f_i%28%5Ctilde%7Bx%7D%29+%2B+%5Csum_%7Bi%3D1%7D%5Ep+%5Cnu_i+h_i%28%5Ctilde%7Bx%7D%29+%5C%5C++%26%5Cgeq+%5Cinf_%7Bx+%5Cin+%5Cmathcal%7BD%7D%7D+%5Cleft%5C%7B+f_0%28x%29+%2B+%5Csum_%7Bi%3D1%7D%5Em+%5Clambda_i+f_i%28x%29+%2B+%5Csum_%7Bi%3D1%7D%5Ep+%5Cnu_i+h_i%28x%29+%5Cright%5C%7D+%5C%5C++%26%3D+g%28%5Clambda%2C+%5Cnu%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

![\\begin{aligned} g\(\\lambda, \\nu\) &= \\inf_{x \\in \\mathcal{D}} L\(x,
\\lambda, \\nu\) \\\\  &= \\inf_{x \\in \\mathcal{D}} \\left\\{ f_0\(x\) +
\\sum_{i=1}^m \\lambda_i f_i\(x\) + \\sum_{i=1}^p \\nu_i h_i\(x\) \\right\\}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+g%28%5Clambda%2C+%5Cnu%29+%26%3D+%5Cinf_%7Bx+%5Cin+%5Cmathcal%7BD%7D%7D+L%28x%2C+%5Clambda%2C+%5Cnu%29+%5C%5C++%26%3D+%5Cinf_%7Bx+%5Cin+%5Cmathcal%7BD%7D%7D+%5Cleft%5C%7B+f_0%28x%29+%2B+%5Csum_%7Bi%3D1%7D%5Em+%5Clambda_i+f_i%28x%29+%2B+%5Csum_%7Bi%3D1%7D%5Ep+%5Cnu_i+h_i%28x%29+%5Cright%5C%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

with ![f_0, \\dots,
f_m](https://s0.wp.com/latex.php?latex=f_0%2C+%5Cdots%2C+f_m&bg=ffffff&fg=333333&s=0&c=20201002)
being convex functions. Convex optimization problems don’t always have strong
duality, but there are some simple constraint qualifications under which
strong duality holds. The most commonly cited one is _**Slater’s condition**_
, which seems fairly easy to satisfy.

Slater’s condition states that the problem admits a _strictly feasible_ point,
that is, a point
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
in the [relative interior](https://en.wikipedia.org/wiki/Relative_interior) of
![\\mathcal{D}](https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&bg=ffffff&fg=333333&s=0&c=20201002)
(the domain of the problem) such that

![\\begin{aligned} \\text{minimize} \\quad& f_0\(x\) \\\\  \\text{subject to}
\\quad& f_i\(x\) \\leq 0, \\quad i = 1, \\dots, m, \\\\  & Ax = b,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext%7Bminimize%7D+%5Cquad%26+f_0%28x%29+%5C%5C++%5Ctext%7Bsubject+to%7D+%5Cquad%26+f_i%28x%29+%5Cleq+0%2C+%5Cquad+i+%3D+1%2C+%5Cdots%2C+m%2C+%5C%5C++%26+Ax+%3D+b%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

![\\begin{aligned} f_i\(x\) < 0, \\quad i = 1, \\dots, m, \\quad \\text{and}
\\quad Ax = b.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+f_i%28x%29+%3C+0%2C+%5Cquad+i+%3D+1%2C+%5Cdots%2C+m%2C+%5Cquad+%5Ctext%7Band%7D+%5Cquad+Ax+%3D+b.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

  1. Pingback: [What are the KKT conditions? | Statistical Odds & Ends](https://statisticaloddsandends.wordpress.com/2022/11/15/what-are-the-kkt-conditions/)

with the variable ![x \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Let
![\\mathcal{D}](https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&bg=ffffff&fg=333333&s=0&c=20201002)
be the domain for
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002),
i.e. the intersection of the domains of the
![f_i](https://s0.wp.com/latex.php?latex=f_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
and the
![h_i](https://s0.wp.com/latex.php?latex=h_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
Let
![p^\\star](https://s0.wp.com/latex.php?latex=p%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002)
denote the optimal value of the problem.

_**Strong duality**_ refers to the property that ![d^\\star =
p^\\star](https://s0.wp.com/latex.php?latex=d%5E%5Cstar+%3D+p%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002).
This does not always hold, but it can hold for important classes of problems.
Conditions which guarantee strong duality are known as _**constraint
qualifications**_.

This inequality holds all the time, even when the original problem is not
convex. The proof is straightforward. If
![\\tilde{x}](https://s0.wp.com/latex.php?latex=%5Ctilde%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is feasible for the original problem, then for any ![\\lambda \\geq
0](https://s0.wp.com/latex.php?latex=%5Clambda+%5Cgeq+0&bg=ffffff&fg=333333&s=0&c=20201002)
and any
![\\nu](https://s0.wp.com/latex.php?latex=%5Cnu&bg=ffffff&fg=333333&s=0&c=20201002)
we have

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [convex](https://statisticaloddsandends.wordpress.com/tag/convex/),
[duality](https://statisticaloddsandends.wordpress.com/tag/duality/),
[lagrange dual](https://statisticaloddsandends.wordpress.com/tag/lagrange-
dual/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[slater's condition](https://statisticaloddsandends.wordpress.com/tag/slaters-
condition/), [strong
duality](https://statisticaloddsandends.wordpress.com/tag/strong-duality/),
[weak duality](https://statisticaloddsandends.wordpress.com/tag/weak-duality/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/12/lagrange-
dual-weak-duality-and-strong-duality/ "Permalink to Lagrange dual, weak
duality and strong duality").

Here’s a more explicit version of the proof. Let ![\\theta = \(\\lambda,
\\nu\) \\in \\mathbb{R}^{m +
p}](https://s0.wp.com/latex.php?latex=%5Ctheta+%3D+%28%5Clambda%2C+%5Cnu%29+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm+%2B+p%7D&bg=ffffff&fg=333333&s=0&c=20201002).
Fix ![\\theta_1, \\theta_2 \\in \\mathbb{R}^{m +
p}](https://s0.wp.com/latex.php?latex=%5Ctheta_1%2C+%5Ctheta_2+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm+%2B+p%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![t \\in \[0,
1\]](https://s0.wp.com/latex.php?latex=t+%5Cin+%5B0%2C+1%5D&bg=ffffff&fg=333333&s=0&c=20201002).
For each ![x \\in
\\mathcal{D}](https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathcal%7BD%7D&bg=ffffff&fg=333333&s=0&c=20201002),

![\\begin{aligned} L\(x, t\\theta_1 + \(1-t\)\\theta_2\) &= t L\(x,
\\theta_1\) + \(1-t\) L\(x, \\theta_2\) \\\\  &\\geq t g \(\\theta_1\) +
\(1-t\) g\(\\theta_2\),
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+L%28x%2C+t%5Ctheta_1+%2B+%281-t%29%5Ctheta_2%29+%26%3D+t+L%28x%2C+%5Ctheta_1%29+%2B+%281-t%29+L%28x%2C+%5Ctheta_2%29+%5C%5C++%26%5Cgeq+t+g+%28%5Ctheta_1%29+%2B+%281-t%29+g%28%5Ctheta_2%29%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[concave](https://statisticaloddsandends.wordpress.com/tag/concave/),
[lagrange dual](https://statisticaloddsandends.wordpress.com/tag/lagrange-
dual/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/11/the-
lagrange-dual-function-is-always-concave/ "Permalink to The Lagrange dual
function is always concave").

The dual function is a very important function in optimization theory. _**One
interesting fact about the dual function is that it is always concave, even if
the original optimization problem is not convex.**_ The proof can be stated in
one-line: for each
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002),
![L\(x, \\lambda,
\\nu\)](https://s0.wp.com/latex.php?latex=L%28x%2C+%5Clambda%2C+%5Cnu%29&bg=ffffff&fg=333333&s=0&c=20201002)
is an affine function, and the point-wise infimum of a family of affine
functions is always concave.

where the first equality uses the definition of
![L](https://s0.wp.com/latex.php?latex=L&bg=ffffff&fg=333333&s=0&c=20201002)
(or the fact that
![L](https://s0.wp.com/latex.php?latex=L&bg=ffffff&fg=333333&s=0&c=20201002)
is affine in
![\\theta](https://s0.wp.com/latex.php?latex=%5Ctheta&bg=ffffff&fg=333333&s=0&c=20201002)).
Since this holds for all ![x \\in
\\mathcal{D}](https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathcal%7BD%7D&bg=ffffff&fg=333333&s=0&c=20201002),
we can take an infimum over
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
to obtain ![g\(t\\theta_1 + \(1-t\)\\theta_2\) \\geq t g \(\\theta_1\) +
\(1-t\)
g\(\\theta_2\)](https://s0.wp.com/latex.php?latex=g%28t%5Ctheta_1+%2B+%281-t%29%5Ctheta_2%29+%5Cgeq+t+g+%28%5Ctheta_1%29+%2B+%281-t%29+g%28%5Ctheta_2%29&bg=ffffff&fg=333333&s=0&c=20201002),
i.e.
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
is concave.

  1. Pingback: [Lagrange dual, weak duality and strong duality | Statistical Odds & Ends](https://statisticaloddsandends.wordpress.com/2022/11/12/lagrange-dual-weak-duality-and-strong-duality/)

The _**Lagrangian**_ associated with this problem is the function ![L:
\\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb{R}^p \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=L%3A+%5Cmathbb%7BR%7D%5En+%5Ctimes+%5Cmathbb%7BR%7D%5Em+%5Ctimes+%5Cmathbb%7BR%7D%5Ep+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
defined as

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F11%2Fthe-lagrange-dual-function-is-always-concave%2F&signup_flow=account)

with the variable ![x \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Let
![\\mathcal{D}](https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&bg=ffffff&fg=333333&s=0&c=20201002)
be the domain for
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002),
i.e. the intersection of the domains of the
![f_i](https://s0.wp.com/latex.php?latex=f_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
and the
![h_i](https://s0.wp.com/latex.php?latex=h_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s.

![\\begin{aligned} L\(x, \\lambda, \\nu\) = f_0\(x\) + \\sum_{i=1}^m
\\lambda_i f_i\(x\) + \\sum_{i=1}^p \\nu_i h_i\(x\),
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+L%28x%2C+%5Clambda%2C+%5Cnu%29+%3D+f_0%28x%29+%2B+%5Csum_%7Bi%3D1%7D%5Em+%5Clambda_i+f_i%28x%29+%2B+%5Csum_%7Bi%3D1%7D%5Ep+%5Cnu_i+h_i%28x%29%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

with domain ![\\mathcal{L} = \\mathcal{D} \\times \\mathbb{R}^m \\times
\\mathbb{R}^p](https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D+%3D+%5Cmathcal%7BD%7D+%5Ctimes+%5Cmathbb%7BR%7D%5Em+%5Ctimes+%5Cmathbb%7BR%7D%5Ep&bg=ffffff&fg=333333&s=0&c=20201002).
The _**Lagrange dual function**_ is the function ![g: \\mathbb{R}^m \\times
\\mathbb{R}^p \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=g%3A+%5Cmathbb%7BR%7D%5Em+%5Ctimes+%5Cmathbb%7BR%7D%5Ep+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
defined as the minimum value of the Lagrangian over
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002):

The column space of an ![m \\times
n](https://s0.wp.com/latex.php?latex=m+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002)
can be expressed as the set ![\\{ \\mathbf{A}x : x \\in \\mathbb{R}^m
\\}](https://s0.wp.com/latex.php?latex=%5C%7B+%5Cmathbf%7BA%7Dx+%3A+x+%5Cin+%5Cmathbb%7BR%7D%5Em+%5C%7D&bg=ffffff&fg=333333&s=0&c=20201002).
We can also write ![\\mathbf{A}x = \\displaystyle\\sum_{i = 1}^m x_i
A_i](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7Dx+%3D+%5Cdisplaystyle%5Csum_%7Bi+%3D+1%7D%5Em+x_i+A_i&bg=ffffff&fg=333333&s=0&c=20201002),
where
![A_i](https://s0.wp.com/latex.php?latex=A_i&bg=ffffff&fg=333333&s=0&c=20201002)
is the
![i](https://s0.wp.com/latex.php?latex=i&bg=ffffff&fg=333333&s=0&c=20201002)th
column of
![A](https://s0.wp.com/latex.php?latex=A&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F03%2F27%2Fmatrix-expansions%2F&signup_flow=account)

If ![\\mathbf{A} \\in \\mathbb{R}^{m \\times
n}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm+%5Ctimes+n%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and we want a matrix
![\\mathbf{B}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BB%7D&bg=ffffff&fg=333333&s=0&c=20201002)
whose
![i](https://s0.wp.com/latex.php?latex=i&bg=ffffff&fg=333333&s=0&c=20201002)th
row is
![c_i](https://s0.wp.com/latex.php?latex=c_i&bg=ffffff&fg=333333&s=0&c=20201002)
multiplied by the
![i](https://s0.wp.com/latex.php?latex=i&bg=ffffff&fg=333333&s=0&c=20201002)th
row of
![\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002),
then ![\\mathbf{B} = \\text{diag} \(c_1, \\dots, c_n\)
\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BB%7D+%3D+%5Ctext%7Bdiag%7D+%28c_1%2C+%5Cdots%2C+c_n%29+%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002).  
If we want a matrix
![\\mathbf{B}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BB%7D&bg=ffffff&fg=333333&s=0&c=20201002)
whose
![i](https://s0.wp.com/latex.php?latex=i&bg=ffffff&fg=333333&s=0&c=20201002)th
column is
![c_i](https://s0.wp.com/latex.php?latex=c_i&bg=ffffff&fg=333333&s=0&c=20201002)
multiplied by the
![i](https://s0.wp.com/latex.php?latex=i&bg=ffffff&fg=333333&s=0&c=20201002)th
column of
![\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002),
then ![\\mathbf{B} =  \\mathbf{A}\\text{diag} \(c_1, \\dots,
c_n\)](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BB%7D+%3D%C2%A0+%5Cmathbf%7BA%7D%5Ctext%7Bdiag%7D+%28c_1%2C+%5Cdots%2C+c_n%29&bg=ffffff&fg=333333&s=0&c=20201002).

If ![\\mathbf{A} \\in \\mathbb{R}^{m \\times
n}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm+%5Ctimes+n%7D&bg=ffffff&fg=333333&s=0&c=20201002)
has columns ![A_1, \\dots, A_n \\in \\mathbb{R}^{m \\times
1}](https://s0.wp.com/latex.php?latex=A_1%2C+%5Cdots%2C+A_n+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm+%5Ctimes+1%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![\\mathbf{B} \\in \\mathbb{R}^{n \\times
p}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BB%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn+%5Ctimes+p%7D&bg=ffffff&fg=333333&s=0&c=20201002)
has rows ![B_1, \\dots, B_n \\in \\mathbb{R}^{1 \\times
n}](https://s0.wp.com/latex.php?latex=B_1%2C+%5Cdots%2C+B_n+%5Cin+%5Cmathbb%7BR%7D%5E%7B1+%5Ctimes+n%7D&bg=ffffff&fg=333333&s=0&c=20201002),
then ![\\mathbf{AB} = \\displaystyle\\sum_{i=1}^n A_i
B_i](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BAB%7D+%3D+%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5En+A_i+B_i&bg=ffffff&fg=333333&s=0&c=20201002).  
In particular, ![\\mathbf{A A}^T =\\displaystyle\\sum_{i=1}^n A_i
A_i^T](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA+A%7D%5ET+%3D%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5En+A_i+A_i%5ET&bg=ffffff&fg=333333&s=0&c=20201002),
where
![A_i](https://s0.wp.com/latex.php?latex=A_i&bg=ffffff&fg=333333&s=0&c=20201002)
are the columns of
![\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![\\mathbf{A}^T \\mathbf{A} = \\displaystyle\\sum_{i=1}^m a_i
a_i^T](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D%5ET+%5Cmathbf%7BA%7D+%3D+%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5Em+a_i+a_i%5ET&bg=ffffff&fg=333333&s=0&c=20201002),
where
![a_i](https://s0.wp.com/latex.php?latex=a_i&bg=ffffff&fg=333333&s=0&c=20201002)
are the rows of
![\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002)
(written as column vectors).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[identities](https://statisticaloddsandends.wordpress.com/tag/identities/),
[linear algebra](https://statisticaloddsandends.wordpress.com/tag/linear-
algebra/),
[matrices](https://statisticaloddsandends.wordpress.com/tag/matrices/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/03/27/matrix-
expansions/ "Permalink to Matrix expansions and manipulations").

Let ![\\mathbf{A} \\in \\mathbb{R}^{m \\times
n}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm+%5Ctimes+n%7D&bg=ffffff&fg=333333&s=0&c=20201002),
![x \\in
\\mathbb{R}^m](https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5Em&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=y+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002),
and let ![a = \\text{vec}\(\\mathbf{A}\) \\in
\\mathbb{R}^{mn}](https://s0.wp.com/latex.php?latex=a+%3D+%5Ctext%7Bvec%7D%28%5Cmathbf%7BA%7D%29+%5Cin+%5Cmathbb%7BR%7D%5E%7Bmn%7D&bg=ffffff&fg=333333&s=0&c=20201002)
be the [vectorized
form](https://en.wikipedia.org/wiki/Vectorization_\(mathematics\)) of
![\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002).
Then ![x^T \\mathbf{A} y = a^T \(x \\otimes
y\)](https://s0.wp.com/latex.php?latex=x%5ET+%5Cmathbf%7BA%7D+y+%3D+a%5ET+%28x+%5Cotimes+y%29&bg=ffffff&fg=333333&s=0&c=20201002),
where
![\\otimes](https://s0.wp.com/latex.php?latex=%5Cotimes&bg=ffffff&fg=333333&s=0&c=20201002)
is the [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product).

If
![\\mathbf{D}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BD%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is a diagonal matrix,
![\\mathbf{U}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BU%7D&bg=ffffff&fg=333333&s=0&c=20201002)
a matrix with columns
![U_j](https://s0.wp.com/latex.php?latex=U_j&bg=ffffff&fg=333333&s=0&c=20201002),
and
![\\mathbf{V}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BV%7D&bg=ffffff&fg=333333&s=0&c=20201002)
a matrix with columns
![V_j](https://s0.wp.com/latex.php?latex=V_j&bg=ffffff&fg=333333&s=0&c=20201002),
then ![\\mathbf{UDV}^T = \\displaystyle\\sum_j d_j U_j
V_j^T](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BUDV%7D%5ET+%3D+%5Cdisplaystyle%5Csum_j+d_j+U_j+V_j%5ET&bg=ffffff&fg=333333&s=0&c=20201002).

If ![\\mathbf{A} \\in \\mathbb{R}^{m \\times
n}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm+%5Ctimes+n%7D&bg=ffffff&fg=333333&s=0&c=20201002),
![x \\in
\\mathbb{R}^m](https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5Em&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=y+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002),
then ![x^T \\mathbf{A} y = \\displaystyle\\sum_{i=1}^m\\sum_{j=1}^n x_i a_{ij}
y_j](https://s0.wp.com/latex.php?latex=x%5ET+%5Cmathbf%7BA%7D+y+%3D+%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5Em%5Csum_%7Bj%3D1%7D%5En+x_i+a_%7Bij%7D+y_j&bg=ffffff&fg=333333&s=0&c=20201002),
where
![a_{ij}](https://s0.wp.com/latex.php?latex=a_%7Bij%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is the element of
![\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002)
in row
![i](https://s0.wp.com/latex.php?latex=i&bg=ffffff&fg=333333&s=0&c=20201002)
and column
![j](https://s0.wp.com/latex.php?latex=j&bg=ffffff&fg=333333&s=0&c=20201002).

When doing matrix computations, I find it pretty hard to switch between the
matrix form and element-wise form. This is a reference for some of the more
common ones I come across. (Proofs are often just a matter of doing some
tedious algebra.)

Similarly, the row space of
![\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002)
can be expressed as the set ![\\{ \\mathbf{A}^Tx : x \\in \\mathbb{R}^n
\\}](https://s0.wp.com/latex.php?latex=%5C%7B+%5Cmathbf%7BA%7D%5ETx+%3A+x+%5Cin+%5Cmathbb%7BR%7D%5En+%5C%7D&bg=ffffff&fg=333333&s=0&c=20201002).

`isofit` is the black box we seek: a function that we give uncalibrated
predictions to get calibrated predictions in return. As the plot below shows,
the overall fit is still monotonic, and the calibrated predictions for the
training data do not change.

The CORRECT way to make the isotonic regression model into a black box is to
pass the output of `isoreg` to the `as.stepfun` function, like so:

Isotonic regression is one commonly used method for _**calibration**_ : see
[this previous
post](https://statisticaloddsandends.wordpress.com/2020/10/07/what-is-
calibration/) for background on calibration and [this
link](https://kingsubham27.medium.com/calibration-techniques-and-its-
importance-in-machine-learning-71bec997b661) for more details with python
code. In this setting, we want the isotonic regression model to be a black
box: we hand it uncalibrated predictions as an input, and it returns us
calibrated predictions.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2021%2F07%2F29%2Fgetting-predictions-from-an-isotonic-regression-model%2F&signup_flow=account)

_**Isotonic regression**_ is a method for obtaining a monotonic fit for
1-dimensional data. Let’s say we have data ![\(x_1, y_1\), \\dots, \(x_n,
y_n\) \\in
\\mathbb{R}^2](https://s0.wp.com/latex.php?latex=%28x_1%2C+y_1%29%2C+%5Cdots%2C+%28x_n%2C+y_n%29+%5Cin+%5Cmathbb%7BR%7D%5E2&bg=ffffff&fg=333333&s=0&c=20201002)
such that ![x_1 < x_2 < \\dots <
x_n](https://s0.wp.com/latex.php?latex=x_1+%3C+x_2+%3C+%5Cdots+%3C+x_n&bg=ffffff&fg=333333&s=0&c=20201002).
(We assume no ties among the
![x_i](https://s0.wp.com/latex.php?latex=x_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
for simplicity.) Informally, isotonic regression looks for ![\\beta_1, \\dots,
\\beta_n \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=%5Cbeta_1%2C+%5Cdots%2C+%5Cbeta_n+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
such that the
![\\beta_i](https://s0.wp.com/latex.php?latex=%5Cbeta_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
approximate the
![y_i](https://s0.wp.com/latex.php?latex=y_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
well while being monotonically non-decreasing. (See [this previous
post](https://statisticaloddsandends.wordpress.com/2020/05/24/what-is-
isotonic-regression/) for more technical details.)

    
    
     plot(all_x, all_y, pch = 4) points(all_x, isofit(all_x), pch = 15, col = "red") points(fit$x[fit$ord], fit$yf, pch = 16, col = "blue") 

A naive and WRONG way to calibrate `test_y` would be to run isotonic
regression on just the test data. The plot shows the fits for the training
data as blue dots and the fits for the testing data as red squares: the
overall fit is not monotonic.

    
    
     set.seed(2) test_x <- sample(2 * 1:15 - 1) test_y <- 0.2 * test_x + rnorm(length(test_x)) 

If you inspect the return value of the `isoreg` function, you will find that
it is unable to interact with any new test data. Imagine that we have some new
test data that we want to calibrate:

_**TLDR:** Pass the output of the `isoreg` function to `as.stepfun` to make an
isotonic regression model into a black box object that takes in uncalibrated
predictions and outputs calibrated ones._

Isotonic regression can be performed easily in R with the `stats` package’s
`isoreg` function. Note the slightly unusual syntax when pulling out the
fitted values (see the function’s documentation with `?isoreg` to understand
why this is the case). The plot shows the original data values as black
crosses and the fitted values as blue dots. As expected, the blue dots are
monotonically increasing.

    
    
     isofit <- as.stepfun(isoreg(x, y)) 

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [isotonic
regression](https://statisticaloddsandends.wordpress.com/tag/isotonic-
regression/),
[monotone](https://statisticaloddsandends.wordpress.com/tag/monotone/),
[prediction](https://statisticaloddsandends.wordpress.com/tag/prediction/),
[R](https://statisticaloddsandends.wordpress.com/tag/r/),
[regression](https://statisticaloddsandends.wordpress.com/tag/regression/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2021/07/29/getting-
predictions-from-an-isotonic-regression-model/ "Permalink to Getting
predictions from an isotonic regression model").

A second WRONG way to calibrate `test_y</`code> would be to run isotonic
regression on the combined training/test data. The plot shows that while the
overall fit is monotonic, the predictions for the training data have shifted,
i.e. you have changed the black box.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F03%2F11%2Fsylvesters-determinant-identity%2F&signup_flow=account)

![\\begin{aligned} \\det \\begin{pmatrix} I_m & -A \\\\ B & I_n \\end{pmatrix}
&= \\det \(I_m\) \\det \(I_n - BI_m^{-1}\(-A\)\) \\\\  &= \\det \(I_n + BA\), 
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cdet+%5Cbegin%7Bpmatrix%7D+I_m+%26+-A+%5C%5C+B+%26+I_n+%5Cend%7Bpmatrix%7D+%26%3D+%5Cdet+%28I_m%29+%5Cdet+%28I_n+-+BI_m%5E%7B-1%7D%28-A%29%29+%5C%5C++%26%3D+%5Cdet+%28I_n+%2B+BA%29%2C%C2%A0+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

_**Note:**_ There are more general versions of this identity as well (stated
in Reference 1). If ![A \\in \\mathbb{R}^{n \\times
n}](https://s0.wp.com/latex.php?latex=A+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn+%5Ctimes+n%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is invertible and ![U, V \\in \\mathbb{R}^{n \\times
m}](https://s0.wp.com/latex.php?latex=U%2C+V+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn+%5Ctimes+m%7D&bg=ffffff&fg=333333&s=0&c=20201002),
then

![\\det \(A + UWV^T\) = \\det \(W^{-1} + V^T A^{-1} U\) \\det \(W\) \\det
\(A\).](https://s0.wp.com/latex.php?latex=%5Cdet+%28A+%2B+UWV%5ET%29+%3D+%5Cdet+%28W%5E%7B-1%7D+%2B+V%5ET+A%5E%7B-1%7D+U%29+%5Cdet+%28W%29+%5Cdet+%28A%29.&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[determinant](https://statisticaloddsandends.wordpress.com/tag/determinant/),
[linear algebra](https://statisticaloddsandends.wordpress.com/tag/linear-
algebra/),
[matrices](https://statisticaloddsandends.wordpress.com/tag/matrices/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/03/11/sylvesters-
determinant-identity/ "Permalink to Sylvester’s determinant identity").

Let
![A](https://s0.wp.com/latex.php?latex=A&bg=ffffff&fg=333333&s=0&c=20201002)
be an ![m \\times
n](https://s0.wp.com/latex.php?latex=m+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix and let
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
be an ![n \\times
m](https://s0.wp.com/latex.php?latex=n+%5Ctimes+m&bg=ffffff&fg=333333&s=0&c=20201002)
matrix. From our [previous
post](https://statisticaloddsandends.wordpress.com/2018/03/10/determinant-of-
a-block-matrix/), we have

This identity is especially useful for computing the determinant of the sum of
an identity matrix and a rank-1 matrix: if
![c](https://s0.wp.com/latex.php?latex=c&bg=ffffff&fg=333333&s=0&c=20201002)
is a ![m \\times
1](https://s0.wp.com/latex.php?latex=m+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
matrix and
![r](https://s0.wp.com/latex.php?latex=r&bg=ffffff&fg=333333&s=0&c=20201002)
is a ![1 \\times
m](https://s0.wp.com/latex.php?latex=1+%5Ctimes+m&bg=ffffff&fg=333333&s=0&c=20201002)
matrix, then

![\\begin{aligned} \\det \\begin{pmatrix} I_m & -A \\\\ B & I_n \\end{pmatrix}
&= \\det \(I_n\) \\det \(I_m - \(-A\)I_n^{-1}B\) \\\\  &= \\det \(I_m + AB\). 
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cdet+%5Cbegin%7Bpmatrix%7D+I_m+%26+-A+%5C%5C+B+%26+I_n+%5Cend%7Bpmatrix%7D+%26%3D+%5Cdet+%28I_n%29+%5Cdet+%28I_m+-+%28-A%29I_n%5E%7B-1%7DB%29+%5C%5C++%26%3D+%5Cdet+%28I_m+%2B+AB%29.%C2%A0+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

![\\begin{aligned} \\text{proj}_{C\(P\)}\(\\hat{x}\) &= \\hat{x} +
\\text{proj}_{C\(P - \\hat{x}\)}\(0\), \\\\  \\text{proj}_{C\(P -
\\hat{x}\)}\(0\) &= \\text{proj}_{C\(P\)}\(\\hat{x}\) - \\hat{x}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext%7Bproj%7D_%7BC%28P%29%7D%28%5Chat%7Bx%7D%29+%26%3D+%5Chat%7Bx%7D+%2B+%5Ctext%7Bproj%7D_%7BC%28P+-+%5Chat%7Bx%7D%29%7D%280%29%2C+%5C%5C++%5Ctext%7Bproj%7D_%7BC%28P+-+%5Chat%7Bx%7D%29%7D%280%29+%26%3D+%5Ctext%7Bproj%7D_%7BC%28P%29%7D%28%5Chat%7Bx%7D%29+-+%5Chat%7Bx%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

![\\begin{aligned} \(x - \\hat{x}\)^\\top \(p_j - \\hat{x}\) &\\geq \\|x -
\\hat{x}\\|^2 \\quad \\text{for all } j = 1, \\dots, m, \\\\  \(x -
\\hat{x}\)^\\top p_j &\\geq \(x - \\hat{x}\)^\\top x \\quad \\text{for all } j
= 1, \\dots, m.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%28x+-+%5Chat%7Bx%7D%29%5E%5Ctop+%28p_j+-+%5Chat%7Bx%7D%29+%26%5Cgeq+%5C%7Cx+-+%5Chat%7Bx%7D%5C%7C%5E2+%5Cquad+%5Ctext%7Bfor+all+%7D+j+%3D+1%2C+%5Cdots%2C+m%2C+%5C%5C++%28x+-+%5Chat%7Bx%7D%29%5E%5Ctop+p_j+%26%5Cgeq+%28x+-+%5Chat%7Bx%7D%29%5E%5Ctop+x+%5Cquad+%5Ctext%7Bfor+all+%7D+j+%3D+1%2C+%5Cdots%2C+m.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

****You are commenting using your Facebook account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'facebook' \);) / Change
)

Let ![P = \\{ p_1, \\dots, p_m
\\}](https://s0.wp.com/latex.php?latex=P+%3D+%5C%7B+p_1%2C+%5Cdots%2C+p_m+%5C%7D&bg=ffffff&fg=333333&s=0&c=20201002)
be a set of points in
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002),
and let
![C\(P\)](https://s0.wp.com/latex.php?latex=C%28P%29&bg=ffffff&fg=333333&s=0&c=20201002)
denote the [convex hull](https://en.wikipedia.org/wiki/Convex_hull) of
![P](https://s0.wp.com/latex.php?latex=P&bg=ffffff&fg=333333&s=0&c=20201002).
We are often interested in finding the point in
![C\(P\)](https://s0.wp.com/latex.php?latex=C%28P%29&bg=ffffff&fg=333333&s=0&c=20201002)
that is nearest to some reference point
![\\hat{x}](https://s0.wp.com/latex.php?latex=%5Chat%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002),
known as the Euclidean projection onto
![C\(P\)](https://s0.wp.com/latex.php?latex=C%28P%29&bg=ffffff&fg=333333&s=0&c=20201002).
Let’s denote this point by
![\\text{proj}_{C\(P\)}\(\\hat{x}\)](https://s0.wp.com/latex.php?latex=%5Ctext%7Bproj%7D_%7BC%28P%29%7D%28%5Chat%7Bx%7D%29&bg=ffffff&fg=333333&s=0&c=20201002).

> **Theorem.** Let
> ![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
> be a point in
> ![C\(P\)](https://s0.wp.com/latex.php?latex=C%28P%29&bg=ffffff&fg=333333&s=0&c=20201002).
> ![x =
> \\text{proj}_{C\(P\)}\(0\)](https://s0.wp.com/latex.php?latex=x+%3D+%5Ctext%7Bproj%7D_%7BC%28P%29%7D%280%29&bg=ffffff&fg=333333&s=0&c=20201002)
> if and only if
>
> ![\\begin{aligned} x^\\top p_j \\geq \\| x \\|^2 \\quad \\text{for all } j =
> 1, \\dots, m.
> \\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+x%5E%5Ctop+p_j+%5Cgeq+%5C%7C+x+%5C%7C%5E2+%5Cquad+%5Ctext%7Bfor+all+%7D+j+%3D+1%2C+%5Cdots%2C+m.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[projection](https://statisticaloddsandends.wordpress.com/tag/projection/),
[wolfe](https://statisticaloddsandends.wordpress.com/tag/wolfe/),
[hyperplane](https://statisticaloddsandends.wordpress.com/tag/hyperplane/),
[polytope](https://statisticaloddsandends.wordpress.com/tag/polytope/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/12/10/optimality-
condition-for-euclidean-projection-onto-a-convex-polytope/ "Permalink to
Optimality condition for Euclidean projection onto a convex polytope").

( _ **Note:**_ I learned of the iff condition for general
![\\hat{x}](https://s0.wp.com/latex.php?latex=%5Chat%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002)
from Reference 2.)

  *     * [ ![](https://statisticaloddsandends.files.wordpress.com/2021/04/cropped-header-icon.png?w=50) Statistical Odds & Ends ](https://statisticaloddsandends.wordpress.com)

****You are commenting using your Twitter account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'twitter' \);) / Change
)

Rearranging the condition above gives us ![x^\\top \(p_j - x\) \\geq
0](https://s0.wp.com/latex.php?latex=x%5E%5Ctop+%28p_j+-+x%29+%5Cgeq+0&bg=ffffff&fg=333333&s=0&c=20201002).
Fixing
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002),
note that ![x^\\top \(y - x\) =
0](https://s0.wp.com/latex.php?latex=x%5E%5Ctop+%28y+-+x%29+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002)
represents the hyperplane passing through
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
which is orthogonal to the line segment going from
![0](https://s0.wp.com/latex.php?latex=0&bg=ffffff&fg=333333&s=0&c=20201002)
to
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002).
Hence, the condition ![x^\\top \(p_j - x\) \\geq
0](https://s0.wp.com/latex.php?latex=x%5E%5Ctop+%28p_j+-+x%29+%5Cgeq+0&bg=ffffff&fg=333333&s=0&c=20201002)
means that the directed line segment from
![0](https://s0.wp.com/latex.php?latex=0&bg=ffffff&fg=333333&s=0&c=20201002)
to
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
and the directed line segment from
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
to
![p_j](https://s0.wp.com/latex.php?latex=p_j&bg=ffffff&fg=333333&s=0&c=20201002)
make an angle that is ![\\leq
90^\\circ](https://s0.wp.com/latex.php?latex=%5Cleq+90%5E%5Ccirc&bg=ffffff&fg=333333&s=0&c=20201002).
Geometrically, this means that
![p_j](https://s0.wp.com/latex.php?latex=p_j&bg=ffffff&fg=333333&s=0&c=20201002)
and
![0](https://s0.wp.com/latex.php?latex=0&bg=ffffff&fg=333333&s=0&c=20201002)
lie on different sides of the hyperplane ![x^\\top \(y - x\) =
0](https://s0.wp.com/latex.php?latex=x%5E%5Ctop+%28y+-+x%29+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002),
and so
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
is nearer to
![0](https://s0.wp.com/latex.php?latex=0&bg=ffffff&fg=333333&s=0&c=20201002)
than
![p_j](https://s0.wp.com/latex.php?latex=p_j&bg=ffffff&fg=333333&s=0&c=20201002).

Wolfe (1976) (Reference 1) provides an optimality condition for
![\\text{proj}_{C\(P\)}\(\\hat{x}\)](https://s0.wp.com/latex.php?latex=%5Ctext%7Bproj%7D_%7BC%28P%29%7D%28%5Chat%7Bx%7D%29&bg=ffffff&fg=333333&s=0&c=20201002)
when the reference point is the origin, i.e. ![\\hat{x} =
0](https://s0.wp.com/latex.php?latex=%5Chat%7Bx%7D+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002).

The diagram below demonstrates this in two dimensions. In both figures, the
candidate for being the nearest point,
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002),
is ![X =
P_1](https://s0.wp.com/latex.php?latex=X+%3D+P_1&bg=ffffff&fg=333333&s=0&c=20201002).
![P_1](https://s0.wp.com/latex.php?latex=P_1&bg=ffffff&fg=333333&s=0&c=20201002)
is the nearest point to the origin in the left figure but not the right
figure. The angles that the optimality condition looks at are the angles
between the black arrow and the colored arrows: they are also marked by little
arcs. We see that in the left figure, both the angles are ![\\leq
90^\\circ](https://s0.wp.com/latex.php?latex=%5Cleq+90%5E%5Ccirc&bg=ffffff&fg=333333&s=0&c=20201002).
In the right figure, the yellow angle is ![>
90^\\circ](https://s0.wp.com/latex.php?latex=%3E+90%5E%5Ccirc&bg=ffffff&fg=333333&s=0&c=20201002).
It corresponds to a vertex of the polytope that is on the same side of the
hyperplane (dotted line) as the origin, and hence some point on the line
![XP_3](https://s0.wp.com/latex.php?latex=XP_3&bg=ffffff&fg=333333&s=0&c=20201002)
is going to be closer to the origin than
![X](https://s0.wp.com/latex.php?latex=X&bg=ffffff&fg=333333&s=0&c=20201002).

Let ![P - \\hat{x} = \\{ p_1 - \\hat{x}, \\dots, p_m - \\hat{x}
\\}](https://s0.wp.com/latex.php?latex=P+-+%5Chat%7Bx%7D+%3D+%5C%7B+p_1+-+%5Chat%7Bx%7D%2C+%5Cdots%2C+p_m+-+%5Chat%7Bx%7D+%5C%7D&bg=ffffff&fg=333333&s=0&c=20201002).
Projecting
![\\hat{x}](https://s0.wp.com/latex.php?latex=%5Chat%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002)
onto
![C\(P\)](https://s0.wp.com/latex.php?latex=C%28P%29&bg=ffffff&fg=333333&s=0&c=20201002)
is equivalent to projecting
![0](https://s0.wp.com/latex.php?latex=0&bg=ffffff&fg=333333&s=0&c=20201002)
onto ![C\(P -
\\hat{x}\)](https://s0.wp.com/latex.php?latex=C%28P+-+%5Chat%7Bx%7D%29&bg=ffffff&fg=333333&s=0&c=20201002)
and then adding
![\\hat{x}](https://s0.wp.com/latex.php?latex=%5Chat%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to it:

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F12%2F10%2Foptimality-condition-for-euclidean-projection-onto-a-convex-polytope%2F&signup_flow=account)

****You are commenting using your WordPress.com account. ( [Log
Out](javascript:HighlanderComments.doExternalLogout\( 'wordpress' \);) /
Change )

Update: I’ve just added the distribution for the last 3 world cups (tabulated
by hand).

(A 4-4-4-4 has happened on one occasion, in the 1994 World Cup, in the “group
of death” between Italy, Mexico, Ireland and Norway)

First place’s average points would be 6.62 points with a range between 3 and
9.  
Second place’s average points would be 4.7 points with a range between 2 and
7.  
Third place’s average points would be 3.21 points with a range between 1 and
6.  
Fourth place’s average would be 1.47 points with a range between 0 and 4.

There are **40** different possible group score configurations, with **7, 4,
4, 1** and **6, 4, 4, 3** being the most “common”, in the sense that they are
the most common result if each of “win”, “draw” and “loss” was equally likely
for each game. The table below shows the full list.  
( _ **Update 2022-11-26:**_ I’ve added the actual distribution of score
configurations for the last 3 world cups. Would be interesting if someone
could do it for the entire history of the world cup.)  
( _ **Update 2022-12-02:**_ I’ve added the actual distribution of score
configurations for world cup 2022.)

At 3 points, the team is 4.53% likely to advance.  
At 4 points, the team is 46.4% likely to advance.  
At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F06%2F27%2Fworld-cup-group-stage-outcomes%2F&signup_flow=account)

If the team scores 2 points in 3 matches, they are 0.5% likely to advance out
of the group stage.

I forgot to catch for the 4,4,4,3 ; 4,4,4,4 ; and 3,3,3,3 which are the
missing 2% : )

Score configuration | No. of permutations | WC 2022 | WC 2018 | WC 2014 | WC
2010  
---|---|---|---|---|---  
7, 4, 4, 1 | 36 (4.9%) |  |  | 1 | 1  
6, 4, 4, 3 | 36 (4.9%) | 3 | 1 |  | 1  
9, 6, 3, 0 | 24 (3.3%) |  | 2 | 1 | 1  
9, 4, 3, 1 | 24 (3.3%) |  | 1 | 2 | 1  
9, 4, 2, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 4, 0 | 24 (3.3%) | 1 |  | 1 |  
7, 6, 3, 1 | 24 (3.3%) |  |  | 1 |  
7, 6, 2, 1 | 24 (3.3%) |  |  |  |  
7, 5, 4, 0 | 24 (3.3%) | 1 |  |  | 1  
7, 5, 3, 1 | 24 (3.3%) | 1 | 2 |  |  
7, 5, 2, 1 | 24 (3.3%) |  |  |  |  
7, 4, 3, 3 | 24 (3.3%) |  |  |  |  
7, 4, 3, 2 | 24 (3.3%) |  |  |  |  
7, 4, 3, 1 | 24 (3.3%) |  |  |  |  
7, 4, 2, 2 | 24 (3.3%) |  |  |  |  
6, 6, 4, 1 | 24 (3.3%) | 2 |  |  | 1  
6, 6, 3, 3 | 24 (3.3%) |  | 1 |  |  
6, 5, 4, 1 | 24 (3.3%) |  |  |  |  
6, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 5, 4, 1 | 24 (3.3%) |  | 1 |  | 1  
5, 4, 4, 3 | 24 (3.3%) |  |  |  |  
5, 4, 4, 2 | 24 (3.3%) |  |  |  |  
5, 4, 3, 2 | 24 (3.3%) |  |  |  | 1  
9, 6, 1, 1 | 12 (1.6%) |  |  |  |  
9, 4, 4, 0 | 12 (1.6%) |  |  |  |  
7, 7, 3, 0 | 12 (1.6%) |  |  | 1 |  
7, 3, 2, 2 | 12 (1.6%) |  |  |  |  
6, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 2 | 12 (1.6%) |  |  |  |  
5, 5, 3, 1 | 12 (1.6%) |  |  |  |  
5, 5, 2, 2 | 12 (1.6%) |  |  |  |  
5, 3, 3, 2 | 12 (1.6%) |  |  |  |  
9, 3, 3, 3 | 8 (1.1%) |  |  |  |  
6, 6, 6, 0 | 8 (1.1%) |  |  |  |  
4, 4, 4, 3 | 8 (1.1%) |  |  |  |  
7, 7, 1, 1 | 6 (0.8%) |  |  |  |  
4, 4, 4, 4 | 6 (0.8%) |  |  |  |  
9, 2, 2, 2 | 4 (0.5%) |  |  |  |  
5, 5, 5, 0 | 4 (0.5%) |  |  |  |  
3, 3, 3, 3 | 1 (0.1%) |  |  |  |  
  
The code I used to produce the table above is below:

I came across your post because I was wondering how rare it is to see a
4-4-4-4- outcome like if each team wins one match draws one and loses one. is
there a way to add in actual results from competitions (or say World Cups) so
as to get the chances in real life of each outcome?

I found this post while wondering about the same thing and just compiled all
the data from world cups and European Championships going back to 1994 –
before that it was 2 points for a win so the maths was different. The most
prevalent outcome is 9-6-3-0 with 10 occurrences followed by 7-5-3-1 with 8.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [soccer](https://statisticaloddsandends.wordpress.com/tag/soccer/),
[sport](https://statisticaloddsandends.wordpress.com/tag/sport/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/06/27/world-cup-
group-stage-outcomes/ "Permalink to World cup group stage outcomes").

That’s certainly possible… The data is all out there, the hard part is
tabulating it (either by hand or via some script).

“At 5 points or more, the team is 97.9% likely to advance to the knockout
stage.” should be at 100%.

With the group stages of [World Cup 2018](https://www.fifa.com/worldcup/)
drawing to a close, I was wondering what the possible scores were attainable
in each group (e.g. 9, 6, 3, 0 for Group A), and how many different match
outcomes resulted in each score configuration. With just ![3^6 =
729](https://s0.wp.com/latex.php?latex=3%5E6+%3D+729&bg=ffffff&fg=333333&s=0&c=20201002)
possibilities (“win”, “draw” or “loss” for each of 6 games), this was easy to
code up.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[L-BFGS](https://statisticaloddsandends.wordpress.com/tag/l-bfgs/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-l-bfgs/ "Permalink to Quasi-Newton methods: L-BFGS").

For ![i = \\min \(k-m, 0\), \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+%5Cmin+%28k-m%2C+0%29%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

But we do have an efficient way to computing ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002):
simply replace
![k+1](https://s0.wp.com/latex.php?latex=k%2B1&bg=ffffff&fg=333333&s=0&c=20201002)
with
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
and
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
with
![q_k](https://s0.wp.com/latex.php?latex=q_k&bg=ffffff&fg=333333&s=0&c=20201002)
above! By recursively unfolding Step 3, we get the quasi-Newton update for
iteration
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
(computation of ![p_k = -H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)):

Peña, J. (2016). [Quasi-Newton
Methods](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/quasi-
newton.pdf). (CMU Convex Optimization: Fall 2016, Lecture on Nov 2.)

Thus, if we had an efficient way to compute ![p_k = H_k
q_k](https://s0.wp.com/latex.php?latex=p_k+%3D+H_k+q_k&bg=ffffff&fg=333333&s=0&c=20201002),
we have an efficient way to compute ![H_{k+1}
g](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+g&bg=ffffff&fg=333333&s=0&c=20201002)
involving just ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors:

The issue with BFGS is that we have to maintain the ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002),
which is too costly when
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
is large. The main idea behind _**limited memory BFGS (L-BFGS)**_ is that we
don’t really need
![H_k](https://s0.wp.com/latex.php?latex=H_k&bg=ffffff&fg=333333&s=0&c=20201002):
what we really want is ![p_k = - H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=p_k+%3D+-+H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
If we can compute ![H_k \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=H_k+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002)
implicitly by maintaining a few ![n \\times
1](https://s0.wp.com/latex.php?latex=n+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
vectors, then we never have to deal with an ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix.

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**BFGS**_](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/) is a popular quasi-Newton method. At each
iteration, we take the following steps:

![\\begin{aligned} H_{k+1}&= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k}
\\right\)  + \\dfrac{s_k s_k^\\top}{y_k^\\top s_k},
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

![\\begin{aligned} H_{k+1} g &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( I - \\dfrac{y_k s_k^\\top}{y_k^\\top s_k} \\right\)
g  + \\dfrac{s_k s_k^\\top g}{y_k^\\top s_k}  \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k \\left\( g -
\\dfrac{s_k^\\top g}{y_k^\\top s_k} y_k \\right\)  + \\dfrac{s_k^\\top
g}{y_k^\\top s_k} s_k \\\\  &= \\left\( I - \\dfrac{s_k y_k^\\top}{y_k^\\top
s_k}\\right\) H_k \\left\( g - \\alpha_k y_k \\right\)  + \\alpha_k s_k
&\(\\alpha_k = \\dfrac{s_k^\\top g}{y_k^\\top s_k}\) \\\\  &= \\left\( I -
\\dfrac{s_k y_k^\\top}{y_k^\\top s_k}\\right\) H_k q_k  + \\alpha_k s_k &\(q_k
= g - \\alpha_k y_k\) \\\\  &= p_k - \\dfrac{s_k y_k^\\top p_k}{y_k^\\top s_k}
+ \\alpha_k s_k &\(p_k = H_k q_k\) \\\\  &= p_k - \(\\alpha_k - \\beta_k\)
s_k. &\(\\beta_k = \\dfrac{y_k^\\top p_k}{y_k^\\top s_k}\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+H_%7Bk%2B1%7D+g+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+I+-+%5Cdfrac%7By_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D+%5Cright%29+g%C2%A0+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%C2%A0+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+y_k+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D+s_k+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+%5Cleft%28+g+-+%5Calpha_k+y_k+%5Cright%29%C2%A0+%2B+%5Calpha_k+s_k+%26%28%5Calpha_k+%3D+%5Cdfrac%7Bs_k%5E%5Ctop+g%7D%7By_k%5E%5Ctop+s_k%7D%29+%5C%5C++%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D%5Cright%29+H_k+q_k%C2%A0+%2B+%5Calpha_k+s_k+%26%28q_k+%3D+g+-+%5Calpha_k+y_k%29+%5C%5C++%26%3D+p_k+-+%5Cdfrac%7Bs_k+y_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D+%2B+%5Calpha_k+s_k+%26%28p_k+%3D+H_k+q_k%29+%5C%5C++%26%3D+p_k+-+%28%5Calpha_k+-+%5Cbeta_k%29+s_k.+%26%28%5Cbeta_k+%3D+%5Cdfrac%7By_k%5E%5Ctop+p_k%7D%7By_k%5E%5Ctop+s_k%7D%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-l-bfgs%2F&signup_flow=account)

For ![i = 0, \\dots,
k-1](https://s0.wp.com/latex.php?latex=i+%3D+0%2C+%5Cdots%2C+k-1&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\beta \\leftarrow \\dfrac{y_i^\\top p}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cleftarrow+%5Cdfrac%7By_i%5E%5Ctop+p%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![p \\leftarrow p + \(\\alpha_i - \\beta\)
s_i](https://s0.wp.com/latex.php?latex=p+%5Cleftarrow+p+%2B+%28%5Calpha_i+-+%5Cbeta%29+s_i&bg=ffffff&fg=333333&s=0&c=20201002).

For ![i = k-1, \\dots,
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

Limited memory BFGS goes one step further: instead of doing 2 loops of length
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)
for the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration, do 2 loops of length ![\\min \(m,
k\)](https://s0.wp.com/latex.php?latex=%5Cmin+%28m%2C+k%29&bg=ffffff&fg=333333&s=0&c=20201002)
instead, where
![m](https://s0.wp.com/latex.php?latex=m&bg=ffffff&fg=333333&s=0&c=20201002)
is usually set to 5 or 10. Here is the appropriately modified update:

For ![i = k-1, \\dots, \\min \(k-m\),
0](https://s0.wp.com/latex.php?latex=i+%3D+k-1%2C+%5Cdots%2C+%5Cmin+%28k-m%29%2C+0&bg=ffffff&fg=333333&s=0&c=20201002),
set ![\\alpha_i \\leftarrow \\dfrac{s_i^\\top q}{y_i^\\top
s_i}](https://s0.wp.com/latex.php?latex=%5Calpha_i+%5Cleftarrow+%5Cdfrac%7Bs_i%5E%5Ctop+q%7D%7By_i%5E%5Ctop+s_i%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![q \\leftarrow q - \\alpha
y_i](https://s0.wp.com/latex.php?latex=q+%5Cleftarrow+q+-+%5Calpha+y_i&bg=ffffff&fg=333333&s=0&c=20201002).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [BFGS](https://statisticaloddsandends.wordpress.com/tag/bfgs/),
[DFP](https://statisticaloddsandends.wordpress.com/tag/dfp/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-newton/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/26/quasi-
newton-methods-dfp-and-bfgs/ "Permalink to Quasi-Newton methods: DFP
and BFGS").

Recall that the _**[SR1
update](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-sr1/)**_ assumes that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix. The _**DFP update**_ (Davidon-Fletcher-
Powell) assumes that the next
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, i.e.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F26%2Fquasi-newton-methods-dfp-and-bfgs%2F&signup_flow=account)

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. If we let
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
denote the inverse of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
then the secant equation is equivalent to ![H^+ y =
s](https://s0.wp.com/latex.php?latex=H%5E%2B+y+%3D+s&bg=ffffff&fg=333333&s=0&c=20201002).

Set ![H_{k+1} = H_k - \\dfrac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k} +
\\dfrac{s_k s_k^\\top}{y_k^\\top
s_k}](https://s0.wp.com/latex.php?latex=H_%7Bk%2B1%7D+%3D+H_k+-+%5Cdfrac%7BH_k+y_k+y_k%5E%5Ctop+H_k%7D%7By_k%5E%5Ctop+H_k+y_k%7D+%2B+%5Cdfrac%7Bs_k+s_k%5E%5Ctop%7D%7By_k%5E%5Ctop+s_k%7D&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} B+ &= B - \\dfrac{Bss^\\top B}{s^\\top Bs} +
\\dfrac{yy^\\top}{y^\\top s}, \\\\  H^+ &= \\left\( I -
\\dfrac{sy^\\top}{y^\\top s}\\right\) H \\left\( I - \\dfrac{ys^\\top}{y^\\top
s} \\right\)  + \\dfrac{ss^\\top}{y^\\top s}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B%2B+%26%3D+B+-+%5Cdfrac%7BBss%5E%5Ctop+B%7D%7Bs%5E%5Ctop+Bs%7D+%2B+%5Cdfrac%7Byy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%2C+%5C%5C++H%5E%2B+%26%3D+%5Cleft%28+I+-+%5Cdfrac%7Bsy%5E%5Ctop%7D%7By%5E%5Ctop+s%7D%5Cright%29+H+%5Cleft%28+I+-+%5Cdfrac%7Bys%5E%5Ctop%7D%7By%5E%5Ctop+s%7D+%5Cright%29%C2%A0+%2B+%5Cdfrac%7Bss%5E%5Ctop%7D%7By%5E%5Ctop+s%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Whereas DFP assumes
![H^+](https://s0.wp.com/latex.php?latex=H%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix, the _**BFGS update**_ (Broyden-Fletcher-
Goldfarb-Shanno) assumes that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-two matrix. Going through the same chain of reasoning as
the DFP update but with
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
replacing
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002)
and
![y](https://s0.wp.com/latex.php?latex=y&bg=ffffff&fg=333333&s=0&c=20201002)
and
![s](https://s0.wp.com/latex.php?latex=s&bg=ffffff&fg=333333&s=0&c=20201002)
switching roles, we get the updates

In [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/), we described how _**Quasi-Newton methods**_ can be
used to minimize a twice-differentiable function
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
whose domain is all of
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
At each iteration of the method, we take the following steps:

Like the DFP update, the BFGS update maintains the positive definiteness of
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).
In addition, BFGS also has a self-correcting property that makes it attractive
in practice (see Nocedal 1992, Reference 2). The BFGS update is the most
popular quasi-Newton update in practice.

One advantage that the DFP update has over the SR1 update is that
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
being positive definite implies that
![B^+](https://s0.wp.com/latex.php?latex=B%5E%2B&bg=ffffff&fg=333333&s=0&c=20201002)
is also positive definite.

where ![\\nabla
f](https://s0.wp.com/latex.php?latex=%5Cnabla+f&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\\nabla^2
f](https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+f&bg=ffffff&fg=333333&s=0&c=20201002)
are the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
respectively, and
![t_k](https://s0.wp.com/latex.php?latex=t_k&bg=ffffff&fg=333333&s=0&c=20201002)
is a step size chosen appropriately (e.g. with [backtracking line
search](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-line-
search-and-backtracking-line-search/)). Another way to write the update is

![\\begin{aligned} \\nabla^2 f\(x_k\) p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1}
&= x_k + t_k p_k.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cnabla%5E2+f%28x_k%29+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
according to some procedure so that the secant equation (and other conditions)
are satisfied.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-in-optimization%2F&signup_flow=account)

A big drawback of Newton’s method is that computing the Hessian can be very
expensive. _**Quasi-Newton methods**_ address this problem: update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
with

where ![f : \\mathbb{R}^n \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f+%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is twice differentiable and ![\\text{dom}\(f\) =
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Ctext%7Bdom%7D%28f%29+%3D+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
[_**Newton’s
method**_](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is
a second-order descent method for finding the minimum. Starting at some
initial point, at the
![k](https://s0.wp.com/latex.php?latex=k&bg=ffffff&fg=333333&s=0&c=20201002)th
iteration we update the candidate solution with the formula

That is, we (i) compute the gradient and Hessian of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002)
at
![x_k](https://s0.wp.com/latex.php?latex=x_k&bg=ffffff&fg=333333&s=0&c=20201002),
(ii) solve for
![p_k](https://s0.wp.com/latex.php?latex=p_k&bg=ffffff&fg=333333&s=0&c=20201002),
then (iii) update
![x_{k+1}](https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
according to the second line.

Reblogged this on [Statistical Methods, Paradigms and
Application](https://ibiloyeacresearch.wordpress.com/2022/11/24/quasi-newton-
methods-in-optimization/) and commented:  
Interesting and Really good explanation here:

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [secant
equation](https://statisticaloddsandends.wordpress.com/tag/secant-equation/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-in-optimization/ "Permalink to Quasi-Newton methods
in optimization").

![\\begin{aligned} B_k p_k &= - \\nabla f\(x_k\), \\\\  x_{k+1} &= x_k + t_k
p_k,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+B_k+p_k+%26%3D+-+%5Cnabla+f%28x_k%29%2C+%5C%5C++x_%7Bk%2B1%7D+%26%3D+x_k+%2B+t_k+p_k%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Quasi-Newton methods include SR1, DFP, BFGS and L-BFGS, which I hope to
describe in future posts.

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This last equation is known as the _**secant equation**_ , and is sometimes
written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Note that this equation involves
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations in ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, so there is a lot of freedom in how to choose the
![B_k](https://s0.wp.com/latex.php?latex=B_k&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
On top of satisfying the secant equation, we often require 3 other conditions:

This is known as the _**SR1 update**_. SR1 is simple to understand and
implement, but has two shortcomings. First, the denominator in the update for
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002),
![\(y - Bs\)^\\top
s](https://s0.wp.com/latex.php?latex=%28y+-+Bs%29%5E%5Ctop+s&bg=ffffff&fg=333333&s=0&c=20201002),
might be approximately zero, causing numerical issues. Second, it does not
preserve positive definiteness.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F24%2Fquasi-newton-methods-sr1%2F&signup_flow=account)

One way to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
easily is to assume that the next
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
is the current
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
plus a symmetric rank-one matrix, i.e.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[quasi-newton](https://statisticaloddsandends.wordpress.com/tag/quasi-
newton/), [SR1](https://statisticaloddsandends.wordpress.com/tag/sr1/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/24/quasi-
newton-methods-sr1/ "Permalink to Quasi-Newton methods: SR1").

where ![s_k = x_{k+1} -
x_k](https://s0.wp.com/latex.php?latex=s_k+%3D+x_%7Bk%2B1%7D+-+x_k&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y_k = \\nabla f\(x_{k+1}\) - \\nabla
f\(x_k\)](https://s0.wp.com/latex.php?latex=y_k+%3D+%5Cnabla+f%28x_%7Bk%2B1%7D%29+-+%5Cnabla+f%28x_k%29&bg=ffffff&fg=333333&s=0&c=20201002).
This is often written as ![B^+ s =
y](https://s0.wp.com/latex.php?latex=B%5E%2B+s+%3D+y&bg=ffffff&fg=333333&s=0&c=20201002)
for brevity. Because this is a system of
![n](https://s0.wp.com/latex.php?latex=n&bg=ffffff&fg=333333&s=0&c=20201002)
equations with ![n \\times
n](https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
unknowns, there are many ways to update
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002).

Recall from the quasi-Newton updates that we need to solve the equation ![B p
= - \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=B+p+%3D+-+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002)
for
![p](https://s0.wp.com/latex.php?latex=p&bg=ffffff&fg=333333&s=0&c=20201002),
which means that we need to compute the inverse ![H =
B^{-1}](https://s0.wp.com/latex.php?latex=H+%3D+B%5E%7B-1%7D&bg=ffffff&fg=333333&s=0&c=20201002)
to get ![p = - H \\nabla
f\(x\)](https://s0.wp.com/latex.php?latex=p+%3D+-+H+%5Cnabla+f%28x%29&bg=ffffff&fg=333333&s=0&c=20201002).
Because of the [Sherman-Morrison-Woodbury
formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we
can use the update to
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
to derive an update for
![H](https://s0.wp.com/latex.php?latex=H&bg=ffffff&fg=333333&s=0&c=20201002):

which implies that
![u](https://s0.wp.com/latex.php?latex=u&bg=ffffff&fg=333333&s=0&c=20201002)
must be a multiple of ![y -
Bs](https://s0.wp.com/latex.php?latex=y+-+Bs&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this into the equation and solving for
![a](https://s0.wp.com/latex.php?latex=a&bg=ffffff&fg=333333&s=0&c=20201002),
we obtain the update

for some ![a \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=a+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![u \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=u+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Plugging this update into the secant equation and rearranging, we get

_**Backtracking line search**_ uses a heuristic to quickly determine a step
size which decreases the objective function value by a certain amount. The
user needs to set two parameters, ![0 < \\alpha <
0.5](https://s0.wp.com/latex.php?latex=0+%3C+%5Calpha+%3C+0.5&bg=ffffff&fg=333333&s=0&c=20201002)
and ![0 < \\beta <
1](https://s0.wp.com/latex.php?latex=0+%3C+%5Cbeta+%3C+1&bg=ffffff&fg=333333&s=0&c=20201002).
In each iteration, after the descent direction ![\\Delta
x](https://s0.wp.com/latex.php?latex=%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002)
has been chosen, the step size is determine by the following:

How do we know that
![t_0](https://s0.wp.com/latex.php?latex=t_0&bg=ffffff&fg=333333&s=0&c=20201002)
is always ![>
0](https://s0.wp.com/latex.php?latex=%3E+0&bg=ffffff&fg=333333&s=0&c=20201002)
(i.e. the blue segment exists)? It’s because of how we chose the term
![\\alpha t \\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=%5Calpha+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002).
It can be shown that the derivative of ![f\(x + t \\Delta
x\)](https://s0.wp.com/latex.php?latex=f%28x+%2B+t+%5CDelta+x%29&bg=ffffff&fg=333333&s=0&c=20201002)
w.r.t.
![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=333333&s=0&c=20201002)
at the point ![t =
0](https://s0.wp.com/latex.php?latex=t+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002)
is ![\\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002).
Hence, the line ![y = f\(x\) + t \\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=y+%3D+f%28x%29+%2B+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002)
is tangent to ![y = f\(x + t \\nabla
x\)](https://s0.wp.com/latex.php?latex=y+%3D+f%28x+%2B+t+%5Cnabla+x%29&bg=ffffff&fg=333333&s=0&c=20201002)
at ![t =
0](https://s0.wp.com/latex.php?latex=t+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002).
Multiplying the term ![t \\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002)
by some ![0 < \\alpha <
1](https://s0.wp.com/latex.php?latex=0+%3C+%5Calpha+%3C+1&bg=ffffff&fg=333333&s=0&c=20201002)
ensures that the stopping condition will hold for some non-trivial segment
![\[0,
t_0\]](https://s0.wp.com/latex.php?latex=%5B0%2C+t_0%5D&bg=ffffff&fg=333333&s=0&c=20201002).

[![](https://statisticaloddsandends.files.wordpress.com/2022/11/exact-line-
search.png?w=584&h=327)](https://statisticaloddsandends.files.wordpress.com/2022/11/exact-
line-search.png)The curve is the value of the function along the ray ![x + t
\\Delta
x](https://s0.wp.com/latex.php?latex=x+%2B+t+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002)
with ![t \\geq
0](https://s0.wp.com/latex.php?latex=t+%5Cgeq+0&bg=ffffff&fg=333333&s=0&c=20201002).
![t^*](https://s0.wp.com/latex.php?latex=t%5E%2A&bg=ffffff&fg=333333&s=0&c=20201002)
is the point where this function is minimized. It should be clear from the
description that this choice will result in smaller values of the function in
each iteration.

( _ **Note:**_ The last diagram is taken from Reference 1, and the other
diagrams are this base + my annotations.)

In other words, we keep reducing the step size by the same multiplicative
factor until the ![f\(x + t\\Delta x\) \\leq f\(x\) + \\alpha t \\nabla
f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=f%28x+%2B+t%5CDelta+x%29+%5Cleq+f%28x%29+%2B+%5Calpha+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002).

There are several ways to choose a descent direction (see [this previous
post](https://statisticaloddsandends.wordpress.com/2021/04/29/what-is-
steepest-descent/) on steepest descent for some examples). This post is about
two ways to choose the step size. This step is called a _**line search**_
because the step size determines where along the line ![\\{x + t \\Delta x : t
\\geq 0
\\}](https://s0.wp.com/latex.php?latex=%5C%7Bx+%2B+t+%5CDelta+x+%3A+t+%5Cgeq+0+%5C%7D&bg=ffffff&fg=333333&s=0&c=20201002)
the next iterate will be.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [backtracking line
search](https://statisticaloddsandends.wordpress.com/tag/backtracking-line-
search/), [convex](https://statisticaloddsandends.wordpress.com/tag/convex/),
[exact line search](https://statisticaloddsandends.wordpress.com/tag/exact-
line-search/), [line
search](https://statisticaloddsandends.wordpress.com/tag/line-search/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/19/exact-
line-search-and-backtracking-line-search/ "Permalink to Exact line search and
backtracking line search").

While ![f\(x + t\\Delta x\) > f\(x\) + \\alpha t \\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=f%28x+%2B+t%5CDelta+x%29+%3E+f%28x%29+%2B+%5Calpha+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002),
set ![t \\leftarrow \\beta
t](https://s0.wp.com/latex.php?latex=t+%5Cleftarrow+%5Cbeta+t&bg=ffffff&fg=333333&s=0&c=20201002).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F19%2Fexact-line-search-and-backtracking-line-search%2F&signup_flow=account)

Assume we are trying to minimize some convex function ![f: \\mathbb{R}^n
\\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=f%3A+%5Cmathbb%7BR%7D%5En+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
which is differentiable. One way to do this is to use a _**descent method**_.
A general descent method can be described as follows: Given a starting point
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
in the domain of
![f](https://s0.wp.com/latex.php?latex=f&bg=ffffff&fg=333333&s=0&c=20201002),
iterate over the following 3 steps until some stopping criterion is satisfied:

[![](https://statisticaloddsandends.files.wordpress.com/2022/11/backtracking-
line-
search.png?w=584&h=327)](https://statisticaloddsandends.files.wordpress.com/2022/11/backtracking-
line-search.png)This is the same picture as that for exact line search, except
we’ve added the line ![y = f\(x\) + \\alpha t \\nabla f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=y+%3D+f%28x%29+%2B+%5Calpha+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002).
This line always has negative slope: because ![\\Delta
x](https://s0.wp.com/latex.php?latex=%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002)
is a descent direction, we must have ![\\nabla f\(x\)^\\top \\Delta x <
0](https://s0.wp.com/latex.php?latex=%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x+%3C+0&bg=ffffff&fg=333333&s=0&c=20201002).
The stopping condition, ![f\(x + t\\Delta x\) \\leq f\(x\) + \\alpha t \\nabla
f\(x\)^\\top \\Delta
x](https://s0.wp.com/latex.php?latex=f%28x+%2B+t%5CDelta+x%29+%5Cleq+f%28x%29+%2B+%5Calpha+t+%5Cnabla+f%28x%29%5E%5Ctop+%5CDelta+x&bg=ffffff&fg=333333&s=0&c=20201002),
holds for the blue segment ![\[0,
t_0\]](https://s0.wp.com/latex.php?latex=%5B0%2C+t_0%5D&bg=ffffff&fg=333333&s=0&c=20201002).
The operation ![t \\leftarrow \\beta
t](https://s0.wp.com/latex.php?latex=t+%5Cleftarrow+%5Cbeta+t&bg=ffffff&fg=333333&s=0&c=20201002)
makes the step size smaller and smaller until
![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=333333&s=0&c=20201002)
falls in the blue segment, and by the way it’s set up, we know that the
objective function will have a strictly smaller value.

Since exact line search requires solving a minimization problem (albeit in one
dimension), it can be expensively unless
![t^*](https://s0.wp.com/latex.php?latex=t%5E%2A&bg=ffffff&fg=333333&s=0&c=20201002)
can be computed efficiently (or even analytically).

_**Exact line search**_ finds
![t^\\star](https://s0.wp.com/latex.php?latex=t%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002)
such that ![f\(x + t \\Delta
x\)](https://s0.wp.com/latex.php?latex=f%28x+%2B+t+%5CDelta+x%29&bg=ffffff&fg=333333&s=0&c=20201002)
attains its minimum value at ![t =
t^\\star](https://s0.wp.com/latex.php?latex=t+%3D+t%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002),
where we let
![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=333333&s=0&c=20201002)
range over non-negative values. The diagram below demonstrates this:

![\\begin{aligned} \\text{conv}\(S\) = \\left\\{ \\sum_{i=1}^k a_i x_i : k >
0, x_i \\in S, a_i \\in \\mathbb{R}, a_i \\geq 0, \\sum_{i=1}^k a_i = 1
\\right\\}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext%7Bconv%7D%28S%29+%3D+%5Cleft%5C%7B+%5Csum_%7Bi%3D1%7D%5Ek+a_i+x_i+%3A+k+%3E+0%2C+x_i+%5Cin+S%2C+a_i+%5Cin+%5Cmathbb%7BR%7D%2C+a_i+%5Cgeq+0%2C+%5Csum_%7Bi%3D1%7D%5Ek+a_i+%3D+1+%5Cright%5C%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

Let’s have a look at a few examples. If
![S](https://s0.wp.com/latex.php?latex=S&bg=ffffff&fg=333333&s=0&c=20201002)
consists of two points, the convex hull is the line segment joining them
(including the endpoints) while the affine hull is the entire line through
these two points:

[![](https://statisticaloddsandends.files.wordpress.com/2022/11/2-2d.png?w=584&h=316)](https://statisticaloddsandends.files.wordpress.com/2022/11/2-2d.png)The
final illustration below shows the affine and convex hulls of 3 non-collinear
points in
![\\mathbb{R}^3](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E3&bg=ffffff&fg=333333&s=0&c=20201002).
They are both two-dimensional sets living in 3-dimensional space. Notice also
how the affine hull need not pass through the origin (whereas all subspaces of
![\\mathbb{R}^3](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E3&bg=ffffff&fg=333333&s=0&c=20201002)
must pass through it).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [affine
hull](https://statisticaloddsandends.wordpress.com/tag/affine-hull/),
[convex](https://statisticaloddsandends.wordpress.com/tag/convex/), [convex
hull](https://statisticaloddsandends.wordpress.com/tag/convex-hull/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/18/affine-
hull-vs-convex-hull/ "Permalink to Affine hull vs. convex hull").

The [_**affine hull**_](https://en.wikipedia.org/wiki/Affine_hull) and
[_**convex hull**_](https://en.wikipedia.org/wiki/Convex_hull) are closely
related concepts. Let
![S](https://s0.wp.com/latex.php?latex=S&bg=ffffff&fg=333333&s=0&c=20201002)
be a set in
![\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
The affine hull of
![S](https://s0.wp.com/latex.php?latex=S&bg=ffffff&fg=333333&s=0&c=20201002)
is the set of all _affine combinations_ of elements of
![S](https://s0.wp.com/latex.php?latex=S&bg=ffffff&fg=333333&s=0&c=20201002):

![\\begin{aligned} \\text{aff}\(S\) = \\left\\{ \\sum_{i=1}^k a_i x_i : k > 0,
x_i \\in S, a_i \\in \\mathbb{R}, \\sum_{i=1}^k a_i = 1 \\right\\}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext%7Baff%7D%28S%29+%3D+%5Cleft%5C%7B+%5Csum_%7Bi%3D1%7D%5Ek+a_i+x_i+%3A+k+%3E+0%2C+x_i+%5Cin+S%2C+a_i+%5Cin+%5Cmathbb%7BR%7D%2C+%5Csum_%7Bi%3D1%7D%5Ek+a_i+%3D+1+%5Cright%5C%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F18%2Faffine-hull-vs-convex-hull%2F&signup_flow=account)

The convex hull of
![S](https://s0.wp.com/latex.php?latex=S&bg=ffffff&fg=333333&s=0&c=20201002)
is the set of all _convex combinations_ of the elements of
![S](https://s0.wp.com/latex.php?latex=S&bg=ffffff&fg=333333&s=0&c=20201002):

[![](https://statisticaloddsandends.files.wordpress.com/2022/11/1-2d.png?w=584&h=297)](https://statisticaloddsandends.files.wordpress.com/2022/11/1-2d.png)For
3 non-collinear points in two dimensions, the convex hull is the triangle with
these 3 points as vertices while the affine hull is the entire plane
![\\mathbb{R}^2](https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E2&bg=ffffff&fg=333333&s=0&c=20201002):

Putting the definitions side by side, we see that the only difference is that
for the convex hull, the weights in the linear combination (the
![a_i](https://s0.wp.com/latex.php?latex=a_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s)
have an additional restriction of being non-negative. The only restriction on
the
![a_i](https://s0.wp.com/latex.php?latex=a_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
for combinations in the affine hull is that they sum to 1. This also means
that the affine hull always contains the convex hull.

_**Why am I not simply posting the git commands?**_ That’s because the correct
git command depends a lot on the context. By linking to the full reference,
you can check if the situation the reference is addressing matches yours
before applying its solution.

[**How to see html on Github as a rendered HTML
page:**](https://stackoverflow.com/questions/8446218/how-to-see-an-html-page-
on-github-as-a-normal-rendered-html-page-to-see-preview) Go to
<https://htmlpreview.github.io/> and put in the Github html.

[**How to push changes on a local branch to the remote version of the
branch.**](https://github.com/Kunena/Kunena-Forum/wiki/Create-a-new-branch-
with-git-and-manage-branches) This includes how to delete local and remote
branches.

[**How to amend a commit
message.**](https://help.github.com/en/github/committing-changes-to-your-
project/changing-a-commit-message) This reference also deals with the case
where you want to amend messages for older commits, or if you want to amend
messages for commits that have been pushed to a remote repository.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [git](https://statisticaloddsandends.wordpress.com/tag/git/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2020/04/07/git-
references/ "Permalink to Git references").

( _ **Note:**_ This post will be updated periodically as I find other useful
idioms that I keep forgetting.)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2020%2F04%2F07%2Fgit-references%2F&signup_flow=account)

[**_Git_**](https://git-scm.com/) is an essential distributed version control
system for managing programming projects (and in my case, maintaining R
packages). While essential, I find it quite difficult to remember common git
idioms for simple operations. This post lists references that I personally
find useful: hopefully some of you might find them useful too.

[**How to “squash” multiple commits into a single
commit.**](https://www.internalpointers.com/post/squash-commits-into-one-git)
This is especially useful for removing small, intermediate checkpoints that
help during development but clutter up the commit history. See [this
link](https://thoughtbot.com/blog/git-interactive-rebase-squash-amend-
rewriting-history) for more interactive rebase options.

**[How to merge a branch into master.](https://stackabuse.com/git-merge-
branch-into-master/)** This deals with the simple case when there are no merge
conflicts.

**[How to add tags to certain commits.](https://git-scm.com/book/en/v2/Git-
Basics-Tagging)** Helpful for keeping track of versions of the repo.

![\\begin{aligned} \\text{minimize} \\quad& f_0\(x\) \\\\  \\text{subject to}
\\quad& f_i\(x\) \\leq 0, \\quad i = 1, \\dots, m, \\\\  & h_i\(x\) = 0,
\\quad i = 1, \\dots, p,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext%7Bminimize%7D+%5Cquad%26+f_0%28x%29+%5C%5C++%5Ctext%7Bsubject+to%7D+%5Cquad%26+f_i%28x%29+%5Cleq+0%2C+%5Cquad+i+%3D+1%2C+%5Cdots%2C+m%2C+%5C%5C++%26+h_i%28x%29+%3D+0%2C+%5Cquad+i+%3D+1%2C+%5Cdots%2C+p%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

_**Note:**_ There are more general versions of the KKT conditions; see
Reference 2 for details.

Here are 3 statements (theorems, if you will) involving the KKT conditions
which highlight their importance:

_Stationarity :_ ![\\partial_x L\(x^\\star, \\lambda^\\star, \\nu^\\star\) =
0](https://s0.wp.com/latex.php?latex=%5Cpartial_x+L%28x%5E%5Cstar%2C+%5Clambda%5E%5Cstar%2C+%5Cnu%5E%5Cstar%29+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002),
i.e. ![\\nabla f_0\(x^\\star\) + \\displaystyle\\sum_{i=1}^m \\lambda_i^\\star
\\nabla f_i\(x^\\star\) + \\displaystyle\\sum_{i=1}^p \\nu_i^\\star \\nabla
h_i\(x^\\star\) =
0](https://s0.wp.com/latex.php?latex=%5Cnabla+f_0%28x%5E%5Cstar%29+%2B+%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5Em+%5Clambda_i%5E%5Cstar+%5Cnabla+f_i%28x%5E%5Cstar%29+%2B+%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5Ep+%5Cnu_i%5E%5Cstar+%5Cnabla+h_i%28x%5E%5Cstar%29+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002).

with the variable ![x \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Assume that the
![f_i](https://s0.wp.com/latex.php?latex=f_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
and
![h_i](https://s0.wp.com/latex.php?latex=h_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
are differentiable. (At this point, we are not assuming anything about their
convexity.)

_Primal feasibility :_ ![f_i\(x^\\star\) \\leq
0](https://s0.wp.com/latex.php?latex=f_i%28x%5E%5Cstar%29+%5Cleq+0&bg=ffffff&fg=333333&s=0&c=20201002)
for ![i = 1, \\dots,
m](https://s0.wp.com/latex.php?latex=i+%3D+1%2C+%5Cdots%2C+m&bg=ffffff&fg=333333&s=0&c=20201002),
and ![h_i\(x^\\star\) =
0](https://s0.wp.com/latex.php?latex=h_i%28x%5E%5Cstar%29+%3D+0&bg=ffffff&fg=333333&s=0&c=20201002)
for ![i = 1, \\dots,
p](https://s0.wp.com/latex.php?latex=i+%3D+1%2C+%5Cdots%2C+p&bg=ffffff&fg=333333&s=0&c=20201002).

_(Sufficiency)_ For a convex optimization problem (i.e.
![f_i](https://s0.wp.com/latex.php?latex=f_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
convex and
![h_i](https://s0.wp.com/latex.php?latex=h_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
affine), if the KKT conditions hold for some ![\(\\tilde{x},
\\tilde{\\lambda},
\\tilde{\\nu}\)](https://s0.wp.com/latex.php?latex=%28%5Ctilde%7Bx%7D%2C+%5Ctilde%7B%5Clambda%7D%2C+%5Ctilde%7B%5Cnu%7D%29&bg=ffffff&fg=333333&s=0&c=20201002),
then
![\\tilde{x}](https://s0.wp.com/latex.php?latex=%5Ctilde%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\(\\tilde{\\lambda},
\\tilde{\\nu}\)](https://s0.wp.com/latex.php?latex=%28%5Ctilde%7B%5Clambda%7D%2C+%5Ctilde%7B%5Cnu%7D%29&bg=ffffff&fg=333333&s=0&c=20201002)
are primal and dual optimal, and there is zero duality gap.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F15%2Fwhat-are-the-kkt-conditions%2F&signup_flow=account)

Let
![x^\\star](https://s0.wp.com/latex.php?latex=x%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002)
and ![\(\\lambda^\\star,
\\nu^\\star\)](https://s0.wp.com/latex.php?latex=%28%5Clambda%5E%5Cstar%2C+%5Cnu%5E%5Cstar%29&bg=ffffff&fg=333333&s=0&c=20201002)
be the primal and dual optimal points respectively (i.e. points where the
primal and dual problems are optimized). The _**Karush-Kuhn-Tucker (KKT)**_
conditions refer to the following set of 4 assumptions:

_(Necessity)_ For any optimization problem for which [strong
duality](https://statisticaloddsandends.wordpress.com/2022/11/12/lagrange-
dual-weak-duality-and-strong-duality/) holds, any pair of primal and dual
optimal points must satisfy the KKT conditions.

_(Necessity & sufficiency)_ For a convex optimization problem that satisfies
[Slater’s
condition](https://statisticaloddsandends.wordpress.com/2022/11/12/lagrange-
dual-weak-duality-and-strong-duality/), then
![\\tilde{x}](https://s0.wp.com/latex.php?latex=%5Ctilde%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is primal optimal if and only if there exists ![\(\\tilde{\\lambda},
\\tilde{\\nu}\)](https://s0.wp.com/latex.php?latex=%28%5Ctilde%7B%5Clambda%7D%2C+%5Ctilde%7B%5Cnu%7D%29&bg=ffffff&fg=333333&s=0&c=20201002)
such that the KKT conditions are satisfied.

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [convex](https://statisticaloddsandends.wordpress.com/tag/convex/),
[kkt conditions](https://statisticaloddsandends.wordpress.com/tag/kkt-
conditions/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[strong duality](https://statisticaloddsandends.wordpress.com/tag/strong-
duality/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/15/what-are-
the-kkt-conditions/ "Permalink to What are the KKT conditions?").

![\\begin{aligned} L\(x, \\lambda, \\nu\) = f_0\(x\) + \\sum_{i=1}^m
\\lambda_i f_i\(x\) + \\sum_{i=1}^p \\nu_i h_i\(x\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+L%28x%2C+%5Clambda%2C+%5Cnu%29+%3D+f_0%28x%29+%2B+%5Csum_%7Bi%3D1%7D%5Em+%5Clambda_i+f_i%28x%29+%2B+%5Csum_%7Bi%3D1%7D%5Ep+%5Cnu_i+h_i%28x%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [convex](https://statisticaloddsandends.wordpress.com/tag/convex/),
[duality](https://statisticaloddsandends.wordpress.com/tag/duality/),
[lagrange dual](https://statisticaloddsandends.wordpress.com/tag/lagrange-
dual/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/),
[slater's condition](https://statisticaloddsandends.wordpress.com/tag/slaters-
condition/), [strong
duality](https://statisticaloddsandends.wordpress.com/tag/strong-duality/),
[weak duality](https://statisticaloddsandends.wordpress.com/tag/weak-duality/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/12/lagrange-
dual-weak-duality-and-strong-duality/ "Permalink to Lagrange dual, weak
duality and strong duality").

Since the above holds for any ![\\lambda \\geq
0](https://s0.wp.com/latex.php?latex=%5Clambda+%5Cgeq+0&bg=ffffff&fg=333333&s=0&c=20201002)
and any
![\\nu](https://s0.wp.com/latex.php?latex=%5Cnu&bg=ffffff&fg=333333&s=0&c=20201002),
maximizing over these variables gives us ![f_0\(\\tilde{x}\) \\geq
d^\\star](https://s0.wp.com/latex.php?latex=f_0%28%5Ctilde%7Bx%7D%29+%5Cgeq+d%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002).
Since this holds for all feasible
![\\tilde{x}](https://s0.wp.com/latex.php?latex=%5Ctilde%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002),
minimizing over all feasible
![\\tilde{x}](https://s0.wp.com/latex.php?latex=%5Ctilde%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002)
gives us ![p^\\star \\geq
d^\\star](https://s0.wp.com/latex.php?latex=p%5E%5Cstar+%5Cgeq+d%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002).

![\\begin{aligned} f_0\(\\tilde{x}\) &\\geq f_0\(\\tilde{x}\) + \\sum_{i=1}^m
\\lambda_i f_i\(\\tilde{x}\) + \\sum_{i=1}^p \\nu_i h_i\(\\tilde{x}\) \\\\
&\\geq \\inf_{x \\in \\mathcal{D}} \\left\\{ f_0\(x\) + \\sum_{i=1}^m
\\lambda_i f_i\(x\) + \\sum_{i=1}^p \\nu_i h_i\(x\) \\right\\} \\\\  &=
g\(\\lambda, \\nu\).
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+f_0%28%5Ctilde%7Bx%7D%29+%26%5Cgeq+f_0%28%5Ctilde%7Bx%7D%29+%2B+%5Csum_%7Bi%3D1%7D%5Em+%5Clambda_i+f_i%28%5Ctilde%7Bx%7D%29+%2B+%5Csum_%7Bi%3D1%7D%5Ep+%5Cnu_i+h_i%28%5Ctilde%7Bx%7D%29+%5C%5C++%26%5Cgeq+%5Cinf_%7Bx+%5Cin+%5Cmathcal%7BD%7D%7D+%5Cleft%5C%7B+f_0%28x%29+%2B+%5Csum_%7Bi%3D1%7D%5Em+%5Clambda_i+f_i%28x%29+%2B+%5Csum_%7Bi%3D1%7D%5Ep+%5Cnu_i+h_i%28x%29+%5Cright%5C%7D+%5C%5C++%26%3D+g%28%5Clambda%2C+%5Cnu%29.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

_**Strong duality**_ refers to the property that ![d^\\star =
p^\\star](https://s0.wp.com/latex.php?latex=d%5E%5Cstar+%3D+p%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002).
This does not always hold, but it can hold for important classes of problems.
Conditions which guarantee strong duality are known as _**constraint
qualifications**_.

![\\begin{aligned} g\(\\lambda, \\nu\) &= \\inf_{x \\in \\mathcal{D}} L\(x,
\\lambda, \\nu\) \\\\  &= \\inf_{x \\in \\mathcal{D}} \\left\\{ f_0\(x\) +
\\sum_{i=1}^m \\lambda_i f_i\(x\) + \\sum_{i=1}^p \\nu_i h_i\(x\) \\right\\}.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+g%28%5Clambda%2C+%5Cnu%29+%26%3D+%5Cinf_%7Bx+%5Cin+%5Cmathcal%7BD%7D%7D+L%28x%2C+%5Clambda%2C+%5Cnu%29+%5C%5C++%26%3D+%5Cinf_%7Bx+%5Cin+%5Cmathcal%7BD%7D%7D+%5Cleft%5C%7B+f_0%28x%29+%2B+%5Csum_%7Bi%3D1%7D%5Em+%5Clambda_i+f_i%28x%29+%2B+%5Csum_%7Bi%3D1%7D%5Ep+%5Cnu_i+h_i%28x%29+%5Cright%5C%7D.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

where the inequality above is understood to be element-wise. Let
![d^\\star](https://s0.wp.com/latex.php?latex=d%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002)
denote the optimal value of this problem.

![\\begin{aligned} f_i\(x\) < 0, \\quad i = 1, \\dots, m, \\quad \\text{and}
\\quad Ax = b.
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+f_i%28x%29+%3C+0%2C+%5Cquad+i+%3D+1%2C+%5Cdots%2C+m%2C+%5Cquad+%5Ctext%7Band%7D+%5Cquad+Ax+%3D+b.+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

This inequality holds all the time, even when the original problem is not
convex. The proof is straightforward. If
![\\tilde{x}](https://s0.wp.com/latex.php?latex=%5Ctilde%7Bx%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is feasible for the original problem, then for any ![\\lambda \\geq
0](https://s0.wp.com/latex.php?latex=%5Clambda+%5Cgeq+0&bg=ffffff&fg=333333&s=0&c=20201002)
and any
![\\nu](https://s0.wp.com/latex.php?latex=%5Cnu&bg=ffffff&fg=333333&s=0&c=20201002)
we have

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F12%2Flagrange-dual-weak-duality-and-strong-duality%2F&signup_flow=account)

with ![f_0, \\dots,
f_m](https://s0.wp.com/latex.php?latex=f_0%2C+%5Cdots%2C+f_m&bg=ffffff&fg=333333&s=0&c=20201002)
being convex functions. Convex optimization problems don’t always have strong
duality, but there are some simple constraint qualifications under which
strong duality holds. The most commonly cited one is _**Slater’s condition**_
, which seems fairly easy to satisfy.

Slater’s condition states that the problem admits a _strictly feasible_ point,
that is, a point
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
in the [relative interior](https://en.wikipedia.org/wiki/Relative_interior) of
![\\mathcal{D}](https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&bg=ffffff&fg=333333&s=0&c=20201002)
(the domain of the problem) such that

with the variable ![x \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Let
![\\mathcal{D}](https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&bg=ffffff&fg=333333&s=0&c=20201002)
be the domain for
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002),
i.e. the intersection of the domains of the
![f_i](https://s0.wp.com/latex.php?latex=f_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
and the
![h_i](https://s0.wp.com/latex.php?latex=h_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s.
Let
![p^\\star](https://s0.wp.com/latex.php?latex=p%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002)
denote the optimal value of the problem.

![\\begin{aligned} \\text{minimize} \\quad& f_0\(x\) \\\\  \\text{subject to}
\\quad& f_i\(x\) \\leq 0, \\quad i = 1, \\dots, m, \\\\  & Ax = b,
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Ctext%7Bminimize%7D+%5Cquad%26+f_0%28x%29+%5C%5C++%5Ctext%7Bsubject+to%7D+%5Cquad%26+f_i%28x%29+%5Cleq+0%2C+%5Cquad+i+%3D+1%2C+%5Cdots%2C+m%2C+%5C%5C++%26+Ax+%3D+b%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

The Lagrange dual problem is _always_ a convex optimization problem, because
the objective being maximized is concave (see [this previous
post](https://statisticaloddsandends.wordpress.com/2022/11/11/the-lagrange-
dual-function-is-always-concave/) for a proof) and the constraint is convex.
One reason the dual problem is of critical importance is that it provides a
lower bound on
![p^\\star](https://s0.wp.com/latex.php?latex=p%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002)
(it is the best lower bound that can be obtained from the Lagrange dual
function) and in some cases, gives the value of
![p^\\star](https://s0.wp.com/latex.php?latex=p%5E%5Cstar&bg=ffffff&fg=333333&s=0&c=20201002).

  1. Pingback: [What are the KKT conditions? | Statistical Odds & Ends](https://statisticaloddsandends.wordpress.com/2022/11/15/what-are-the-kkt-conditions/)

The _**Lagrange dual function**_ is the function ![g: \\mathbb{R}^m \\times
\\mathbb{R}^p \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=g%3A+%5Cmathbb%7BR%7D%5Em+%5Ctimes+%5Cmathbb%7BR%7D%5Ep+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
defined as the minimum value of the Lagrangian over
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002):

_**Slater’s theorem**_ says that strongly duality holds if Slater’s condition
holds and the problem is convex. A proof of this can be found in Section 5.3.2
of Reference 1; see Section 2.1.3 of the same reference for more details on
the concept of relative interior.

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2022%2F11%2F11%2Fthe-lagrange-dual-function-is-always-concave%2F&signup_flow=account)

with the variable ![x \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002).
Let
![\\mathcal{D}](https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&bg=ffffff&fg=333333&s=0&c=20201002)
be the domain for
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002),
i.e. the intersection of the domains of the
![f_i](https://s0.wp.com/latex.php?latex=f_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
and the
![h_i](https://s0.wp.com/latex.php?latex=h_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s.

Here’s a more explicit version of the proof. Let ![\\theta = \(\\lambda,
\\nu\) \\in \\mathbb{R}^{m +
p}](https://s0.wp.com/latex.php?latex=%5Ctheta+%3D+%28%5Clambda%2C+%5Cnu%29+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm+%2B+p%7D&bg=ffffff&fg=333333&s=0&c=20201002).
Fix ![\\theta_1, \\theta_2 \\in \\mathbb{R}^{m +
p}](https://s0.wp.com/latex.php?latex=%5Ctheta_1%2C+%5Ctheta_2+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm+%2B+p%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and ![t \\in \[0,
1\]](https://s0.wp.com/latex.php?latex=t+%5Cin+%5B0%2C+1%5D&bg=ffffff&fg=333333&s=0&c=20201002).
For each ![x \\in
\\mathcal{D}](https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathcal%7BD%7D&bg=ffffff&fg=333333&s=0&c=20201002),

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[concave](https://statisticaloddsandends.wordpress.com/tag/concave/),
[lagrange dual](https://statisticaloddsandends.wordpress.com/tag/lagrange-
dual/),
[optimization](https://statisticaloddsandends.wordpress.com/tag/optimization/)
by [kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2022/11/11/the-
lagrange-dual-function-is-always-concave/ "Permalink to The Lagrange dual
function is always concave").

The _**Lagrangian**_ associated with this problem is the function ![L:
\\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb{R}^p \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=L%3A+%5Cmathbb%7BR%7D%5En+%5Ctimes+%5Cmathbb%7BR%7D%5Em+%5Ctimes+%5Cmathbb%7BR%7D%5Ep+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
defined as

  1. Pingback: [Lagrange dual, weak duality and strong duality | Statistical Odds & Ends](https://statisticaloddsandends.wordpress.com/2022/11/12/lagrange-dual-weak-duality-and-strong-duality/)

![\\begin{aligned} L\(x, t\\theta_1 + \(1-t\)\\theta_2\) &= t L\(x,
\\theta_1\) + \(1-t\) L\(x, \\theta_2\) \\\\  &\\geq t g \(\\theta_1\) +
\(1-t\) g\(\\theta_2\),
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+L%28x%2C+t%5Ctheta_1+%2B+%281-t%29%5Ctheta_2%29+%26%3D+t+L%28x%2C+%5Ctheta_1%29+%2B+%281-t%29+L%28x%2C+%5Ctheta_2%29+%5C%5C++%26%5Cgeq+t+g+%28%5Ctheta_1%29+%2B+%281-t%29+g%28%5Ctheta_2%29%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

where the first equality uses the definition of
![L](https://s0.wp.com/latex.php?latex=L&bg=ffffff&fg=333333&s=0&c=20201002)
(or the fact that
![L](https://s0.wp.com/latex.php?latex=L&bg=ffffff&fg=333333&s=0&c=20201002)
is affine in
![\\theta](https://s0.wp.com/latex.php?latex=%5Ctheta&bg=ffffff&fg=333333&s=0&c=20201002)).
Since this holds for all ![x \\in
\\mathcal{D}](https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathcal%7BD%7D&bg=ffffff&fg=333333&s=0&c=20201002),
we can take an infimum over
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002)
to obtain ![g\(t\\theta_1 + \(1-t\)\\theta_2\) \\geq t g \(\\theta_1\) +
\(1-t\)
g\(\\theta_2\)](https://s0.wp.com/latex.php?latex=g%28t%5Ctheta_1+%2B+%281-t%29%5Ctheta_2%29+%5Cgeq+t+g+%28%5Ctheta_1%29+%2B+%281-t%29+g%28%5Ctheta_2%29&bg=ffffff&fg=333333&s=0&c=20201002),
i.e.
![g](https://s0.wp.com/latex.php?latex=g&bg=ffffff&fg=333333&s=0&c=20201002)
is concave.

![\\begin{aligned} L\(x, \\lambda, \\nu\) = f_0\(x\) + \\sum_{i=1}^m
\\lambda_i f_i\(x\) + \\sum_{i=1}^p \\nu_i h_i\(x\),
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+L%28x%2C+%5Clambda%2C+%5Cnu%29+%3D+f_0%28x%29+%2B+%5Csum_%7Bi%3D1%7D%5Em+%5Clambda_i+f_i%28x%29+%2B+%5Csum_%7Bi%3D1%7D%5Ep+%5Cnu_i+h_i%28x%29%2C+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

with domain ![\\mathcal{L} = \\mathcal{D} \\times \\mathbb{R}^m \\times
\\mathbb{R}^p](https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D+%3D+%5Cmathcal%7BD%7D+%5Ctimes+%5Cmathbb%7BR%7D%5Em+%5Ctimes+%5Cmathbb%7BR%7D%5Ep&bg=ffffff&fg=333333&s=0&c=20201002).
The _**Lagrange dual function**_ is the function ![g: \\mathbb{R}^m \\times
\\mathbb{R}^p \\mapsto
\\mathbb{R}](https://s0.wp.com/latex.php?latex=g%3A+%5Cmathbb%7BR%7D%5Em+%5Ctimes+%5Cmathbb%7BR%7D%5Ep+%5Cmapsto+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
defined as the minimum value of the Lagrangian over
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002):

The dual function is a very important function in optimization theory. _**One
interesting fact about the dual function is that it is always concave, even if
the original optimization problem is not convex.**_ The proof can be stated in
one-line: for each
![x](https://s0.wp.com/latex.php?latex=x&bg=ffffff&fg=333333&s=0&c=20201002),
![L\(x, \\lambda,
\\nu\)](https://s0.wp.com/latex.php?latex=L%28x%2C+%5Clambda%2C+%5Cnu%29&bg=ffffff&fg=333333&s=0&c=20201002)
is an affine function, and the point-wise infimum of a family of affine
functions is always concave.

The column space of an ![m \\times
n](https://s0.wp.com/latex.php?latex=m+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix
![\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002)
can be expressed as the set ![\\{ \\mathbf{A}x : x \\in \\mathbb{R}^m
\\}](https://s0.wp.com/latex.php?latex=%5C%7B+%5Cmathbf%7BA%7Dx+%3A+x+%5Cin+%5Cmathbb%7BR%7D%5Em+%5C%7D&bg=ffffff&fg=333333&s=0&c=20201002).
We can also write ![\\mathbf{A}x = \\displaystyle\\sum_{i = 1}^m x_i
A_i](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7Dx+%3D+%5Cdisplaystyle%5Csum_%7Bi+%3D+1%7D%5Em+x_i+A_i&bg=ffffff&fg=333333&s=0&c=20201002),
where
![A_i](https://s0.wp.com/latex.php?latex=A_i&bg=ffffff&fg=333333&s=0&c=20201002)
is the
![i](https://s0.wp.com/latex.php?latex=i&bg=ffffff&fg=333333&s=0&c=20201002)th
column of
![A](https://s0.wp.com/latex.php?latex=A&bg=ffffff&fg=333333&s=0&c=20201002).

Let ![\\mathbf{A} \\in \\mathbb{R}^{m \\times
n}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm+%5Ctimes+n%7D&bg=ffffff&fg=333333&s=0&c=20201002),
![x \\in
\\mathbb{R}^m](https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5Em&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=y+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002),
and let ![a = \\text{vec}\(\\mathbf{A}\) \\in
\\mathbb{R}^{mn}](https://s0.wp.com/latex.php?latex=a+%3D+%5Ctext%7Bvec%7D%28%5Cmathbf%7BA%7D%29+%5Cin+%5Cmathbb%7BR%7D%5E%7Bmn%7D&bg=ffffff&fg=333333&s=0&c=20201002)
be the [vectorized
form](https://en.wikipedia.org/wiki/Vectorization_\(mathematics\)) of
![\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002).
Then ![x^T \\mathbf{A} y = a^T \(x \\otimes
y\)](https://s0.wp.com/latex.php?latex=x%5ET+%5Cmathbf%7BA%7D+y+%3D+a%5ET+%28x+%5Cotimes+y%29&bg=ffffff&fg=333333&s=0&c=20201002),
where
![\\otimes](https://s0.wp.com/latex.php?latex=%5Cotimes&bg=ffffff&fg=333333&s=0&c=20201002)
is the [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product).

If ![\\mathbf{A} \\in \\mathbb{R}^{m \\times
n}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm+%5Ctimes+n%7D&bg=ffffff&fg=333333&s=0&c=20201002)
and we want a matrix
![\\mathbf{B}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BB%7D&bg=ffffff&fg=333333&s=0&c=20201002)
whose
![i](https://s0.wp.com/latex.php?latex=i&bg=ffffff&fg=333333&s=0&c=20201002)th
row is
![c_i](https://s0.wp.com/latex.php?latex=c_i&bg=ffffff&fg=333333&s=0&c=20201002)
multiplied by the
![i](https://s0.wp.com/latex.php?latex=i&bg=ffffff&fg=333333&s=0&c=20201002)th
row of
![\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002),
then ![\\mathbf{B} = \\text{diag} \(c_1, \\dots, c_n\)
\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BB%7D+%3D+%5Ctext%7Bdiag%7D+%28c_1%2C+%5Cdots%2C+c_n%29+%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002).  
If we want a matrix
![\\mathbf{B}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BB%7D&bg=ffffff&fg=333333&s=0&c=20201002)
whose
![i](https://s0.wp.com/latex.php?latex=i&bg=ffffff&fg=333333&s=0&c=20201002)th
column is
![c_i](https://s0.wp.com/latex.php?latex=c_i&bg=ffffff&fg=333333&s=0&c=20201002)
multiplied by the
![i](https://s0.wp.com/latex.php?latex=i&bg=ffffff&fg=333333&s=0&c=20201002)th
column of
![\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002),
then ![\\mathbf{B} =  \\mathbf{A}\\text{diag} \(c_1, \\dots,
c_n\)](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BB%7D+%3D%C2%A0+%5Cmathbf%7BA%7D%5Ctext%7Bdiag%7D+%28c_1%2C+%5Cdots%2C+c_n%29&bg=ffffff&fg=333333&s=0&c=20201002).

If ![\\mathbf{A} \\in \\mathbb{R}^{m \\times
n}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm+%5Ctimes+n%7D&bg=ffffff&fg=333333&s=0&c=20201002)
has columns ![A_1, \\dots, A_n \\in \\mathbb{R}^{m \\times
1}](https://s0.wp.com/latex.php?latex=A_1%2C+%5Cdots%2C+A_n+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm+%5Ctimes+1%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![\\mathbf{B} \\in \\mathbb{R}^{n \\times
p}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BB%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn+%5Ctimes+p%7D&bg=ffffff&fg=333333&s=0&c=20201002)
has rows ![B_1, \\dots, B_n \\in \\mathbb{R}^{1 \\times
n}](https://s0.wp.com/latex.php?latex=B_1%2C+%5Cdots%2C+B_n+%5Cin+%5Cmathbb%7BR%7D%5E%7B1+%5Ctimes+n%7D&bg=ffffff&fg=333333&s=0&c=20201002),
then ![\\mathbf{AB} = \\displaystyle\\sum_{i=1}^n A_i
B_i](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BAB%7D+%3D+%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5En+A_i+B_i&bg=ffffff&fg=333333&s=0&c=20201002).  
In particular, ![\\mathbf{A A}^T =\\displaystyle\\sum_{i=1}^n A_i
A_i^T](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA+A%7D%5ET+%3D%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5En+A_i+A_i%5ET&bg=ffffff&fg=333333&s=0&c=20201002),
where
![A_i](https://s0.wp.com/latex.php?latex=A_i&bg=ffffff&fg=333333&s=0&c=20201002)
are the columns of
![\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002),
and ![\\mathbf{A}^T \\mathbf{A} = \\displaystyle\\sum_{i=1}^m a_i
a_i^T](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D%5ET+%5Cmathbf%7BA%7D+%3D+%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5Em+a_i+a_i%5ET&bg=ffffff&fg=333333&s=0&c=20201002),
where
![a_i](https://s0.wp.com/latex.php?latex=a_i&bg=ffffff&fg=333333&s=0&c=20201002)
are the rows of
![\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002)
(written as column vectors).

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F03%2F27%2Fmatrix-expansions%2F&signup_flow=account)

If ![\\mathbf{A} \\in \\mathbb{R}^{m \\times
n}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm+%5Ctimes+n%7D&bg=ffffff&fg=333333&s=0&c=20201002),
![x \\in
\\mathbb{R}^m](https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathbb%7BR%7D%5Em&bg=ffffff&fg=333333&s=0&c=20201002)
and ![y \\in
\\mathbb{R}^n](https://s0.wp.com/latex.php?latex=y+%5Cin+%5Cmathbb%7BR%7D%5En&bg=ffffff&fg=333333&s=0&c=20201002),
then ![x^T \\mathbf{A} y = \\displaystyle\\sum_{i=1}^m\\sum_{j=1}^n x_i a_{ij}
y_j](https://s0.wp.com/latex.php?latex=x%5ET+%5Cmathbf%7BA%7D+y+%3D+%5Cdisplaystyle%5Csum_%7Bi%3D1%7D%5Em%5Csum_%7Bj%3D1%7D%5En+x_i+a_%7Bij%7D+y_j&bg=ffffff&fg=333333&s=0&c=20201002),
where
![a_{ij}](https://s0.wp.com/latex.php?latex=a_%7Bij%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is the element of
![\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002)
in row
![i](https://s0.wp.com/latex.php?latex=i&bg=ffffff&fg=333333&s=0&c=20201002)
and column
![j](https://s0.wp.com/latex.php?latex=j&bg=ffffff&fg=333333&s=0&c=20201002).

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[identities](https://statisticaloddsandends.wordpress.com/tag/identities/),
[linear algebra](https://statisticaloddsandends.wordpress.com/tag/linear-
algebra/),
[matrices](https://statisticaloddsandends.wordpress.com/tag/matrices/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/03/27/matrix-
expansions/ "Permalink to Matrix expansions and manipulations").

If
![\\mathbf{D}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BD%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is a diagonal matrix,
![\\mathbf{U}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BU%7D&bg=ffffff&fg=333333&s=0&c=20201002)
a matrix with columns
![U_j](https://s0.wp.com/latex.php?latex=U_j&bg=ffffff&fg=333333&s=0&c=20201002),
and
![\\mathbf{V}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BV%7D&bg=ffffff&fg=333333&s=0&c=20201002)
a matrix with columns
![V_j](https://s0.wp.com/latex.php?latex=V_j&bg=ffffff&fg=333333&s=0&c=20201002),
then ![\\mathbf{UDV}^T = \\displaystyle\\sum_j d_j U_j
V_j^T](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BUDV%7D%5ET+%3D+%5Cdisplaystyle%5Csum_j+d_j+U_j+V_j%5ET&bg=ffffff&fg=333333&s=0&c=20201002).

Similarly, the row space of
![\\mathbf{A}](https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&bg=ffffff&fg=333333&s=0&c=20201002)
can be expressed as the set ![\\{ \\mathbf{A}^Tx : x \\in \\mathbb{R}^n
\\}](https://s0.wp.com/latex.php?latex=%5C%7B+%5Cmathbf%7BA%7D%5ETx+%3A+x+%5Cin+%5Cmathbb%7BR%7D%5En+%5C%7D&bg=ffffff&fg=333333&s=0&c=20201002).

When doing matrix computations, I find it pretty hard to switch between the
matrix form and element-wise form. This is a reference for some of the more
common ones I come across. (Proofs are often just a matter of doing some
tedious algebra.)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2021%2F07%2F29%2Fgetting-predictions-from-an-isotonic-regression-model%2F&signup_flow=account)

    
    
     set.seed(2) test_x <- sample(2 * 1:15 - 1) test_y <- 0.2 * test_x + rnorm(length(test_x)) 

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged [isotonic
regression](https://statisticaloddsandends.wordpress.com/tag/isotonic-
regression/),
[monotone](https://statisticaloddsandends.wordpress.com/tag/monotone/),
[prediction](https://statisticaloddsandends.wordpress.com/tag/prediction/),
[R](https://statisticaloddsandends.wordpress.com/tag/r/),
[regression](https://statisticaloddsandends.wordpress.com/tag/regression/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2021/07/29/getting-
predictions-from-an-isotonic-regression-model/ "Permalink to Getting
predictions from an isotonic regression model").

A naive and WRONG way to calibrate `test_y` would be to run isotonic
regression on just the test data. The plot shows the fits for the training
data as blue dots and the fits for the testing data as red squares: the
overall fit is not monotonic.

The CORRECT way to make the isotonic regression model into a black box is to
pass the output of `isoreg` to the `as.stepfun` function, like so:

_**TLDR:** Pass the output of the `isoreg` function to `as.stepfun` to make an
isotonic regression model into a black box object that takes in uncalibrated
predictions and outputs calibrated ones._

Isotonic regression can be performed easily in R with the `stats` package’s
`isoreg` function. Note the slightly unusual syntax when pulling out the
fitted values (see the function’s documentation with `?isoreg` to understand
why this is the case). The plot shows the original data values as black
crosses and the fitted values as blue dots. As expected, the blue dots are
monotonically increasing.

Isotonic regression is one commonly used method for _**calibration**_ : see
[this previous
post](https://statisticaloddsandends.wordpress.com/2020/10/07/what-is-
calibration/) for background on calibration and [this
link](https://kingsubham27.medium.com/calibration-techniques-and-its-
importance-in-machine-learning-71bec997b661) for more details with python
code. In this setting, we want the isotonic regression model to be a black
box: we hand it uncalibrated predictions as an input, and it returns us
calibrated predictions.

_**Isotonic regression**_ is a method for obtaining a monotonic fit for
1-dimensional data. Let’s say we have data ![\(x_1, y_1\), \\dots, \(x_n,
y_n\) \\in
\\mathbb{R}^2](https://s0.wp.com/latex.php?latex=%28x_1%2C+y_1%29%2C+%5Cdots%2C+%28x_n%2C+y_n%29+%5Cin+%5Cmathbb%7BR%7D%5E2&bg=ffffff&fg=333333&s=0&c=20201002)
such that ![x_1 < x_2 < \\dots <
x_n](https://s0.wp.com/latex.php?latex=x_1+%3C+x_2+%3C+%5Cdots+%3C+x_n&bg=ffffff&fg=333333&s=0&c=20201002).
(We assume no ties among the
![x_i](https://s0.wp.com/latex.php?latex=x_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
for simplicity.) Informally, isotonic regression looks for ![\\beta_1, \\dots,
\\beta_n \\in
\\mathbb{R}](https://s0.wp.com/latex.php?latex=%5Cbeta_1%2C+%5Cdots%2C+%5Cbeta_n+%5Cin+%5Cmathbb%7BR%7D&bg=ffffff&fg=333333&s=0&c=20201002)
such that the
![\\beta_i](https://s0.wp.com/latex.php?latex=%5Cbeta_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
approximate the
![y_i](https://s0.wp.com/latex.php?latex=y_i&bg=ffffff&fg=333333&s=0&c=20201002)‘s
well while being monotonically non-decreasing. (See [this previous
post](https://statisticaloddsandends.wordpress.com/2020/05/24/what-is-
isotonic-regression/) for more technical details.)

A second WRONG way to calibrate `test_y</`code> would be to run isotonic
regression on the combined training/test data. The plot shows that while the
overall fit is monotonic, the predictions for the training data have shifted,
i.e. you have changed the black box.

If you inspect the return value of the `isoreg` function, you will find that
it is unable to interact with any new test data. Imagine that we have some new
test data that we want to calibrate:

    
    
     isofit <- as.stepfun(isoreg(x, y)) 

    
    
     plot(all_x, all_y, pch = 4) points(all_x, isofit(all_x), pch = 15, col = "red") points(fit$x[fit$ord], fit$yf, pch = 16, col = "blue") 

`isofit` is the black box we seek: a function that we give uncalibrated
predictions to get calibrated predictions in return. As the plot below shows,
the overall fit is still monotonic, and the calibrated predictions for the
training data do not change.

![\\begin{aligned} \\det \\begin{pmatrix} I_m & -A \\\\ B & I_n \\end{pmatrix}
&= \\det \(I_m\) \\det \(I_n - BI_m^{-1}\(-A\)\) \\\\  &= \\det \(I_n + BA\), 
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cdet+%5Cbegin%7Bpmatrix%7D+I_m+%26+-A+%5C%5C+B+%26+I_n+%5Cend%7Bpmatrix%7D+%26%3D+%5Cdet+%28I_m%29+%5Cdet+%28I_n+-+BI_m%5E%7B-1%7D%28-A%29%29+%5C%5C++%26%3D+%5Cdet+%28I_n+%2B+BA%29%2C%C2%A0+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

![\\det \(A + UWV^T\) = \\det \(W^{-1} + V^T A^{-1} U\) \\det \(W\) \\det
\(A\).](https://s0.wp.com/latex.php?latex=%5Cdet+%28A+%2B+UWV%5ET%29+%3D+%5Cdet+%28W%5E%7B-1%7D+%2B+V%5ET+A%5E%7B-1%7D+U%29+%5Cdet+%28W%29+%5Cdet+%28A%29.&bg=ffffff&fg=333333&s=0&c=20201002)

This entry was posted in
[Uncategorized](https://statisticaloddsandends.wordpress.com/category/uncategorized/)
and tagged
[determinant](https://statisticaloddsandends.wordpress.com/tag/determinant/),
[linear algebra](https://statisticaloddsandends.wordpress.com/tag/linear-
algebra/),
[matrices](https://statisticaloddsandends.wordpress.com/tag/matrices/) by
[kjytay](https://statisticaloddsandends.wordpress.com/author/kjytay/).
Bookmark the
[permalink](https://statisticaloddsandends.wordpress.com/2018/03/11/sylvesters-
determinant-identity/ "Permalink to Sylvester’s determinant identity").

_**Note:**_ There are more general versions of this identity as well (stated
in Reference 1). If ![A \\in \\mathbb{R}^{n \\times
n}](https://s0.wp.com/latex.php?latex=A+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn+%5Ctimes+n%7D&bg=ffffff&fg=333333&s=0&c=20201002)
is invertible and ![U, V \\in \\mathbb{R}^{n \\times
m}](https://s0.wp.com/latex.php?latex=U%2C+V+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn+%5Ctimes+m%7D&bg=ffffff&fg=333333&s=0&c=20201002),
then

Let
![A](https://s0.wp.com/latex.php?latex=A&bg=ffffff&fg=333333&s=0&c=20201002)
be an ![m \\times
n](https://s0.wp.com/latex.php?latex=m+%5Ctimes+n&bg=ffffff&fg=333333&s=0&c=20201002)
matrix and let
![B](https://s0.wp.com/latex.php?latex=B&bg=ffffff&fg=333333&s=0&c=20201002)
be an ![n \\times
m](https://s0.wp.com/latex.php?latex=n+%5Ctimes+m&bg=ffffff&fg=333333&s=0&c=20201002)
matrix. From our [previous
post](https://statisticaloddsandends.wordpress.com/2018/03/10/determinant-of-
a-block-matrix/), we have

This identity is especially useful for computing the determinant of the sum of
an identity matrix and a rank-1 matrix: if
![c](https://s0.wp.com/latex.php?latex=c&bg=ffffff&fg=333333&s=0&c=20201002)
is a ![m \\times
1](https://s0.wp.com/latex.php?latex=m+%5Ctimes+1&bg=ffffff&fg=333333&s=0&c=20201002)
matrix and
![r](https://s0.wp.com/latex.php?latex=r&bg=ffffff&fg=333333&s=0&c=20201002)
is a ![1 \\times
m](https://s0.wp.com/latex.php?latex=1+%5Ctimes+m&bg=ffffff&fg=333333&s=0&c=20201002)
matrix, then

![\\begin{aligned} \\det \\begin{pmatrix} I_m & -A \\\\ B & I_n \\end{pmatrix}
&= \\det \(I_n\) \\det \(I_m - \(-A\)I_n^{-1}B\) \\\\  &= \\det \(I_m + AB\). 
\\end{aligned}](https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cdet+%5Cbegin%7Bpmatrix%7D+I_m+%26+-A+%5C%5C+B+%26+I_n+%5Cend%7Bpmatrix%7D+%26%3D+%5Cdet+%28I_n%29+%5Cdet+%28I_m+-+%28-A%29I_n%5E%7B-1%7DB%29+%5C%5C++%26%3D+%5Cdet+%28I_m+%2B+AB%29.%C2%A0+%5Cend%7Baligned%7D&bg=ffffff&fg=333333&s=0&c=20201002)

    * Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstatisticaloddsandends.wordpress.com%2F2018%2F03%2F11%2Fsylvesters-determinant-identity%2F&signup_flow=account)

