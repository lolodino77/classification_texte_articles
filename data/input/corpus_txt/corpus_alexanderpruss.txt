_s_ 1( _x_ , _t_ ) = _a_ if _r_ < _x_ and _t_ = 1 or if _x_ < 1 − _r_ and _t_
= 0

This hypothesis initially seemed to me to have an unfortunate consequence.
Suppose Alice has just barely exceeded the threshold for knowledge of _p_ ,
and she is offered a cost-free piece of information that may turn out to
slightly increase or slightly decrease her overall evidence with respect to
_p_ , where the decrease would be sufficient to lose her knowledge of _p_
(since she has only “barely” exceeded the evidential threshold for knowledge).
It seems that Alice should refuse to look at the information, since the
benefit of a slight improvement in credence if the evidence is non-misleading
is outweighed by the danger of a significant and discontinuous loss of value
due to loss of knowledge.

If we think that it’s never rational for a rational agent to refuse free
information, then the above argument can be made rigorous to establish that
any discontinuous rise in the epistemic value of credence at the point at
which knowledge of a truth is reached is exactly mirrored by a discontinuous
fall in the epistemic value of a state of credence where seeming-knowledge of
a falsehood is reached. Moreover, the rise and the fall must be in the ratio 1
− _r_ : _r_ where _r_ is the knowledge threshold. Note that for knowledge, _r_
is plausibly pretty large, around 0.95 at least, and so the ratio between the
special value of knowledge of a truth and the special disvalue of evidence-
sufficient-for-knowledge for a falsehood will need to be at most 1:19. This
kind of a ratio seems intuitively implausible to me. It seems unlikely that
the special disvalue of evidence-sufficient-for-knowledge of a falsehood is an
order of magnitude greater than the special value of knowledge. This
contributes to my scepticism that there is a special value of knowledge.

But that’s not quite right. For from Alice’s point of view, because the
threshold for knowledge is not 1, there is a real possibility that _p_ is
false. But it may be that just as there is a discontinuous gain in epistemic
value when your (rational) credence becomes sufficient for knowledge of
something that is in fact true, it may be that there is a discontinuous loss
of epistemic value when your credence becomes sufficient for knowledge of
something false. (Of course, you can’t know anything false, but you can have
evidence-sufficient-for-knowledge with respect to something false.) This is
not implausible, and given this, by looking at the information, by her lights
Alice also has a chance of a significant gain in value due to losing the
illusion of knowledge in something false.

_s_ 1( _x_ , _t_ ) = − _b_ if _r_ < _x_ and _t_ = 0 or if _x_ < 1 − _r_ and
_t_ = 1.

Can we rigorously model this kind of an epistemic value assignment? I think
so. Consider the following discontinuous accuracy scoring rule _s_ 1( _x_ ,
_t_ ), where _x_ is a probability and _t_ is a 0 or 1 truth value:

Plausibly, then, if we imagine Alice has some evidence for a truth _p_ that is
insufficient for knowledge, and slowly and continuously her evidence for _p_
mounts up, when the evidence has crossed the threshold needed for knowledge,
the value of Alice’s state with respect to _p_ will have suddenly and
discontinuously increased.

Suppose that _a_ and _b_ are positive and _a_ / _b_ = (1− _r_ )/ _r_. Then if
my scribbled notes are correct, it is straightforward but annoying to check
that _s_ 1 is proper, and it has a discontinuous reward _a_ for meeting
threshold _r_ with respect to a truth and a discontinuous penalty  − _a_ for
meeting threshold _r_ with respect to a falsehood. To get a strictly proper
scoring rule, just add to it any strictly proper continous accuracy scoring
rule (e.g., Brier).

Suppose there is a distinctive and significant value to knowledge. What I mean
by that is that if two epistemic are very similar in terms of truth, the level
and type of justification, the subject matter and its relevant to life, the
degree of belief, etc., but one is knowledge and the other is not, then the
one that is knowledge has a significantly higher value because it is
knowledge.

_s_ 1( _x_ , _t_ ) = 0 if 1 − _r_ ≤ _x_ ≤ _r_

_P_ ( _p_ | _E_ ) = _P_ ( _p_ )/ _P_ ( _E_ ) = ( _r_ − _α_ )/( _r_ − _α_ + _β_
(1−( _r_ − _α_ ))).

Note that by appropriate choice of _β_ , we can make _z_ be anything between
_r_ − _α_ and 1, and the right-hand-side of (3) is at least _r_ − _α_ since
_r_ \+ _α_ ≤ 1. Thus we can make (c) definitely hold as long as the right-
hand-side of (3) is bigger than or equal to the right-hand-side of (4), i.e.,
if and only if:

(We definitely won’t meet _r_ on a negative test result.) Thus to get (c)
definitely true, we need (3) to hold as well as the probability of a positive
test result to be at least _r_ \+ _α_ :

we meet _r_ with respect to the proposition that we will meet the threshold
with respect to _p_ after we examine evidence _E_.

For suppose we have a credence threshold _r_ and that our intuitions agree
that we can’t have a situation where:

In [a recent post](http://alexanderpruss.blogspot.com/2023/01/knowing-you-
will-soon-have-enough.html), I noted that it is possible to cook up a Bayesian
setup where you don’t meet some threshold, say for belief or knowledge, with
respect to some proposition, but you do meet the same threshold with respect
to the claim that after you examine a piece of evidence, then you _will_ meet
the threshold. This is counterintuitive: it seems to imply that you can know
that you will have enough evidence to know something even though you don’t
yet. In a comment, Ian noted that one way out of this is to say that beliefs
do not correspond to sharp credences. It then occurred to me that one could
use the setup to probe the question of how sharp our credences are and what
the thresholds for things like belief and knowledge are, perhaps
complementarily to the considerations in [this
paper](https://philarchive.org/rec/PRUBSA).

Let _z_ = _r_ − _α_ \+ _β_ (1−( _r_ − _α_ )) = (1− _β_ )( _r_ − _α_ ) + _β_.
This is the prior probability of a positive test result. We will definitely
meet _r_ on a positive test result just in case we have ( _r_ − _α_ )/ _z_ =
_P_ ( _p_ | _E_ ) ≥ _r_ \+ _α_ , i.e., just in case

We thus have our joint constraint on the squishiness of our credences: bad
things happen if our credences are so precise as to make (6) true with respect
to a threshold _r_ for which we don’t want (a)–(c) to definitely hold. The
easiest scenario for making (a)–(c) definitely hold will be a binary test with
no false negatives.

Let _α_ > 0 be the “squishiness” of our credences. Let’s say that for one
credence to be definitely bigger than another, their difference has to be at
least _α_ , and that to definitely meet (fail to meet) a threshold, we must be
at least _α_ above (below) it. We assume that our threshold _r_ is definitely
less than one: _r_ \+ _α_ ≤ 1.

It’s in fact not hard to see that (6) is necessary and sufficient for the
existence of a case where (a)–(c) definitely hold.

Note that the squishiness of our credences likely varies with where the
credences lie on the line from 0 to 1, i.e., varies with respect to the
relevant threshold. For we _can_ tell the difference between 0.999 and 1.000,
but we probably can’t tell the difference between 0.700 and 0.701. So the
squishiness should probably be counted relative to the threshold. Or perhaps
it should be correlated to log-odds. But I need to get to looking at grad
admissions files now.

What does this tell us about _r_ and _α_? We can actually figure this out.
Consider a test for _p_ that have no false negatives, but has a false positive
rate of _β_. Let _E_ be a positive test result. Our best bet to generating a
counterexample to (a)–(c) will be if the priors for _p_ are as close to _r_ as
possible while yet definitely below, i.e., if the priors for _p_ are _r_ −
_α_. For making the priors be that makes (c) easier to definitely satisfy
while keeping (b) definitely satisfied. Since there are no false negatives,
the posterior for _p_ will be:

But what’s a reasonable threshold for belief? Maybe something like 0.9 or
0.95. At _r_ = 0.9, the squishiness needed for paradox is _α_ = 0.046. I
suspect our credences are more precise than that. If we agree that the
squishiness of our credences is less than 4.6%, then we have an argument that
the threshold for belief is _more_ than 0.9. On the other hand, at _r_ = 0.95,
the squishiness needed for paradox is 2.4%. At this point, it becomes more
plausible that our credences lack that kind of precision, but it’s not clear.
At _r_ = 0.98, the squishiness needed for paradox dips below 1%. Depending on
how precise we think our credences are, we get an argument that the threshold
for belief is something like 0.95 or 0.98.

We thus have our joint constraint on the squishiness of our credences: bad
things happen if our credences have a level of precision equal to the right-
hand-side of (6). What exactly that says about _α_ depends on where the
relevant threshold lies. If the threshold _r_ is 1/2, the squishiness _α_ is
0.15. That’s surely higher than the actual squishiness of our credences. So if
we are concerned merely with the threshold being more-likely-than-not, then we
can’t avoid the paradox, because there will be cases where our credence is
definitely below the threshold and it’s definitely above the threshold that
examination of the evidence will push us about the threshold.

The harm of violating one’s conscience only happens to one if one _willingly_
violates one’s conscience. If law enforcement physically prevents me from
doing something that conscience requires from me, then I haven’t suffered the
harm. Thus, interestingly, the consideration I sketched against violating
one’s conscience does not apply when one is literally forced (fear of
punishment, unless it is severe enough to suspend one’s freedom of will, does
not actually _force_ , but only incentives).

That said, I think a qualification is plausible. Some wrongdoings are minor,
and in those cases the harm to the wrongdoer may be minor as well. But in any
case, to get someone to act against their conscience in a matter that
according to their conscience is major is to do them grave harm, a harm not
that different from death.

In some cases the person of mistaken conscience will still do the wrong deed
despite the law’s contrary incentive. In such a case, both the perpetrator and
the victim may be slightly better off for the law. The victim has a dignitary
benefit from the very fact that the state says that the harm was unlawful.
That dignitary benefit may be a cold comfort if the victim suffered a grave
harm, but it is still a benefit. And the perpetrator is slightly better off,
because following one’s conscience _against_ external pressure has an element
of admirability even when the conscience is mistaken.

Nonetheless, there will be cases where these considerations do not suffice,
and the law should be tolerant of mistaken conscience.

In cases where doing wrong and suffering wrong are of roughly the same order
of magnitude, it is very intuitive that we should prevent the suffering of
wrong rather than the doing of wrong. Imagine that Alice is drowning while at
the same time Bob is getting ready to assassinate a politician, but we know
for sure that Bob’s bullets have all been replaced with blanks. If our choice
is whether to try to dissuade Bob from attempting murder or keep Alice from
drowning, we should keep Alice from drowning, evne if on the Socratic view the
harm to Bob from attempting murder will be greater than that to Alice from
drowning. (I am assuming that in this case the two harms are nonetheless of
something like the same order of magnitude.)

Now, acting against one’s conscience _is_ always wrong, and is almost always
_culpably_ wrong. For the most common case when doing something wrong isn’t
culpable is that one is ignorant of the wrongness, but when one acts against
one’s conscience one surely isn’t ignorant that one is acting against
conscience, and that we ought follow our conscience is obvious.

When someone’s conscience mistakenly requires something that violates an
objective moral rule, there is a two-fold benefit to that person from a law
incentivizing following the moral rule. The law is a teacher, and the state’s
disapproval may change one’s mind about the matter. And even if it a harm to
one to violate conscience, it is also a harm to one to do something wrong even
inculpably. Thus, the harm of violating conscience is somewhat offset by the
benefit from not doing something else that is wrong.

Now, the state, just like individuals, should _ceteris paribus_ avoid causing
grave harm. Hence, the state should generally avoid getting people to do
things that violate their conscience in major matters.

The difficult case, however, is when people’s consciences are mistaken to such
a degree that conscience requires them to do something that unjustly harms
others. (A less problematic mistake is when conscience is mistaken to such a
degree that conscience requires them to do something that’s permissible, but
not wrong. In those cases, tolerance is clearly called for. We shouldn’t
pressure vegetarians to eat animals even if their conscientious objection to
eating animals happens to be mistaken.)

In a just defensive war, to refuse to fight to defend one’s fellow citizens
without special reason (perhaps priests and doctors should not kill) is wrong.
But a grave harm is done to a conscientious objector who is gotten to fight by
legal incentives. Let’s think through the five considerations above. The first
mainly applies to laws prohibiting a behavior rather than ones requiring a
behavior. Short of brainwashing, it is impossible to _make_ someone fight. (We
could superglue their hands to a gun, and then administer electric shocks
causing their fingers to spasm and fire a bullet, but that wouldn’t count as
_fighting_.) The second applies somewhat: we do need to weigh the harms to
innocent citizens from enemy invaders, harms that might be prevented if our
conscientious objector fought. But note that there is something rather
speculative about these harms. Someone who fights contrary to conscience is
unlikely to be a very effective fighter, and it is far from clear that their
military activity would actually prevent any actual harm to innocents. Now,
regarding the third consideration, one can design a conscription law with an
exemption that few who aren’t conscientious objectors would take advantage of.
One way to do this is to require evidence of one’s conscience’s objection to
fighting (e.g., prior membership in a pacifist organization). Another way is
to impose non-combat duties on conscientious objectors that are as onerous and
maybe as dangerous as combat would be. Regarding the fourth consideration, it
seems unlikely that a typical conscientious objector’s objections to war would
be changed by legal penalties. And the fifth seems a weak consideration in
general. Putting all these together, we do not outweigh the _prima facie_
considerations against pressuring conscientious objectors to act against their
(mistaken) conscience from the harms in going against conscience.

A reasonable optimism says that in most cases most people’s consciences are
correct. Thus typically we would expect that most violators of a legitimate
law will not be acting out of conscience—for a necessarily condition for the
legitimacy of a law is that it does not conflict with a correct conscience.
Thus, even if there is the rare murderer acting from mistaken conscience, most
murderers act against conscience, and by incentivizing abstention from murder,
in most cases the law _helps_ people follow their conscience, and the small
number of other cases can be tolerated as a side effect. Thus the
considerations of conscience favor intolerant laws in such cases. Nonetheless,
there are cases where most violators of a law would likely be acting from
conscience. Thus, if we had a law requiring eating meat, we would expect that
most of the violators would be conscientious. Similarly, a law against
something—say, the wearing of certain clothes or symbols—that is rarely done
except as a religious practice would likely be a law most violators of which
would be conscientious.

One of the central insights of Western philosophy, beginning with Socrates,
has been that few if any things are as bad for an individual as culpably doing
wrong. It is better, we are told through much of the Western philosophical
tradition, that it is better to suffer than do injustice.

One might think that what I said earlier implies that in this difficult case
the state should always allow people to follow their conscience, because after
all it is worse to do wrong—and violating conscience is wrong—than to have
wrong done to one. But that would be absurd and horrible—think of a racist
murderer whose faulty conscience requires them to kill.

As long as the test could be a true negative, you could always be surprised
and update to P of 0.0 if the test is negative. So waiting on the test before
you update is justified.

If the test comes out positive, it will be another piece of evidence for the
hypothesis that I have _C_ , and it will push me over the edge to belief that
I have _C_.

Ian,  
  
I hadn't thought about this in the context of reflection. But technically
there is no counterexample here to van Fraassen's reflection principle, which
only says that P(H | credence at t will be p) = p. My examples satisfy that,
assuming full transparency about my own credences and full certainty that I
will conditionalize (cf.: https://jonathanweisberg.org/pdf/C_R_and_SKv2.SP.pdf
).  
  
The original reflection principle implies that my credence should be the
expected value of my future credences (at a fixed future time; we might also
generalize to any martingale stopping time). This is not a problem in my
cases, because although it is likely that my future credence will be bigger
than it is now, there is a small chance that my future credence will plummet
to zero due to a negative test result, as William notes.  
  
Upon further thought, I think it can be quite reasonable to say: "I know that
tomorrow I will have knowledge-level evidence for p, but I don't yet." And if
so, then my subsequent post on squishiness, even if technically correct, is
moot.

Suppose I am just the slightest bit short of the evidence needed for belief
that I have some condition _C_. I consider taking a test for _C_ that has a
zero false negative rate and a middling false positive rate—neither close to
zero nor close to one. On reasonable numerical interpretations of the previous
two sentences:

**Appendix:** Suppose the threshold for belief or knowledge is _r_ , where _r_
< 1. Suppose that the false-positive rate for the test is 1/2 and the false-
negative rate is zero. If _E_ is a positive test result, then _P_ ( _C_ | _E_
) = _P_ ( _C_ ) _P_ ( _E_ | _C_ )/ _P_ ( _E_ ) = _P_ ( _C_ )/ _P_ ( _E_ ) = 2
_P_ ( _C_ )/(1+ _P_ ( _C_ )). It follows by a bit of algebra that if my prior
_P_ ( _C_ ) is more than _r_ /(2− _r_ ), then _P_ ( _C_ | _E_ ) is above the
threshold _r_. Since _r_ < 1, we have _r_ /(2− _r_ ) < _r_ , and so the story
(either in the belief or knowledge form) works for the non-empty range of
priors strictly between _r_ /(2− _r_ ) and _r_.

I have enough evidence to believe that the test would come out positive.

To see that (2) is true, note that given that the false negative rate is zero,
and the false positive rate is not close to one, I will indeed have non-
negligible evidence for _C_ if the test is positive.

If I am rational, my beliefs will follow the evidence. So if I am rational, in
a situation like the above, I will take myself to have a way of bringing it
about that I believe, and do so rationally, that I have _C_. Moreover, this
way of bringing it about that I believe that I have _C_ will itself be
perfectly rational if the test is free. For of course it’s rational to accept
free information. So I will be in a position where I am rationally able to
bring it about that I rationally believe _C_ , while not yet believing it.

I don’t know that I have _C_ but I know that I will know.

I’d take this as an argument against the view that belief is (or is justified
by) credence above a threshold.  
  
But that view seems implausible in any case. It’s pretty much only in gambling
setups with well-defined chances that we have sharp credences. But we all have
vast numbers of beliefs (taken in an informal ‘folk’ sense). So it seems that
some other approach is needed.  
  
A thorough Bayesian would be untroubled by this example. She would have her
credences, and that would be enough. She would not care whether they exceeded
some arbitrary threshold for belief. She would be similarly untroubled by the
lottery paradox. But no one in practice is a thorough Bayesian, or even close.
So again, it seems that some other approach is needed.

I don’t yet have enough evidence to know that I have _C_ , but I know that in
a moment I will.

Ian:  
  
I don't think the worries about sharpness of credences apply. For we can
suppose that the cases we apply the argument to are precisely ones with
sufficiently sharp credences. They might be cases concerning gambling
scenarios, or they could be medical cases where the only relevant data one has
is statistics from the most recent publications on the subject.  
  
I think all we really need for the argument is that in certain kinds of cases
meeting a credence threshold is what makes the difference between not having a
belief (or a justified belief) and having a belief (or a justified belief).  
  
By the way, suppose we think that the argument doesn't work for sharpness
reasons. Let's say that u is a level of sharpness such that it only makes
sense to say that you've met the threshold if you are at r+u or higher and
that you haven't met it if you are at r-u or lower, and if you're between r-u
and r+u it's indeterminate. Then working through the Bayesian details should
provide one with an interesting joint constraint on r and u. And then we can
ask see if it's reasonable to think that r and u actually satisfy the joint
constraint.

But (6) is absurd: if I know that I will know something, then I am in a
position to know that the matter is so, since that I will know _p_ entails
that _p_ is true (assuming that _p_ doesn’t concern an open future). However,
there is no similar absurdity in (5). I may know that I will have enough
evidence to know _C_ , but that’s not the same as knowing that I will _know_
_C_ or even be in a position to know _C_. For it is possible to have enough
evidence to know something without being in a position to know it (namely,
when the thing isn’t true or when one is Gettiered).

In fact, the same thing can be said about knowledge, assuming there is
knowledge in lottery situations. For suppose that I am just the slightest bit
short of the evidence needed for _knowledge_ that I have _C_. Then I can set
up the story such that:

Still, there is something odd about (5). It’s a bit like the line:

If the test comes out positive, I will have enough evidence to know that I
have _C_.

I have enough evidence to _know_ that the test would come out positive,

To be clear, I don’t doubt that the example works. It does, and it illustrates
a genuine problem with the view that belief is (or is justified by) credence
above a threshold. Specifically, this violates an apparently natural
reflection principle, that if I rationally believe that I will rationally come
to believe, then I should believe now. Issues like this seem unavoidable with
a threshold (other than 1). The lottery paradox illustrates a different one:
violation of logical closure. Of course, you may be prepared to live with such
issues - maybe beliefs don’t have to be logically consistent.  
  
  
My problem with the threshold approach is more basic. If I don’t have a
credence, I can’t apply the threshold test. If I do have a credence, then
noting whether it is above or below a threshold adds nothing useful. One
response to this is straight Bayesianism, which rejects any role for belief
(beyond credence 1). Another is that credence and belief are separate and
related, but not straightforwardly. In the example, I would say that I
_believe_ the relevant medical information about the chance that I have C and
about the reliability of the test, and set my _credences_ accordingly.

To see that (1) is true, note that the test is certain to come out positive if
I have _C_ and has a significant probability of coming out positive even if I
don’t have _C_. Hence, the probability of a positive test result will be
significantly higher than the probability that I have _C_. But I am just the
slightest bit short of the evidence needed for belief that I have _C_ , so the
evidence that the test would be positive (let’s suppose a deterministic
setting, so we have no worries about the sense of the subjunctive conditional
here) is sufficient for belief.

In other words, oddly enough, just prior to getting the test results I can
reasonably say:

Dr Pruss, do you think that Bradley type regresses must also stop in some sort
of self explanatory fact? For example instead of having an infinite regress of
relations being related to their relata maybe it is somehow self explanatory
that the relation is related to its relata?

I am not sure I buy (1). But it sounds kind of right to me now. Additionally,
(3) kind of sounds correct on its own. If _A_ causes _B_ and _B_ causes _C_
but there is no explanation of _A_ , then it seems that _B_ and _C_ are really
unexplained. Aristotle notes that there was a presocratic philosopher who
explained why the earth doesn’t fall down by saying that it floats on water,
and he notes that the philosopher failed to ask the same question about the
water. I think one lesson of Aristotle’s critique is that if it is unexplained
why the water doesn’t fall down it is unexplained why the earth falls down.

I have a problem with (2). (2) only follows if a complete explanation is
possible, which is kind of what you want to prove. So, it seems to me (2) begs
the question.  
If the 'water' is a brute fact, 'the earth floats on water' is as far as an
explanation can get. In a way it is merely partial but it isn't actually part
of something complete because there isn't something complete.  
That's why (1) is 'kind of right' but it isn't completely right.  
  
  
  

A partial explanation is one that is a part of a complete explanation.

Any explanation for an event _E_ that does not go all the way back to
something self-explanatory is merely partial.

An explanation going back to something self-explanatory involves the activity
of a necessary being.

So, if any event _E_ has an explanation, it has an explanation going all the
way back to something self-explanatory. (1,2)

_x_ is good and _x_ is _F_ if and only if _x_ is a good _F_

On the "male human" point: it does seem like I can say "I am good and a man
iff I am a good man." But then (if we take the above approach) it seems like I
am centrally (really, deep-down) a man, not merely a human. This seems to sit
less comfortably with the Athanasian point. Perhaps this depends on the notion
of gender-specific norms? I'm not enough of a philosopher of gender to know
the literature on things like that.  
  

Necessarily, _x_ is a good _F_ if and only if _x_ is good.

Maybe, but can you say: What it is for you to be good is for you to be a good
man?  
  
Maybe the issue is that there is an ambiguity. A man (in the relevant sense)
is an adult male human. But "good adult male human" is a bit ambiguous: is one
good at the adulting, the maleness or the humanness, or at all three?  
  
I would say that what makes you be good is that you are good at being human,
and given that you are a man, this may have certain implications that it
wouldn't have if you weren't a man. (E.g., that you are a man makes it bad for
you to say "I am not a man".)

So our initial account of what is being asked for is that we are asking for an
attribute _F_ such that:

What am I? A herring-eater, a husband, a badminton player, a philosopher, a
father, a Canadian, a six-footer and a human are all correct answers. There is
an ancient form of the “What is _x_?” question where we are looking for a
“central” answer, and of course “a human” is usually taken to be that one.
What makes for that answer being central?

So the sense of the “What am I centrally, really, deep-down, essentially?”
question isn’t just modal. What is it?

In other words, if I am a human, being a good human explains my being good. On
the other hand, even if I were a virtuous human, my being a good virtuous
human would not explain my being good. For redundancy is to be avoided in
explanation, and “good virtuous human” is redundant.

Sometimes the word “essential” is thrown in: I am _essentially_ human. But
what does that mean? In contemporary analytic jargon, it just means that I
cannot exist without being human. But the “central” answer to “What am I?” is
not just an answer that states a property that I cannot lack. There are, after
all, many such properties essential in such a way that do not answer the
question. “Someone conceived in 1972” and “Someone in a world where 2+2=4”
attribute properties I cannot lack, but are not the central answers.

_x_ is “centrally, really, deep-down, essentially” _F_ just in case what it is
for _x_ to be good is for _x_ to be a good _F_.

In other words, that which I am centrally, really, deep-down and essentially
is that which sets the norms for me to be good _simpliciter_.

is not _generally_ logically valid. But some instances of a schema can be
logically valid even if the schema is not logically valid in general.

Andrew:  
  
Nice paper!  
  
But I think classification is not what I am getting at. I am aptly classified
as a male human, a human, a mammal, a vertebrate, an animal, an organism, and
an enmattered thing. However, I think only "human" is the answer to the
question I am after, the "what am I really" question. I am looking for a
particular "deep" classification. This is neither the most general
classification nor the most specific one, but the one that captures what most
deeply I am.  
  
On Athanasian theology, what makes me be saved is that the second person of
the Trinity became human just as I am human. He also became a male human just
as I am a male human and he became a vertebrate just as I am a vertebrate. But
these things do not save me. What saves me is his co-humanity, not his co-
vertebricity or co-animality or co-maleness. There is something deeper about
humanity than about any of these other things.  
  
This may be ethically important, too. Should I care more for a squid or a
mouse, if their intelligence is equal? The mouse is a fellow vertebrate, but
so what? It makes no difference. Similarly, that someone is a fellow male
matters not a whit. However, that someone is a fellow human, that seems to
matter.

Necessarily, I am good and a virtuous human if and only if I am a good
virtuous human.

Necessarily, if I am human, what it is for me to be good is for me to be a
good human.

And yet “a virtuous human” would not be the answer to the ancient “What am I
centrally, really, deep-down, essentially?” question even if I were in fact a
virtuous human.

**Response:** I deny that it’s absurd. It’s just that all the electrons we
meet _are_ good at electronicity.

But the same is not true for any other attribute besides “human” among those
of the first paragraph. I can be good and a badminton player while not being a
good badminton player, and I can be a good herring-eater without being good
and a herring-eater. And I have no idea what it is to be a good six-footer,
but perhaps the fact that I am a quarter of an inch short of six feet right
now makes me not be one. (In some cases one direction may hold. It may be that
if I am good and a human, then I am a good human.)

**Objection 2:** “Good” is attributive, and hence there is no such thing as
being good _simpliciter_.

Necessarily, I am good and a human if and only if I am a good human.

But perhaps we can do better. The necessary biconditional (2) holds in the
case “virtuous human”, but in a kind of trivial way: “a good virtuous human”
is repetition. I think that, as often, we need to pass from a modal to a
hyperintensional characterization. Consider that not only is (1) true, but
also:

**Objection 1:** Some things are “centrally” electrons, but something’s being
good at electronicity is absurd.

Another interpretation of the question of what we are is that it is asking for
*classification*. Generics can be useful here, because they (a) are apt
answers to classification questions and (b) still allow for exceptions. The
former feature makes them substantive. The latter makes them modest and immune
to certain styles of counterexample. For more on this (with respect to the
question of what *we* are, but the tools at play are widely applicable), see:
https://andrewmbailey.com/GenericAnimalism.pdf

Consider worlds created by God that contain four hundred people, each of whom
has an independent 1/2 chance of freely choosing to eat an orange tomorrow
(they love their oranges). Let _p_ be the proposition that at least one of
these 400 people will freely choose to eat an orange tomorrow. The chance of
not- _p_ in any such world will be (1/2)400 < 1/10100. Assuming open theism,
so God doesn’t just directly know whether _p_ is true or not, God will take
the probability of _p_ in any such world to be bigger than 1 − (1/10100) and
by (1) God will believe _p_ in these worlds. But in some of these worlds, that
belief will turn out to be false—no one will freely eat the orange. And this
violates (3).

I’d reject (1) in any case. Perfectly rational beings would have no use for
belief. They would have consistent credences (which could be 0 or 1) for
everything. On this view, beliefs are a shortcut that we, with our limited
mental capacity, use to reduce processing load.

A rational being believes anything that they take to have probability bigger
than 1 − (1/10100) given their evidence.

Speaking of "God's beliefs" seems about as coherent as discussing what God ate
for lunch. He's not a finite creature.

These three theses, together with some auxiliary assumptions, yield a serious
problem for open theism.

Alex  
  
As Ian says, (1) applies to finite creatures, it doesn't apply to an
omniscient being. On open theism, God knows everything that can be known, but
libertarian free choices cannot be known in advance. If God were to "guess"
what I will do tomorrow, of course on open theism, he could turn out to be
wrong, but it would be a guess and not a belief.

Ian:  
  
I am somewhat inclined to say that a high credence just *is* what a belief is.  
  
THToD:  
  
Like you and Alston, I am inclined to agree that "belief" is at best an
awkward way to describe God's cognition. However, I think a major part of the
reason why God doesn't fundamentally have beliefs is that there is no room for
a gap in God between cognition and reality. Whether an open theist can have
the "no gap" view of God depends on the variety of open theism. If the open
theist is an open futurist, who thinks there are no facts in reality about
future contingents, then they can have the "no gap" view. But an open theist
like Swinburne or van Inwagen who thinks there are facts about future
contingents and God doesn't know them thinks there is a gap between God's
knowledge and reality, and now God is sounding very much like us -- there is
stuff that God is uncertain about.

I suppose the best way out is for the open theist to deny (1).

Clearly the follow-through is _intended_ —it’s consciously planned, aimed at,
etc. But it need not be a means to anything one cares about in the game
(though, of course, in some cases it can be a means to impressing the
spectators or intimidating an opponent). But is it an end? It seems pointless
as an end!

This is interesting action-theoretically. The follow-through appears
pointless, because the agent’s interest is in what happens _before_ the
follow-through, the impact’s having the right physical properties, and yet
there is surely no backwards causation here. But there not appear to be an
effective way to reliably secure these physical properties of the impact
except by trying for the follow-through. So the follow-through itself is
pointless, but one’s _aiming at_ or _trying for_ the follow-through very much
has a point. And here the order of causality is respected: one swings aiming
at the follow-through, which causes an impact with the right physical
properties, and the swing then continues on to the “pointless” follow-through.

Another move is this. We actually have a normative power to make something
_be_ an end. And then it becomes genuinely worth pursuing, because we have
adopted it as an end. So the player first exercises the normative power to
make follow-through be an end, and then pursues that end as an end.

As far as I know, in all racquet sports players are told to follow-through: to
continue the racquet swing after the ball or shuttle have left the racquet.
But of course the ball or shuttle doesn’t care what the racquet is doing at
that point. So what’s the point of follow-through? The usual story is this: by
aiming to follow-through, one hits the ball or shuttle better. If one weren’t
trying to follow-through, the swing’s direction would be wrong or the swing
might slow down.

Maybe we should say this. Even if all intentional action is end-directed,
there are two kinds of reasons for an action: the reasons that come from the
value of the end and the reasons that come from the value of the pursuit of
that end. In the case of follow-through, there may be a fairly trivial success
value in the follow-through—a success value that comes from one’s exercise of
normative power in adopting the follow-through as one’s end—but that success
value provides only fairly trivial reasons. However, there can be
significantly non-trivial reasons for one’s pursuing that end, reasons
independent of that end.

Yet it seems that whatever is intended is intended as a means or an end. One
might reject this principle, taking follow-through to be a counterexample.

But there is a problem here. For even if there is a “success value” in
accomplishing a self-set goal, the strength of the reasons for pursuing the
follow-through is also proportioned to facts independent of this exercise of
normative power. Rather, the reasons for pursuing the follow-through will
include the internal and external goods of victory (winning as such, prizes,
adulation, etc.), and these are independent of one’s setting follow-through as
one’s goal.

Alex  
  
How does it follow from the idea that God is not simple that God’s beliefs
change as the reality they are about changes?  
  
If (2) is true, isn't that a problem for divine simplicity as well? For on
divine simplicity, God id identical to God's beliefs, so if (2) is true it
seems that God changes as the reality changes, which conradicts divine
simplicity as well as divine immutability.

God is not simple, and in particular God’s beliefs are proper parts of God.

I think one can hold that God's beliefs are accidents of his which are not
identical to him (rejecting a strong doctrine of divine simplicity) without
holding that they are "parts" of him in any sense that makes (3) or (4)
absurd. Even for created beings like us, the accident-substance relationship
does not appear to be at all the same as the part-whole relationship as common
sense conceives it. (I would find it weird to say that my beliefs are a part
of me in the same sense that my arm is a part of my body, for instance.)

It depends on the details of how they think beliefs work. But suppose they
hold to this picture: for each proposition p that God knows, there is God's
act of believing p. Moreover, these opponents of simplicity think that God's
act of believing p is a different act for each different p that God believes.
These different acts are on that view something like accidents of God. Thus,
by raising one's arm one brings it about that God's belief that one's arm is
in a lowered state does not exist and that God's belief that one's arm is in a
raised state does exist, and so one destroys a part of God and creates a new
one. (It is a part of the package that I am criticizing that propositions are
tensed, and so it's not enough for God to eternally know that at t1 the arm is
lowered and at t2 the arm is raised.)

How? Easy. I am now sitting, and God knows that. So, a part of God is the
belief that I am sitting. But I can destroy that belief of God’s by standing
up! For as soon as I stand up, the belief that I am sitting will no longer
exist. But on the view in question, God’s beliefs are parts of him. So by
standing up, I would bring it about that a part of God doesn’t exist.

I agree with Walter. It seems to me that 2 is more directly incompatible with
God's atemporality than with his lack of complexity as such. An atemporal God
who sees Creation across its history in toto as a block, whether He is simple
or complex, has only true beliefs about everything that has been or will be,
and about when each fact will be true for finite creatures who are temporal.
But his beliefs don't change.  
  
Although a God who changed with and experienced time such that his beliefs
also changed might be thus considered complex across time by having temporal
parts, that would seem to be assuming an A-theory of time for all (including
God), then importing a B perspective at the end, incoherently. So, simplicity
of essence (at any point in time) and atemporality could be distinguished I
suppose, such that a person could affirm both simplicity and temporality of
God. I'm not saying that any open theist holds this opinion, or should, but it
seems a logical possibility from within that framework.

I'm not sure that the creaturely changes are necessarily "destroying" (or
creating) parts of God (beliefs) on the open theists' assumption. I think they
are determining them. I will assume open theists who believe God is maximally
knowledgeable, that is, knows all that can be known, which does not include
perfect knowledge of the future according to them.  
  
On this set of assumptions God will still, however, have perfect foreknowledge
of all possible outcomes as he can understand all possible cause/effect
branches as First Cause. He just doesn't know which path along the branching
options will eventuate before the time that they do. This means his knowledge
grows only in the sense that he is able to pick out which of each fully
foreseen branch is actualised. In other words, the only way his knowledge
grows is by the steady lengthening of a "track" through a foreknown "space" of
possible world-states over time. In such a scenario, creatures with free will
would determine the direction of the track and thus the precise determination
of God’s knowledge of it, but they would not be adding to or subtracting from
God's beliefs in any quantitative sense. The addition that would occur comes
about automatically as time flows, no matter what finite agents decide.  
  
Please note, I do not hold this position, but it seems to me it is not guilty
of the exact absurdity alleged here, whatever its other flaws.  

I agree that one could hold 1 without 2. But my argument is against people who
hold the package of 1 and 2. Think here of process theologians, and their more
orthodox cousins.

And of course by standing up, I bring it about that a new divine belief
exists. So:

However, if God exists, we would _expect_ there to be an unbounded upward
qualitative hierarchy of possible finite goods. God is infinitely good, and
finite goods are participations in God, so we would expect a hierarchy of
qualitatively greater and greater types of good that reflect God’s infinite
goodness better and better.

Why not? Here is a hypothesis. There are so very many possible much greater
goods—goods qualitatively and not just quantitatively much greter—that it
would be easy to suppose that God’s permitting the trivial evil could promote
or enhance one of these goods to a degree sufficient to yield justification.

There is, however, a difference between the cases. Many great ordinary goods
that dwarf trivial evils, like the courage of a Socrates, are known to us. Few
if any finite goods that dwarf horrendous evils are known to us. Nonetheless,
if theism is true, it is very likely that such goods are possible. And since
the argument from evil is addressed against the theist, it seems fair for the
theist to invoke that hierarchy.

Unknown  
  
I wouldn't say "forced" is the correct word here, but it is true that God
_necessarily_ loves you. So, in a sense, God _did/does_ have to love you.

On the other hand, if we think of _horrendous_ evils, like the torture of
children, it is difficult to think of much greater goods. Maybe with
difficulty we can come up with one or two possibilities, but not enough to
make it _easy_ to suppose a justification for God’s permission of the evil in
terms of the goods.

Unknown  
  
So, God's actions do not mean much if He can't interact with people
negatively?

Moreover, we might ask whether our ignorance of goods higher up in the
hierarchy of goods beyond the ordinary goods is not itself evidence against
the existence of such goods. Here, I think the answer is that it is very
little evidence. We would expect any particular finite being to be able to
recognize only a finite number of types of good, and thus the fact that there
are only a finite number of goods that we recognize is very little evidence
against the hypothesis of the upward hierarchy of goods.

Nobody seriously runs an argument against the existence of God from _trivial_
evils, like hangnails or mosquito bites.

There seems to be a legitimate reason to allow evil choices to exist and to
allow those evil choices to affect other people. Seemingly, actions wouldn't
mean much (such as freely loving God) if we were all in cubicles and couldn't
actually interact with people negatively. Free love and free hatred seems to
be two sides of the same coin. I would also say that there are legitimate
reasons for suffering to exist. Perhaps the world would have less good in it
if there is no suffering. Would God permit suffering to allow more good? I
don't see a reason as to why He wouldn't. Theodicies and defenses are very
interesting, though it seems to me that, intuitively at least, most theodicies
seem to deal with the problem of trivial evils.

The reason why few people seriously run an argument from trivial evils is
simply because the case is much more obvious in the case of horrendous evil.  
But a truly opnipotent being should be able to accomplish Evert good without
any evil, unless this is logically impossible  
So, unless there is a convincing argument for why a certain good cannot
posdibly be accomplished without permitting evil, the default position should
be that it is possible.  

Perhaps not negatively, but certainly 'without love'. It seems to me that God
did not have to create you and thus God did not have to love you. If God were
forced to love you or loved you randomly, I wouldn't consider God to be a
moral being. I also wouldn't consider love to be a moral good if it were
forced. Therefore, it seems possible that evil actions are merely privations
of good actions, and thus one cannot choose to do "nothing" because "nothing"
is a lack of good, which seems to be some sort of evil. Perhaps one could make
a distinction between evils, but it seems that this might be a better
distinction that the one I previously drew.

So if God exists, we would expect there to be unknown possible finite goods
that are related to the known horrendous evils in something like the
proportion in which the known great finite goods are related to the known
trivial evils. Thus, if God exists, there very likely is  
an upward hierarchy of possible goods to which the horrendous evils of this
life stand like a mosquito bite to the courage of a Socrates. If we believe in
this hierarchy of goods, then it seems we should be no more impressed by the
atheological evidential force of horrendous evils than the ordinary person is
by the atheological evidential force of trivial evils.

Wesley:  
  
"by that same logic one could maybe deny one truly possesses being or goodness
or value"  
  
There is precedent for saying something like that. "No one is good but God
alone" (Mark 10:8).  
  
There has got to be a sense, and a very important one, in which what Jesus
says is true. But there is also a sense in which we are good--but only good-
by-participation.

Walter:  
  
Good point.  
  
I propose to revise premise 2 to read:  
2*. There are some highly accomplished individuals such that for them humility
is an appropriate attitude only if God exists.  
  
Then we don't even need premise 3.  
  
To respond to your point, now, I suggest that we think of highly accomplished
individuals who did not have many of the kinds of advantages you list, and who
were hindered in various ways (e.g., by racism, sexism, poverty, war, etc.).
While no doubt everyone got some help from other humans, in some cases a
realistic appraisal of the degree of that help, especially when combined with
the degree to which fellow humans hindered the individual, will not suffice
for humility to be appropriate *if* God does not exist.

@Alex I think an important nuance here is that in this specific view of
participation, goodness-by-participation isn't actually **true or intrinsic**
goodness. But this is problematic because we do have other verses affirming
the true goodness of creatures, such as Genesis 1 explicitly affirms this, as
does 1 Timothy 4 (first four verses on marriage & food) and Matthew 10
(sparrows).  
  
The main problem would be that this view of participationism _wouldn't just
make it impossible_ to take pride or "brag" about something we did (which I
can concede is wrong), but that it also makes it impossible to actually say we
are good or have value as well. In the **same manner & for the same reasons**
we can't take pride in or brag about our actions, we also can't affirm we are
**good,** or have being, and this seems to be a big problem with that specific
account.  
  
And it's also important to point out that Scripture often uses **hyperbolic
negation** or hyperbolic merism - God for example in Jeremiah 7:22 apparently
denies He ever commanded the Hebrews to do sacrifice, yet that's not a literal
negation but a hyperbolic one to point out that loving God is more important -
rhetorically DENYING one thing to point out the greater importance of another,
without intending to truly deny the importance of the secondary. Same thing
with loving Christ & hating one's parents - intentionally hyperbolic contrasts
that actually convey a hierarchy of love, but not pure exclusivity.  
  
So basing a very specific view of participationism (because not all models of
participationism would agree that we aren't actually truly good) on a phrase
that is likely using intentional rhetorical hyperbole is more speculative than
solid.

Here’s another way to think about it. Given (1) and (3), we need an
explanation of how it is that humility is an appropriate attitude for a highly
accomplished individual. Classical theism’s doctrine of participation provides
such an explanation: all the efforts and all the accomplishments are not truly
theirs but a participation in God’s perfection.

1) I think the participationism explanation is problematic, since by that same
logic one could maybe deny one truly possesses being or goodness or value; one
could even deny one causes anything at all, concluding occasionalism. But we
do have our own being & goodness, so it's not the divine being & goodness in
us, per Aquinas.  
  
  
2) In fact, there's a real sense in which our actions & thereby
accomplishments are uniquely our own in a way they aren't God's precisely
because our actions are rooted in our secondary causality, which is distinct
from God's causality. And secondary causality being distinct from primary
causality, the causal responsibility for secondary causal acts is in the
creature, since the primary causality of God in sustaining our actions in
existence only determins them insofar as they exist, but we determine them
insofar as whether we cause them to occur (the occurrence of them is distinct
from their basic existence) and what we cause to occur.  
  
3) Additionally, there may be a difference between bragging and general pride
- some moral theology manuals seem to suggest being proud of one's
accomplishments isn't sinful, because that's distinct from bragging morally.
And one can't be proud of things one doesn't truly possess or cause in some
way.  
  
Of course, other sources say any form of pride is sinful, so this isn't a
clear issue it seems - but if one did take the route that some forms of pride
ARE morally okay, then this makes the thesis we don't possess anything,
whether being or our actions, unlikely.

This seems question-begging. It's not because highly accomplished individuals
have a lot to brag about that they can brag about everything.  
  
Suppose you think this is a great argument, you certainly have reason to brag
about it, but even if there is no God, you also have reason to be humble,
because you may have been the first to come up with this particular argument,
but you could only have built a successful argument because of your education,
because of what you have studied, read, even the way your parents raised you.  
Nothing we do is exclusively our own accomplishment.  
Now if you don't agree with this, you don't have grounds for premise (1),
because if something really is your own accomplishment, humililty is not
appropriate because there is nothing wrong with being proud of what you have
accomplished.  
  
  
  

Premise (1) is controversial. The ancient Greeks would have denied it. But I
think the reason they denied it is that they didn’t have the examples that the
Christian tradition does, highly attractive examples examples of accomplished
lives of great humility.

Here’s the thought behind premise (2). If there is no God, then highly
accomplished individuals have much to brag about. Many of their
accomplishments are primarily _theirs_.

Alex  
  
I have already answered this in my first reply. For the individuals you
describe, humililty is not appropriate.

But what is wrong with being such that you adopt means inappropriate to your
ends is not necessarily the means—it could be the ends.

Unjust laws have no normative force, and stupid ends have no normative force,
either.

I am inclined to think (1) is false if by “end” is meant the end the agent
actually adopts, as opposed to a natural end of the agent. If your ends are
sufficiently irrational, adopting means appropriate to them may be less
rational than adopting means inappropriate to them.

Suppose your end is irrationality. Is it really true that you _should_ adopt
the means to that, such as reasoning badly? Surely not! Instead, you should
reject the end.

The amount of things one person can do with enough time is insane. I wouldn't
believe you if you told me that one person can get two PhDs, teach classes in
metaphysics, run a blog with constantly new and exciting arguments and
positions, is called one of the foremost Christian philosophers currently
alive, and STILL has time to measure bicycle energy output. I am convinced
that Dr. Pruss is some sort of superhuman.

[Adapting](https://www.instructables.com/Playing-NES-Power-Pad-Games-in-
Emulation/) Dance Dance Revolution and other mat controllers to work as NES
Power Pad controllers for emulation.

An action is right (respectively, wrong) if and only if it is
noninstrumentally good (respectively, bad) to do it.

This is compatible with there being cases where it is bad for one to do the
right thing. Thus, refraining from stealing the money that one would need to
sign up for a class on virtue is right and noninstrumentally good, but if the
class is really effective then stealing the money might be instrumentally good
for one, though noninstrumentally ba.

There is a way to connect the right and wrong with the good and bad:

I think (1) is something that everyone should accept. Even consequentialists
can and should accept (1) (though utilitarian consequentialists have too
shallow an axiology to make (1) true). But natural law theorists might add a
further claim to (1): the left-hand-side is true because the right-hand-side
is true.

The title of this post contradicts the title of [another recent
post](http://alexanderpruss.blogspot.com/2022/12/the-right-cannot-be-derived-
from-good.html), but the contents do not.

Of course, that it would be _good_ for the community if some norm of
individual rationality obtained does not prove that the norm obtains.

This could well be true because differences in priors lead to a variety of
lines of investigation, a greater need for effort in convincing others, and
less danger of the community as a whole getting stuck in a local epistemic
optimum. If this hypothesis is true, then we would have an interesting story
about why it would be good for our community if a range of priors were
rationally permissible.

I am grateful to Anna Judd for pointing me to a possible connection between
permissivism and natural law epistemology.

It is epistemically better for the human community if human beings do not all
have the same (ur-) priors.

At the same time, the harmony need not be perfect. Just as there may be times
when the good of the community and the good of the individual conflict in
respect of non-epistemic flourishig, there [may be such
conflict](http://alexanderpruss.blogspot.com/2011/01/epistemic-self-sacrifice-
and-prisoner.html) in epistemic flourishing.

I think it does. I have been trying to defend a natural law account of
rationality on which just as our moral norms are given by what is natural for
the will, our epistemic norms are given by what is natural for our intellect.
And just as our will is the will of a particular kind of deliberative animal,
so too our intellect is the intellect of a particular kind of investigative
animal. And we expect a correlation between what a social animal’s nature
impels it to do and what is good for the social animal’s community. Thus, we
expect a degree of harmony between the norms of epistemic rationality—which on
my view are imposed by the nature of the animal—and the good of the community.

Moreover, note that it is very plausible that what range of variation of
priors is good for the community depends on the species of rational animal we
are talking about. Rational apes like us are likely more epistemically
cooperative than rational sharks would be, and so rational sharks would
benefit less from variation of priors, since for them the good of the
community would be closer to just the sum of the individual goods.

Panteleology holds that teleology is ubiquitous. Every substance aims at  
some end.

The main objection to panteleology is the same as that to panpsychism: the
incredulous stare. I think a part of the puzzlement comes from the thought
that things that are neither biological nor artifactual “just do what they
do”, and there is no such thing as failure. But this seems to me to be a
mistake. Imagine a miracle where a rock fails to fall down, despite being
unsupported and in a gravitational field. It seems very natural to say that in
that case the rock failed to do what rocks should do! So it may be that away
from the biological realm (namely organisms and stuff made by organisms)
failure takes a miracle, but the logical possibility of such a miracle makes
it not implausible to think that there really is a directedness.

That said, I think the quantum realm provides room for saying that things
don’t “just do what they do”. If an electron is in a mixed spin up/down state,
it seems right to think about it as having a directedness at a pure spin-up
state and a directedness at a pure spin-down state, and only one of these
directednesses will succeed.

Couldn't one relate teleology to causal powers and the possible effects they
could accomplish? Final causality exists as long as anything has a causal
power TOWARDS anything, and this directedness of power - without needing to be
active even - is itself a real example of teleology.  
  
For any agent that has the power to cause any effect in any way, it must be
directed towards that end at least insofar as any POWER only makes coherent
sense insofar as it has an EFFECT which it includes within itself and thereby
points.

Panteleology is also entailed by a panpsychism that follows Leibniz in
including the ubiquity of “appetitions” and not just perceptions. And it seems
to me that if we think through the kinds of reasons people have for
panpsychism, these reasons extend to appetitions—just as a discontinuity in
perception is mysterious, a discontinuity in action-driving is mysterious.

Panteleology seems to be exactly what we would expect in a world created by
God. Everything _should_ glorify God.

I think many theists would admit that God has created every substance for a
purpose, but that'd be an extrinsic teleology, and would be less
controversial. You've said elsewhere that you think that every substance has a
teleology. Are you meaning to argue that every substance has intrinsic
teleology? That'd be more controversial.  
  
  
  

One might suppose that among the small number of fundamental abstract moral
principles one will have some principles about respect for bodily integrity. I
doubt it, though. Respect for bodily integrity is an immensely complex area of
ethics, and it is very unlikely that it can be encapsulated in a small number
of abstract moral principles. Respect for bodily integrity differs in very
complex ways depending on the body part and the nature of the relationship
between the agent and the patient.

I think Utilitarians might try to say something like, "If governments took
kidneys, this would have a different psychological effect on society than if
they took 20% of our income. Because of this accidental feature of our
psychologies, it actually would be more harmful for them to take our kidneys,
and look here I can show how this extra harm is unjustifiable via my abstract
moral principles."

I should note that the above argument fails against divine command theories.
Divine command theorists will say that about rightness and wrongness are
identified with descriptive facts about what God commands, and these facts can
be very rich and hence include enough data to determine (2). For the argument
against (1) to work, the “descriptive facts” have to be more like the facts of
natural science than like facts about divine commands.

All facts about rightness and wrongness can be derived from descriptive facts,
facts about non-rightness value, and a small number of fundamental abstract
moral principles.

The restriction to non-rightness good and bad is to avoid triviality. By
“rightness value” here, I mean only the value that an action or character has
in virtue of its being right or wrong to the extent that it is.

If loss of a kidney were to impact one’s autonomy significantly more than loss
of 20% of your lifetime income, then again there would be some hope for a
derivation of (2). But whether loss of a kidney is more of an autonomy impact
than loss of 20% of income will differ from person to person.

Consider the following thesis that both Kantians, utilitarians and New Natural
Law thinkers will agree on:

I don’t have a good definition of “abstract moral principle”, but I want them
to be highly general principles about moral agency such as “Choose the greater
over the lesser good”, “Do not will the evil”, etc.

It is not wrong for the government to forcibly and non-punitively take 20% of
your lifetime income, but it is wrong for the government to forcibly and non-
punitively take one of your kidneys.

I don’t think we can derive (2) in accordance with the strictures in (1). If a
kidney were a lot more valuable than 20% of lifetime income, we would have
some hope of deriving (2) from descriptive facts, non-rightness value facts,
and abstract moral principles, for we might have some abstract moral principle
prohibiting the government from forcibly and non-punitively taking something
above some value. But a kidney is not a lot more valuable than 20% of lifetime
income. Indeed, if it would cost you 20% of your lifetime income to prevent
the destruction of one of your kidneys, it need not be unreasonable for you to
refuse to pay. Indeed, it seems that either 20% of lifetime income is
incommensurable with a kidney, or in some cases it is more valuable than a
kidney.

Since then, I wrote a [web-based tool](https://arpruss.github.io//webpanim)
for generating a WebP animation of a timer and text synchronized to a set of
times. The timer can be in seconds or tenths of a second, and you can specify
a list of text messages and the times to display them (or to hide them). You
can then overlay it on a video in Premiere Rush or Pro. There is alpha
support, so you can have a transparent or translucent background if you like,
and a bunch of fonts to choose from (including the geeky-looking Hershey font
that I used in my Python script.)

The code uses [webpxmux.js](https://github.com/sumimakito/webpxmux.js), though
it was a little bit tricky because in-browser Javascript may not have enough
memory to store all the uncompressed images that webpxmux.js needs to generate
an animation. So instead I encode each frame to WebP using webpxmux.js,
extract the compressed ALPH and VP8 chunks from the WebP file, and store only
the compressed chunks, writing them all at the end. (It would be even better
from the memory point of view to write the chunks one by one rather than
storing them in memory, but a WebP file has a filesize in its header, and
that’s not known until all the compressed chunks have been generated. One
could get around this limitation by generating the video twice, but that would
be twice as slow.)

When making my [Guinness application record
video](http://alexanderpruss.blogspot.com/2022/12/a-new-but-uncertified-world-
record.html), I wanted to include a time display in the video and Guinness
also required a running count display. I ended up writing a [Python
script](https://gist.github.com/arpruss/3a705c859d4ea4a482c4e47e96a08f79)
using OpenCV2 to generate a video of the time and lap count, and overlaid it
with the main video in Adobe Premiere Rush.

But if there are cases like (1), there will surely also be cases where the
moral considerations in favor of _ϕ_ ing do not rise to the level of a
requirement, but are sufficient to override the _N_ -prohibition. In those
cases, presumably:

But now consider this. What happens if the moral considerations are at an even
lower level, a level insufficient to override the _N_ -prohibition? (E.g.,
what if to save someone’s finger you would need to sacrifice your arm?) Then,
it seems:

However, there is another story possible. Perhaps in the case where the moral
considerations are at too low a level to override the _N_ -prohibition, we can
still have _moral_ permission to _ϕ_ , but that permission no longer overrides
the _N_ -prohibition. On this story, there are two kinds of cases, in both of
which we have moral permission, but in one case the moral permission comes
along with sufficiently strong moral considerations to override the _N_
-prohibition, while in the other it does not. On this story, moral requirement
always overrides non-moral reasons; but whether moral considerations override
non-moral considerations depends on the relative strengths of the two sets of
considerations.

I don’t want to say that all norms are moral norms. But it may well be that
all norms governing the functioning of the will are moral norms.

Cases of supererogation look like that: you are morally permitted to do
something contrary to prudential norms, but not required to do so.

But this would be quite interesting. It would imply that in the absence of
sufficient moral considerations in favor of _ϕ_ ing, an _N_ -prohibition would
automatically generate a _moral_ prohibition. But this means that the real
normative upshot in all three cases is given by morality, and the _N_ -norms
aren’t actually doing any independent normative work. This suggests strongly
that on such a picture, we should take the _N_ -norms to be simply a species
of moral norms.

People often talk of moral norms as overriding. The paradigm kind of case
seems to be like this:

Still, consider this. The judgment whether moral considerations override the
non-moral ones seems to be an eminently _moral_ judgment. It is the person
with _moral_ virtue who is best suited to figuring out whether such overriding
happens. But what happens if morality says that the moral considerations do
not override the _N_ -prohibition? Is that not a case of morality giving its
endorsement to the _N_ -prohibition, so that the _N_ -prohibition would rise
to the level of a moral prohibition as well? But if so, then that pushes us
back to the previous story where it is reasonable to take _N_ -considerations
to be subsumed into moral considerations.

where “ _N_ ” is some norm like that of prudence or etiquette. In this case,
the moral requirement of _ϕ_ ing overrides the _N_ -prohibition on _ϕ_ ing.
Thus, you might be rude to make a point of justice or sacrifice your life for
the sake of justice.

So far so good. Moral norms can override non-moral norms in two ways: by
creating a moral requirement contrary to the non-moral norms or by creating a
moral permission contrary to the non-moral norms.

They confer before the game and promise to one another to raise the right
hand. They go into their separate rooms. And what happens next?

Suppose Alice and Bob are perfect utilitarians or perfect amoral egoists in
any combination. They are about to play a game where they raise a left hand or
a right hand in a separate booth, and if they both raise the same hand, they
both get something good. Otherwise, nobody gets that good. Nobody sees what
they’re doing in the game: the game is fully automated. And they both have
full shared knowledge of the above.

It is true that if Alice expects Bob to expect her to keep her promise, then
Alice will expect Bob to raise his right hand, and hence she should raise her
right hand. But since she’s known to be an amoral egoist, there is no reason
for Bob to expect Alice to keep her promise. And the same vice versa.

In ordinary life, this problem doesn’t arise as much, because as long as at
least one person is more typical, and hence takes promises to have reason-
giving force, or if public opinion is around to enforce promise-keeping, then
the issue doesn’t come up. But I think there is a lesson here and in the
[previous post](http://alexanderpruss.blogspot.com/2022/12/utilitarianism-and-
communication.html): for many ordinary practice, the utilitarian is free-
riding on the non-utilitarians.

The promise is simply irrelevant here. It is true that in normal
circumstances, it makes sense for egoists to keep promises in order to fool
people into thinking that they have morality. But I’ve assumed full shared
knowledge of each other’s tendencies here, and so no such considerations apply
here.

This is a fun puzzle. It seems like David Lewis’ convention work, or Thomas
Schelling’s coordination and focal points stuff must be relevant.

This means that in cases like this, with full transparency of behavioral
tendencies, utilitarians and amoral egoists will do well to brainwash or
hypnotize themselves into promise-keeping.

What if they are utilitarians? It makes no difference. Since in this case both
always get the same outcome, there is no difference between utilitarians and
amoral egoists.

Take first the case where they are both perfect amoral egoists. Amoral egoists
don’t care about promises. So the fact that an amoral egoist promised to raise
the right hand is no evidence at all that they will raise the right hand,
unless there is something in it for them. But is there anything in it for
them? Well, if Bob raises his right hand, then there is something in it for
Alice to raise her right hand. But note that this conditional is true
_regardless_ of whether they’ve made any promises to each other, and it is
equally true that if Bob raises his left hand, then there is something in it
for Alice to raise her left hand.

Here's a further line of thought. Suppose that Alice and Bob are not fully
utilitarian, but they incorporate into their ethics (which they follow
perfectly) an anti-conventionalist element (and that's also a part of their
shared knowledge). Thus, they think that the fact that one has promised p is a
fairly weak reason, of degree epsilon, against performing p. Then, if
epsilon>0, then it seems that by their lights Alice and Bob should lift their
left hands rather than their right, if they promised to lift their right,
since their behavioral bias is anti-promissory, and each knows the other to
have such a bias. Very well. Now take the limit as epsilon (the strength of
the reason they think favors breaking promises) to zero. For every epsilon>0,
they should lift their left hands rather than their right. It seems to be a
reasonable continuity conclusion that for epsilon=0, they should either be
neutral between lifting their left hands rather than their right or should
still prefer the left, but definitely should not prefer their right. And yet
epsilon=0 is just the full utilitarian case.

My feeling is it's not hard to solve -- as long as you don't place artificial
restrictions on what counts as a reason, in the way the utilitarian or egoist
does.

But I’ve been trying really hard to figure out how is it that such a
conventional behavior would indicate to Bob that the lion is on the left path.

Alice and Bob are both perfect Bayesian epistemic agents and subjectively
perfect utilitarians (i.e., they always do what by their lights maximizes
expected utility). Bob is going to Megara. He comes to a crossroads, from
which two different paths lead to Megara. On exactly one of these paths there
is a man-eating lion and on the other there is nothing special. Alice knows
which path has the lion. The above is all shared knowledge for Alice and Bob.

Suppose the lion is on the left path. What should Alice do? Well, if she can,
she should bring it about that Bob takes the right path, because doing so
would clearly maximize utility. How can she do that? An obvious suggestion:
Engage in a conventional behavior indicating a where the lion is, such as
pointing left and roaring, or saying “Hail well-met traveler, lest you be
eaten, I advise you to avoid the leftward leonine path.”

If the above argument is correct—and I am far from confident of that, since it
makes my head spin—then we have an argument that in order for communication to
be possible, at least one of the agents must be convention-bound. One way to
be convention-bound is to think, in a way utilitarians don’t, that convention
provides non-consequentialist reasons. Another way is to be an akratic
utilitarian, addicted to following convention. Now, the possibility of
communication is essential for the utility of the kinds of social animals that
we are. Thus we have an argument that at least some subjective utilitarians
will have to become convention-bound, either by getting themselves to believe
that convention has normative force or by being akratic.

This is not a refutation of utilitarianism. Utilitarians, following Parfit,
are willing to admit that there could be utility maximization reasons to cease
to be utilitarian. But it is, nonetheless, really interesting if something as
fundamental as communication provides such a reason.

If Alice were a typical human being, she would have a habit of using
established social conventions to tell the truth about things, except perhaps
in exceptional cases (such as the murderer at the door), and so her use of the
conventional lion-indicating behavior would correlate with the presence of
lions, and would provide Bob with evidence of the presence of lions. But Alice
is not a typical human being. She is a subjectively perfect utilitarian.
Social convention has no normative force for Alice (or Bob, for that matter).
Only utility does.

Similarly, if Bob were a typical human being, he would have a habit of forming
his beliefs on the basis of testimony interpreted via established social
conventions absent reason to think one is being misinformed, and so Alice’s
engaging in conventional left-path lion-indicating behavior would lead Bob to
think there is a lion on the left, and hence to go on the right. And while it
woudl still be true that social convention has no normative force for Alice,
Alice would think have reason to think that Bob follows convention, and for
the sake of maximizing utility would suit her behavior to his. But Bob is a
perfect Bayesian. He doesn’t form beliefs out of habit. He updates on
evidence. And given that Alice is not a typical human being, but a
subjectively perfect utilitarian, it is unclear to me why her engaging in the
conventional left-path lion-indicating behavior is more evidence for the lion
being on the left than for the lion being on the right. For Bob knows that
convention carries no normative force for Alice.

I put this as an issue about communication. But maybe it’s really an issue
about communication but coordination. Maybe the literature on repeated games
might help in some way.

Here is a brief way to put it. For Alice and Bob, convention carries no weight
except as a predictor of the behavior of convention-bound people, i.e., people
who are not subjectively perfect utilitarians. It is shared knowledge between
Alice and Bob that neither is convention-bound. So convention is irrelevant to
the problem at hand, the problem of getting Bob to avoid the lion. But there
is no solution to the problem absent convention or some other tool unavailable
to the utilitarian (a natural law theorist might claim that mimicry and
pointing are _natural_ indicators).

I do have a piece of anecdotal data, though. I’ve been doing some endurance-
ish sports. Nothing nearly like a marathon, but things like swimming 2-3 km,
or climbing for an hour, typically (but [not
always](http://alexanderpruss.blogspot.com/2022/12/a-new-but-uncertified-
world-record.html)) competing against myself.

I think that's correct. The more one concentrates on good form, the more
detrimental negative transfer becomes. This also applies in martial arts like
Tae Kwon Do, on the self-defense side, whereby training to 'pull' kicks and
punches (i.e. to safeguard your training partner), negatively transfers to
real-life situations.

This is akin to the 'positive transfer' of skills, as discussed with the sport
science. So-called 'negative transfer', its opposite, is to be avoided at all
costs. For example, if I try to play squash to improve my racket skills, I
will ruin my tennis game. The racket skills are subtly different, due to the
'wrist flick' in the squash technique.

I have no idea if anything like this transfer works for other people.

And I _have_ noticed some transfer of skills and maybe even of the virtue of
patience both between the various sports and between the sports and other
repetitive activities, such as grading. There is a distinctive feeling I have
when I am half-way through something, and where I am fairly confident I can
finish it, and a kind of relaxation past the half-way point where I become
more patient, and time seems to flow “better”. For instance, I can compare how
tired I feel half-way through a long set of climbs and how tired I feel half-
way through a 2 km swim, and the comparison can give me some strength. Similar
positive thinking can happen while grading, things like “I can do it” or
“There isn’t all that much left.” Though there are also differences between
the sports and the grading, because in grading the quality of the work matters
a lot more, and since I am not racing against myself so there is no point of a
burst of speed at the end if I find myself with an excess of energy. Pacing is
also much less important for grading.

I've wondered about such incompossible goods myself. Weightlifting can reduce
agility, for example, and while being tall is great for basketball, it is not
good for weightlifting. Various niches seem to exist. It does raise the
question of what a perfect human being consists of, however. Are some of these
"perfections" merely accidental in the sense that they exploit what are
normatively speaking (in relation to human nature) flaws? We know some sports
are actually bad for the human body, or neglect the overall health of the
body. Certain perfections seem to be perfections only in an analogical sense,
perhaps something along the lines of being a "good thief" or an "effective
deceiver".  
  
Where intelligence is concerned, I am tempted to argue against incompossible
goods either because intelligence isn't like that or for similar natural law
reasons, but even more strongly, especially on account of the centrality of
intelligence to humanity.  
  
Consider your favorite déformation professionnelle, which, I submit, is more
of a result of imprudence, habit, ignorance, lack of practice using other
methods, and even effeminacy and arrogance. Someone with a rigorous
philosophical education is less likely to try to pigeonhole reality into the
reductive and simplified straitjacket of our physical models in the manner of
at least some physicists who generally lack serious exposure to philosophy and
may even hold it in contempt out of ignorance. A physicist may also be tempted
to pigeonhole simply because of pride; if he isn't any good at metaphysics,
then his thoughts on a metaphysical subject matter aren't likely to be very
valuable or interesting, and that stings the prideful man accustomed to
feeling like a hotshot. There is also the threat of seeing one's own field put
in its methodological place, so to speak, deflating any pretensions to the
kind of ultimacy that metaphysics lays claim to. A competent physics may also
derive greater pleasure from exercising his specialized competence and choose
his methods simply on the basis of what feels good and now what is called for
by a problem. So here the question resurfaces: is there an incompossibility
between being a good physicist and a good metaphysician? I suspect there isn't
intrinsically, even if that is often the case which I suspect is rather a
result of how one's time is spent. But even if it is the case, because general
knowledge is superior and more worthy of human attention than specialized
knowledge, we could argue that competence in physics that occurs at the
expense of philosophical depth is, in fact, a kind of failure to attain human
excellence by failing to devote proportional attention and effort to the kinds
of knowledge that are most essential, and in doing so, risking intellectual
deformation in important and even necessary matters.

There are empirical indications that various skills and maybe even virtues are
pretty domain specific. It seems that being good at reasoning about one thing
need not make one good at reasoning about another, even if the reasoning is
formally equivalent.

BTW, it may depend on how much one keeps to proper form in the different
racquet sports. Someone like me who plays lots of different racquet sports
(regularly: badminton; semi-regularly: tennis, racquetball, table tennis,
pickleball; used to do occasional squash until our university's one court
closed as nobody but my son and I played; used to do crossminton during
Covid), maybe at an upper beginner level, perhaps does not have enough good
form in any one of the sports for it to matter. I hadn't played much tennis
this fall, but I played a fair amount of badminton, and seemed to find my
tennis improved when I got back to it, maybe due to transfer of thinking about
things like "how do I hit the shuttle away from where my opponents are", or
maybe just due to general fitness improvement.

**Final remark:** The argument applies to any exclusive and exhaustive
division of reasons into “simple” (i.e., non-combination) types.

One might think that reasons for action are exhaustively and exclusively
divided into the moral and the prudential. Here is a problem with this.
Suppose that you have a spinner divided into red and green areas. If you spin
it and it lands into red, something nice happens to you; if it lands on green,
something nice happens to a deserving stranger. You clearly have reason to
spin the spinner. But, assuming the division of reasons, your reason for
spinning it is neither moral nor prudential.

Now, one might have technical worries about saturated nonmeasurable sets
figuring in decisions. I do. (E.g., see the Axiom of Choice chapter in my
infinity book.) But now instead of supposing saturated nonmeasurable sets,
suppose a case where an agent subjectively has literally no idea whether some
event _E_ will happen—has no probability assignment for _E_ whatsoever, not
even a ranged one (except for the full range from 0 to 1). The spinner landing
on a set believed to be saturated nonmeasurable might be an example of such a
case, but the case could be more humdrum—it’s just a case of extreme
agnosticism. And now suppose that the agent is told that if they so opt, then
they will get something nice on _E_ and a deserving stranger will get
something nice otherwise.

**Objection:** The chance _p_ of the spinner landing on red is a prudential
reason and the chance 1 − _p_ of its landing on green is a moral reason. So
you have _two_ reasons, one moral and one prudential.

So what should we say? One possibility is to say that there are _only_ reasons
of one type, say the moral. I find that attractive. Then benefits to yourself
also give you _moral_ reason to act, and so you simply have a moral reason to
spin the spinner. Another possibility is to say that in addition to moral and
prudential reasons there is some third class of “mixed” or “combination”
reasons.

**Response:** That may be right in the simple case. But now imagine that the
“red” set is a saturated nonmeasurable subset of the spinner edge, and the
“green” set is also such. A saturated nonmeasurable subset has no reasonable
probability assignment, not even a non-trivial range of probabilities like
from 1/3 to 1/2 (at best we can assign it the full range from 0 to 1). Now the
reason-giving strength of a chancy outcome is proportionate to the
probability. But in the saturated nonmeasurable case, there is no probability,
and hence no meaningful strength for the red-based reason or for the green-
based reason. But there is a meaningful strength for the red-or-green moral-
cum-prudential reason. The red-or-green-based reason hence does not reduce to
two separate reasons, one moral and one prudential.

It is correct to say that the Greeks discovered an incommensurability fact.
But it is, I think, worth noting that this incommensurability fact is not
really geometric fact: it is a geometric-cum-arithmetical fact. Here is why.
The claim that two line segments are commensurable says that there are
positive integers _m_ and _n_ such that _m_ copies of the first segment have
the same length as _n_ copies of the second. This claim is essentially
arithmetical in that it quantifies over positive integers.

I think it is sometimes said that it is anachronistic to attribute to the
ancient Greeks the discovery that the square root of two is irrational,
because what they discovered was a properly _geometrical_ fact, that the side
and diagonal of a square are incommensurable, rather than a fact about real
numbers.

I was thinking of the decidability of Th(N), where N is our "intended" model
of the naturals, not of the decidability of any particular recursive
axiomatization. (Th(N) is not recursively axiomatizable, of course.)

Don't you rather mean _"Th(something)"_ to be our "intended" model of
"something", such that "something" could be the naturals N, such that Th(N) is
our "intended" model of naturals N?!?  
  
Why isn't Th(N) recursively axiomattizaböe though?!?

And because pure (Tarskian) geometry is decidable, while the theory of the
positive integers is not decidable, the positive integers are not definable in
terms of pure geometry, so we cannot eliminate the quantification over
positive integers. In fact, it is known that the rational numbers are not
definable in terms of pure geometry either, so neither the incommensurability
formulation nor theory irrationality formulation is a purely geometric claim.

The top of the wall is 15.13 meters vertically from the ground (as measured by
a geology grad student), at 3.5 degree slab.

I am guessing that if one is doing one of the longer records, say an 8 hour
one, they aren't going to want to watch all of the video, except maybe at high
speed, and so looking at notable moments might make sense.  
  
I expect they also choose some small fraction of the records to feature on
social media, and I could see them using the notable moments list to extract
video for that.  
  
I pity those who don't have programming skills, though. Inserting a running
count of laps into a video by manually inserting a title into the video at
each point with a video editor program would be as much an endurance sport as
actually doing the laps. It took me a while to type up the times of all the
climb tops, but after that I just had the computer automatically generate a
video track for the inset window with time and counts.

As we say in Australia, you are 'a gun'! Have you ever thought about writing
philosophically about climbing? Perhaps even just in the vaguely existential
vein that Murakami mines in his 'What I Talk About When I Talk About Running'.

I trained for about three months, not very heavily. In training did two
unofficial full-length practice runs, and in each I beat the previous record:
in the first one I got 947.1 meters and in the second I got 1004.5, so I was
pretty confident I could beat the 928.5 meters on the official attempt (though
I was still pretty nervous). I also trained by doing a small number of
approximately 1/2 or 1/3 sized practices (maybe three or so), and more regular
shorter runs (1-10 climbs) at fast pace.

On the ground there was a sheet of paper with the start and end times of each
break printed in large letters (calculated by [this
script](https://gist.github.com/arpruss/10e364904dfcc19b043a594232e0acde)), as
well as the mid-point time for each set of 10 to keep me better on pace.

I climbed in sets of 10. The planned pace was 8:18 per set and a 44-45 second
rest between sets (clock runs during rests,), averaging at 49.8 seconds per
climb including descent. I was always ahead of pace, and I occasionally took a
mini break at the mid-point time if I was too far ahead.

About half-way through, I ducked into the storage area inside the rock and
changed to a dry shirt.

And now for something not very philosophical. Today, in front of two witnesses
and two timekeepers and with the help of Levi Durham doing an amazing feat of
belaying me for an hour, I beat the [Guinness World
Record](https://www.guinnessworldrecords.com/world-records/438677-greatest-
vertical-distance-climbed-on-an-artificial-climbing-wall-in-1-hour-indi) in
greatest vertical distance climbed in one hour on an indoor climbing wall. The
previous record was 928.5m and I did 1013.7m (with about half a minute to
spare). On Baylor's climbing wall, this involved 67 climbs divided into sets
of 10 (the last was 7), with about a minute of rest between sets (the clock
kept on running during the rest).

Most of my practice was with an auto-belay, and at a shorter distance per
climb (and hence greater number of climbs needed) since the auto-belay makes
it impossible to get to the top of the wall. The auto-belay is also spring
loaded so it effectively decreases body weight (by 7 lbs at the bottom
according to my measurement). Then a couple of weeks ago the auto-belay was
closed by management due to a maintenance issue, and I had a break in training
until the Wednesday before the official attempt when I trained with a manual
belay.

I was actually really stressed yesterday. I had already beaten the record
unofficially in practice, but with lots of people coming to watch (local
climbers, grad students and colleagues), I didn't want to disappoint.
Contributing to the stress was that the conditions changed from my practice
runs (the auto-belay was down from maintenance and I had to use a manual
belay; this involved lengthening the route slightly, increasing the length of
each set and decreasing the total count, as well as an effective increase in
my weight since the auto-belay subtracts a few pounds due to its spring-
loading), and that I hadn't been able to train on the particular route for
several weeks prior to Wednesday due to the auto-belay being down for
maintenance while on Wednesday's approximately half-distance-at-half-time
practice I was more tired at the end than I should have been due to poor
pacing.

Since Guinness requires video proof in addition to human witnesses, in the
interests of redundancy, I had three cameras pointed at the attempt. The best
footage (above) is from a Sony A7R2 with a zoom lens at 16mm, producing 1080P
at 59.94 fps. Video was processed with Adobe Premiere Rush. The processing
consisted of trimming the start and end, and adding a timing video track I
generated with a [Python OpenCV2
script](https://gist.github.com/arpruss/3a705c859d4ea4a482c4e47e96a08f79),
synchronized with single-frame precision at the 1:00:00 point with the footage
of Giant Stopwatch (barely visible under the table towards the end of the
video; early in the video, glare hides it). For the unofficial version I link
above, I accelerated the middle climbs 10X in Premiere Rush.

I spent over a whole day documenting things for Guinness. They want photos,
attempt video (with running count--there was more python scripting to generate
that, plus looking at the video to figure out the times of all the ascents),
statements from two witnesses and two timekeepers for the attempt, a
measurement statement from a "surveyor or other qualified person" (a geology
grad student in our case), a measurement video, two witnesses for the
measurement, an index to the photos, documentation of timekeeper and witness
qualifications, and a notable moments list for the video.  
  
On the bright side, all this means that their records are probably pretty
reliable.  
  
It's now submitted, and they should respond in three months. Fingers crossed.
For GBP 500, they can do a rush review in five days, but why bother?

In the morning I stress-baked pumpkin muffins for myself and the volunteers. I
had the muffins, water and loose chalk on a table for use during breaks.

The route was a standard 5.7 grade for most of my training (including when I
unofficially beat the records), with Rock management kindly agreeing to keep
the route up for several months for me. For the final attempt, we added holds
to make the finish at the top of the wall, and changed three other holds to
easier ones. (Guinness has no route grade requirements.)

You should make a living out of that, Alex.  
Would give you a lot less stress.

A Kindle Fire running a pre-release version of my [Giant
Stopwatch](https://play.google.com/store/apps/details?id=omegacentauri.mobi.simplestopwatch&hl=en_US&gl=US)
app provided unofficial timing for audience to see and for my pacing. I had to
modify the app to have a periodic beep to meet Guinness's requirements of an
audible stop signal.

I just noticed that you mentioned capturing video footage for Guinness, so I
suppose that answers my prior question. I apologize for the oversight.
Congrats again!

Wow, I had no idea that their confirmation process was so extensive! I wonder
what purpose the "notable moments" list is meant to be playing, especially
since they already have the video itself.

I wore moderately worn (one small hole) and comfortable 5.10 Anasazi shoes, a
Camp USA Energy harness, shorts and a T-shirt. (I have not received any
sponsorship.) My belayer used a tube-style device and wore belay gloves.

First, we have “officially” non-lethal weapons: tasers, gas, etc. Some of
these might violate current international law, but it seems that a pacifist
country could modify its commitment to some accords.

Second, “lethal” weapons can be used less than lethally. For instance, with
modern medicine, abdominal gunshot wounds are only 10% fatal, yet they are no
doubt very effective at stopping an attacker. While it may seem weird to
imagine a pacifist shooting someone in the stomach, when the chance of
survival is 90%, it does not seem unreasonable to say that the pacifist could
be aiming to stop the attacker non-lethally. After all, tasers sometimes kill,
too. They do so less than 0.25% of the time, but that’s a difference of degree
rather than of principle.

Whether such a limited way of waging war could be successful probably depends
on the case. If one combined the non-lethal (or not intentionally lethal)
means with technological and numerical superiority, it wouldn’t be surprising
to me if one could win.

Third, we might subdivide moderate pacifists based on whether they prohibit
all violence that foreseeably leads to death or just violence that
intentionally leads to death. If it is only intentionally lethal violence that
is forbidden, then quite a bit of modern warfare can stand. If the enemy is
attacking with tanks or planes, one can intentionally destroy the tank or
plane as a weapon, while only foreseeing, without intending, the death of the
crew. (I don’t know how far one can take this line without sophistry. Can one
drop a bomb on an infantry unit intending to smash up their rifles without
intending to kill the soldiers?) Similarly, one can bomb enemy weapons
factories.

I’ve been wondering whether it is possible for a country to count as pacifist
and yet wage a defensive war. I think the answer is positive, as long as one
has a moderate pacifism that is opposed to lethal violence but not to all
violence. I think that a prohibition of all violence is untenable. It seems
obvious that if you see someone about to shoot an innocent person, and you can
give the shooter a shove to make them miss, you presumptively should.

Note that a version of this argument goes through even if the moderate
pacifist backs up and says that tasers are too lethal. For suppose instead of
tasers we have drones that destroy the dominant hand of an enemy soldier while
guaranteeing survival (with science fictional medical technology). It’s
clearly right to release such a drone on a soldier who is about to kill ten
innocents. But now compare:

If you can tase one person to stop the murder of ten, then (1) should be
permissible if it’s the only option. But tasers occasionally kill people. We
don’t know how often. Apparently it’s [less than 1 in
400](https://www.usatoday.com/in-depth/news/investigations/2021/04/23/police-
use-tasers-ends-hundreds-deaths-like-daunte-wright/7221153002/) uses. Suppose
it’s 1 in 4000. Then option (1) results in 250 enemy deaths.

I think (4) is still morally preferable to causing the kind of disruption to
the lives of a million people that plan (3) would involve.

One should never murder anyone. Killing, on the other hand, can sometimes be
justified.  
  
One question about killing the leader of an invading country is whether the
leader counts as a civilian, since the killing of civilians is forbidden by
the Geneva Convention. There is also some worry about the Geneva Convention's
killing by "perfidy", which is taken to rule out at least some assassinations.
So, as a matter of positive international law, it seems a difficult question.  
  
Were there no international law on the matter, I wouldn't see a significant
difference between killing a political leader and killing a general, if both
are giving orders to fight. Morally speaking, but not necessarily in
international law, both seem to me to be equally combatants.  
  
Besides the moral and legal questions, there is also a prudential question. In
my post I assumed that killing the general would stop the invasion. I got to
assume that because I was making up the case. Whether in actual fact killing a
leader would stop an invasion is less clear. Indeed such a thing might be seen
as such a serious attack on the country that the retaliation might be really
horrific.

Imagine a moderate pacifist who rejects lethal self-defense, but allows non-
lethal self-defense when appropriate, say by use of tasers.

Now, imagine that one person is attacking you and nine other innocents, with
the intent of killing the ten of you, and you can stop them with a taser.
Surely you should, and surely the moderate pacifist will say that this is an
appropriate use case for the taser.

So maybe our choice is between tasing a million, thereby non-intentionally
killing 250 soldiers, and intentionally killing one general. It seems to me
that (2) is morally preferable, even though our moderate pacifist has to allow
(1) and forbid (2).

Alex  
  
Of course one should never murder anyone, but the question is: would killing
Putin be murder?

Very well. Now consider this on a national level. Suppose there are a million
enemy soldiers ordered to commit genocide against ten million, and you have
two ways to stop them:

These may seem to be consequentialist arguments. I don't think so. I don't
have the same intuitions if we replace the general by the general's innocent
child in (2) and (4), even if killing the child were to stop the war (e.g., by
making the general afraid that their other children would be murdered).

Alex  
  
Doesn't you post here entail that we should murder Putin?

Maybe the answer to both questions is that I could, but only metaphysically
and not causally. In other words, it could be that the laws of nature, or of
human nature, make it impossible for me to exercise one of the powers without
the other, just as I cannot wiggle my ring finger without wiggling my middle
finger as well. On this view, if there is a God, he could cause me to acquire
promissory-type obligations without my promising, and he could let me engage
in the natural act of promising while blocking the exercise of normative power
and leaving me normatively unbound. This doesn’t seem particularly
problematic.

I think the difficulty with a causal model is the fact that in paradigm cases
of normative power, there is a natural power that _is_ being exercised, and we
have the intuition that the exercise of the natural power is necessary and
sufficient for the normative effect. But on a causal model, why couldn’t I
cause a promissory-type obligation without promising, simply causing the
relevant property of being obligated to come to be instantiated in me? And why
couldn’t I engage in the speech act while yet remaining normatively unbound,
because my normative power wasn’t exercised in parallel with the natural
power?

Here is a picture on which this is correct. We exercise a normative power by
exercising a natural power in such a context that the successful exercise of
the natural power is partly constitutive of a normative fact. For instance, we
utter a promise, thereby exercising a natural power to engage in a certain
kind of speech act, and our exercise of that speech act is partly constitutive
of, rather than causal of, the state of affairs of our being obligated to
carry out the promised action.

A normative power is a power to change a normative condition.
[Raz](https://core.ac.uk/download/pdf/230182259.pdf) says the change is not
produced “causally” but “normatively”.

But why not allow for a causal model? Why not suppose that a normative power
is a causal power to make an irreducible normative property come to be
instantiated in someone? Thus, my power to promise is the power to cause
myself to be obligated to do what I have promised.

Perhaps the real problem for a lot of people with a causal view of normative
powers is that it tends to lead to a violation of supervenience. For if it is
metaphysically possble to have the exercise of the normative power without the
exercise of the natural power, or vice versa, then it seems we don’t have
supervenience of the normative on the non-normative. But supervenience does
not seem to me to be inescapable.

There are two versions of the above model. On one version, there is an
underlying fundamental conditional normative fact _C_ , such as that if I have
promised something then I should do it, and my exercise of normative power
supplies the antecedent _A_ of that conditional, and then the normative
consequent of _C_ comes to be grounded in _C_ and _A_. On another version,
there there are some natural acts that are directly constitutive of a
normative state of affairs, not merely by supplying the antecedent of a
conditional normative fact. I think the first version of the model is the more
plausible in paradigmatic cases.

According to the guise of the good thesis, one always acts for the sake of an
apparent good. There is a weaker and a stronger version of this:

**Case 1:** There is some device which does something useful when you trigger
it. It is triggered by electrical activity. You strap it on to your arm, and
raise your arm, so that the electrical activity in your muscles triggers the
device. Your raising your arm has the arm going up as an end, but that end is
not perceived as good, but merely neutral. All you care about is the
electrical activity in your muscles.

**Weak** : Whenever you act, you act for an end that you perceive is good.

This isn’t quite enough for a defense of the strong thesis. For even if the
success is good, it does not follow that you perceive the success as good. You
might subscribe to an axiological theory on which success is not good in
general, but only success at something good.

**Case 2:** Back when they were dating in high school, Bob promised to try his
best to bake a nine-layer chocolate cake for Alice’s 40th birthday. Since
then, Bob and Alice have had a falling out, and hate each other’s guts.
Moreover, Alice and all her guests hate chocolate. But Alice doesn’t release
Bob from his promise. Bob tries his best to bake the cake in order to fulfill
his promise, and happens to succeed. In trying to bake the cake, Bob acted for
the end of producing a cake. But producing the cake was worthless, since no
one would eat it. The only value was in the trying, since that was the
fulfillment of his promise.

**Strong** : Whenever you act, you act for an end, and every end you act for
you perceive as good.

In both cases, it is still true that the agent acts for a good end—the useful
triggering of the device and the production of the cake. But in both cases it
seems they are also acting for a worthless end. Thus the cases seem to fit
with the weak but not the strong guise of the good thesis.

But perhaps we can say this. We have a normative power to endow some neutral
things with value by making them our ends. And in fact the only way to act for
an end that does not have any independent value is by exercising that
normative power. And exercising that normative power involves your seeing the
thing you’re endowing with value as valuable. And maybe the only way to raise
your arm or for Bob to bake the cake in the examples is by exercising the
normative power, and doing so involves seeing the end as good. Maybe. This has
some phenomenological plausibility and it would be nice if it were true,
because the strong guise of the good thesis is pretty plausible to me.

For the strong version to have any plausibility, “good” must include cases of
purely instrumental goodness.

I was going to leave it at this. But then I thought of a way to save the
strong guise of the good thesis. Success is valuable as such. When I try to do
something, succeeding at it has value. So the arm going up or the cake being
produced _are_ valuable as necessary parts of the success of one’s action. So
perhaps every end of your action _is_ trivially good, because it is good for
your action to succeed, and the end is a (constitutive, not causal) means to
success.

I think there is still reason to be sceptical of the strong version.

Is this possible? I think so. We just need to
[distinguish](http://alexanderpruss.blogspot.com/2019/08/two-ways-to-pursue-y-
for-sake-of-z.html) between pursuing victory for the sake of something else
that follows from victory and pursuing victory for the sake of something that
might follow from the pursuit of victory.

If you lose, but you tried to win, she pays you double what you lost.

Clearly the prudent thing to do is to try to win. For if you don’t try to win,
then you are guaranteed not to get any money. But if you do try, you won’t
lose anything, and you might gain.

Here is the oddity: you are trying to win in order to get paid, but you only
get paid if you don’t win. Thus, you are trying to achieve something, the
achievement of which would undercut the end you are pursuing.

Suppose Alice can read your mind, and you are playing poker against a set of
people not including Alice. You don’t care about winning, just about money.
Alice has a deal for you that you can’t refuse.

So the view has to be that sometimes _F_ -norms take precedence over moral
norms, but not always. There must thus be norms which are neither _F_ -norms
nor moral norms that decide whether _F_ -norms or moral norms take precedence.
We can call these “overall norms of combination”. And it is crucial to the
view that the norms of combination themselves be neither _F_ -norms nor moral
norms.

Moreover, the view has the following difficulty: It seems that the best way to
define a type of norm (prudential, meaningfulness, moral, etc.) is in terms of
the types of consideration that the norm is based on. But if the overall norms
of combination take into account the very same types of consideration as the
moral norms of combination, then this way of distinguishing the types of norms
is no longer available.

This view violates Ockham’s razor: Why would we have moral norms of
combination if the overall norms of combination always override them anyway?

Sacrifice a slight amount of _F_ -considerations for a great deal of good for
one’s children.

But here is an oddity. Morality already combines _F_ -considerations and first
order paradigmatically moral considerations. Consider two actions:

Some philosophers think that sometimes norms other than moral norms—e.g.,
prudential norms or norms of the meaningfulness of life—take precedence over
moral norms and make permissible actions that are morally impermissible. Let
_F_ -norms be such norms.

_Morality_ says that (1) is obligatory but (2) is permitted. Thus, morality
already weighs _F_ and paradigmatically moral concerns and provides a
combination verdict. In other words, there already are _moral_ norms of
combination. So the view would be that there are moral norms of combination
and overall norms of combination, both of which take into account exactly the
same first order considerations, but sometimes come to different conclusions
because they weigh the very same first order considerations differently (e.g.,
in the case where a moderate amount of _F_ -considerations needs to be
sacrificed for a moderate amount of good for one’s children).

Sacrifice an enormous amount of _F_ -considerations for a slight good for
one’s children.

A view where _F_ -norms _always_ override moral norms does not seem plausible.
In the case of prudential or meaningfulness, it would point to a fundamental
selfishness in the normative constitution of the human being.

Maybe there is a view on which the overall ones take into account not the
first-order moral and _F_ -considerations, but only the deliverances of the
moral and _F_ -norms of combination, but that seems needlessly complex.

Mutual enmity: _x_ and _y_ have shared knowledge that they each pursue the
other’s ill-being for a reason other than the other’s well-being.

Both competition and moral opposition are compatible with mutual love, but
mutual enmity is not compatible with either direction of love.

Moral opposition: _x_ and _y_ have shared knowledge that they are pursuing
incompatible goals and each takes the other’s pursuit to be morally wrong.

The reason for the qualification on reasons in 3 is that one might say that
someone who punishes someone in the hope of their reform is pursuing their
ill-being for the sake of their well-being. I don’t know if that is the right
way to describe reformative punishment, but it’s safer to include the
qualification in (3).

Michael Huemer advances a version of this argument in his "Ethical
Intuitionism", somewhere around page 180 or 190 or the like (I don't have the
book in front of me, sorry). I forget the exact details, but it may be worth
checking out what he said.

I suspect that most cases of mutual enmity are also cases of moral opposition,
but I am less clear on this.

Woops. This was supposed to be a comment on the overriding moral reasons post.
Sorry!

I think loving one’s competitors could be good practice for loving one’s (then
necessarily non-mutual) enemies.

Note that cases of moral opposition are all cases of competition. Cases of
mutual enmity are also cases of competition, except in rare cases, such as
when a party suffers from depression or acedia which makes them not be opposed
to their own ill-being.

This has the odd result that on externalist consequentialism, in most sports
and other games, at least one side is acting wrongly. For it is extremely rare
that there is an exact tie between the values of one side winning and the
value of the other side winning. (Some people enjoy victory more than others,
or have somewhat more in the way of fans, etc.)

I wonder if consequentialism can be salvaged from this argument with the
following consideration:  
  
Suppose that the utility of winning accrues only if the victory is achieved
against a capable opponent who is highly motivated to win. (For example, I
have seen boxing matches where the victor is visibly disappointed or even
enraged when he perceives that his opponent threw in the towel prematurely.)
In this case, each competitor would be behaving morally if and only if he
tries his hardest to win, even given consequentialism.

So, the result is that either on externalist or internalist consequentialism,
in most sports and other competitions, at least one side is acting morally
wrongly or is acting in the light of an epistemic vice.

In the ideal case, competitors both rightly pursue the incompatible goals, and
each knows that they are both so doing.

On internalist consequentalism, where the right action is defined by expected
utilities, we would expect that if both sides are unbiased investigators, in
most of the games, at least one side would at take the expected utility of the
other side’s winning to be higher. For if both sides are perfect investigators
with the same evidence and perfect priors, then they will assign the same
expected utilities, and so at least one side will take the other’s to have
higher expected utility, except in the rare case where the two expected
utilities are equal. And if both sides assign expected utilities completely at
random, but unbiasedly (i.e., are just as likely to assign a higher expected
utility to the other side winning as to themselves), then bracketing the rare
case where a side assigns equal expected utility to both victory options, any
given side will have a probability of about a half of assigning higher
expected utility to the other side’s victory, and so there will be about a 3/4
chance that at least one side will take the other side’s victory to be more
likely. And other cases of unbiased investigators will likely fall somewhere
between the perfect case and the random case, and so we would expect that in
most games, at least one side will be playing for an outcome that they think
has lower expected utility.

Yeah, I think I made a number of mistakes in my argument.  
  
On consequentialism what is evaluated is the action, not the end of the
action. So even if my winning has lower utility than your winning under
similar circumstances, it does not follow that my trying to win has lower
utility than my not trying to win. There are two possibilities, after all,
assuming I try to win. Either I will succeed or I won't. If I won't succeed,
then the harder I tried, the more enjoyable the victory for you, for your fans
and for my fans. If I do succeed, the utility is lower than if I had tried and
you nonetheless succeeded (I am assuming your victory has higher utility), but
it may still be higher than had I failed to try (in which case very likely I
would not have succeeded) because then there would have been general
disappointment. So a case can be made that my trying to win is better than my
not trying to win, even if other things being equal my winning is worse than
your winning. And similar things seem to hold on expected utility versions.  
  
Still, suppose that the utility of your winning is higher than of my winning,
and I am about to win, but I have some subtle way of throwing the game so no
one can tell, and everyone will enjoy the game pretty much as much. Then on
externalist consequentialism I should throw the game in this way. Whether such
a way of throwing the game is available depends on many factors: what the game
is, who the audience are, what level one and one's opponent are. Still, there
are probably a number of combinations where such throwing is available, and
the slight loss in quality of play is outweighed by the benefits of your
winning.

Given externalist consequentialism, where the right action is the one that
actually would produce better consequences, ideal competition will be
extremely rare, since the only time the pursuit of each of two incompatible
goals will be right is if there is an exact tie between the values of the
goals, and that is extremely rare.

I’ve been thinking about who competitors, opponents and enemies are, and I am
not very clear on it. But I think we can start with this:

Of course, in practice, the two sides are not unbiased. One might overestimate
the value of oneself winning and the underestimate the value of the other
winning. But that is likely to involve some epistemic vice.

What about ( _a_ , _b_ )? Can that always have real number length _b_ − _a_ if
_a_ < _b_? No. For if we had that, then we would absurdly have:

Let _α_ be the non-zero infinitesimal length of a single point. Then [ _a_ ,
_a_ ] is a single point. Its length thus will be _α_ , and not _a_ − _a_ = 0.
So [ _a_ , _b_ ] can’t _always_ have real-number length _b_ − _a_. But maybe
at least it can in the case where _a_ < _b_? No. For suppose that _m_ ([ _a_ ,
_b_ ]) = _b_ − _a_ whenever _a_ < _b_. Then _m_ (( _a_ , _b_ ]) = _b_ − _a_ −
_α_ whenever _a_ < _b_ , since ( _a_ , _b_ ] is missing exactly one point of [
_a_ , _b_ ]. But then let _c_ = ( _a_ + _b_ )/2 be the midpoint of [ _a_ , _b_
]. Then:

_m_ (( _a_ , _b_ )) = _m_ (( _a_ , _c_ )) + _α_ \+ _m_ (( _c_ , _b_ )) = _c_ −
_a_ \+ _α_ \+ _b_ − _c_ = _b_ − _a_ \+ _α_ ,

_m_ ([ _a_ , _b_ ]) = _m_ ([ _a_ , _c_ ]) + _m_ (( _c_ , _b_ ]) = ( _c_ − _a_
) + ( _b_ − _c_ − _α_ ) = _b_ − _a_ − _α_ ,

That leaves [ _a_ , _b_ ) and ( _a_ , _b_ ]. By symmetry if one has length _b_
− _a_ , surely so does the other. And in fact Milovich gave me [a
proof](https://mathoverflow.net/questions/108170/hyperreal-finitely-additive-
measure-on-0-1-assigning-b-a-to-a-b-or-a-b) that there is no contradiction in
supposing that _m_ ([ _a_ , _b_ )) = _m_ (( _b_ , _a_ ]) = _b_ − _a_.

since ( _a_ , _b_ ) is equal to the disjoint union of ( _a_ , _c_ ), the point
_c_ and (c,b).

Suppose that you want to measure the size _m_ ( _I_ ) of an interval _I_ , but
you have the conviction that single points matter, so [ _a_ , _b_ ] is bigger
than ( _a_ , _b_ ), and you want to use infinitesimals to model that
difference. Thus, _m_ ([ _a_ , _b_ ]) will be infinitesimally bigger than _m_
(( _a_ , _b_ )).

As usual, write [ _a_ , _b_ ] for the interval of the real line from _a_ to
_b_ including both _a_ and _b_ , ( _a_ , _b_ ) for the interval of the real
line from _a_ to _b_ excluding _a_ and _b_ , and [ _a_ , _b_ ) and ( _a_ , _b_
] respectively for the intervals that include _a_ and exclude _b_ and vice
versa.

At the same time, intuitively, _some_ intervals from _a_ to _b_ should have
length _exactly_ _b_ − _a_ , which is a real number (assuming _a_ and _b_ are
real). Which ones? The choices are [ _a_ , _b_ ], ( _a_ , _b_ ), [ _a_ , _b_ )
are ( _a_ , _b_ ].

Thus at least some intervals will have lengths that aren’t real numbers: their
length will be a real number plus or minus a (non-zero) infinitesimal.

Now, embed _V_ in a hyperreal field _V_ 2 that contains a supremum for every
subset of _V_ , and embed _V_ 2 in _V_ 3 which has a supremum for every subset
of _V_ 2. Let _Ω_ be our probability space.

The problem of how to value the St Petersburg paradox. The particular version
that interests me is one from [Russell and
Isaacs](https://philarchive.org/rec/RUSINP-2) which says that any finite value
is too small, but any infinite value violates strict dominance (since, no
matter what, the payoff will be less than infinity).

How to value gambles on a countably infinite fair lottery where the gamble is
positive and asymptotically approaches zero at infinity. The
[problem](http://alexanderpruss.blogspot.com/2022/11/dominance-and-countably-
infinite-fair.html) is that any positive non-infinitesimal value is too big
and any infinitesimal value violates strict dominance.

Let _X_ be the space of bounded _V_ 2-valued functions on _Ω_ and let _M_ ⊆
_X_ be the subspace of simple functions (with respect to the algebra of sets
that _Ω_ is defined on). For _f_ ∈ _M_ , let _ϕ_ ( _f_ ) be the integral of
_f_ with respect to _p_ , defined in the obvious way. The supremum on _V_ 2
(which has values in _V_ 3) is then a seminorm dominating _ϕ_. Extend _ϕ_ to a
_V_ -linear function _ϕ_ on _X_ dominated by _V_ 2. Note that if _f_ > 0
everywhere for _f_ with values in _V_ , then _f_ > _α_ > 0 everywhere for some
_α_ ∈ _V_ 2, and hence _ϕ_ (− _f_ ) ≤ − _α_ by seminorm domination, hence 0 <
_α_ ≤ _ϕ_ ( _f_ ). Letting _E_ _p_ be _ϕ_ restricted to the _V_ -valued
functions, our construction is complete.

The apparent solution works as follows. For any gamble with values in some
real or hyperreal field _V_ and any finitely-additive probability _p_ with
values in _V_ , we generate a hyperreal expected value _E_ _p_ , which
satisfies these plausible axioms:

Dominance: if _f_ ≤ _g_ everywhere, then _E_ _p_ _f_ ≤ _E_ _p_ _g_ , and if
_f_ < _g_ everywhere, then _E_ _p_ _f_ < _E_ _p_ _g_.

Probability-match: _E_ _p_ 1 _A_ = _p_ ( _A_ ) for any event _A_ , where 1 _A_
is 1 on _A_ and 0 elsewhere

I think like this. First it looks like the [Hahn-Banach dominated extension
theorem](https://en.wikipedia.org/wiki/Hahn%E2%80%93Banach_theorem) holds for
_V_ 2-valued _V_ 1-linear functionals on _V_ 1-vector spaces _V_ 1 ⊆ _V_ 2 are
real or hyperreal field, except that our extending functional may need to take
values in a field of hyperreals even larger than _V_ 2. The crucial thing to
note is that any subset of a real or hyperreal field has a supremum in a
larger hyperreal field. Then where the proof of the Hahn-Banach theorem uses
infima and suprema, you move to a larger hyperreal field to get them.

How does this get around the arguments I link to in (1) and (2) that seem to
say that this can’t be done? The trick is this: the expected value has values
in a hyperreal field _W_ which will be larger than _V_ , while (4)–(6) only
hold for gambles with values in _V_. The idea is that we distinguish between
what one might call primary values, which are particular goods in the world,
and what one might call distribution values, which specify how much a random
distribution of primary values is worth. We do not allow the distribution
values themselves to be the values of a gamble. This has some downsides, but
at least we can have (4)–(6) on _all_ gambles.

How to evaluate expected utilities of gambles whose values are hyperreal,
where the probabilities may be real or hyperreal, which I raise in Section 4.2
of my paper on [accuracy in infinite domains](http://philsci-
archive.pitt.edu/21251/).

Linearity: _E_ _p_ ( _a_ _f_ + _b_ _g_ ) = _a_ _E_ _p_ _f_ \+ _b_ _E_ _p_ _g_
for _a_ and _b_ in _V_

Disjoint Additivity: If _U_ 1 and _U_ 2 are wagers supported on disjoint
events (i.e., there is no _n_ such  
that _U_ 1( _n_ ) and _U_ 2( _n_ ) are both non-zero), then _E_ _p_ ( _U_ 1+
_U_ 2) = _E_ _p_ _U_ 1 \+ _E_ _p_ _U_ 2.

You could restrict previsions to real-valued wagers. (This is not entirely
arbitrary. What would it mean to win $β?) Then the wager ‘constant β’ would
have no prevision. So there would be no contradiction. The best you could do
would be ‘constant zero’. This has prevision zero, which is strictly less than
β, as expected.

Dominance: If _U_ 1 < _U_ 2 everywhere, then _E_ _p_ _U_ 1 < _E_ _p_ _U_ 2.

Yeah. I have a more general solution along the same lines. Will post soon.

Binary Wagers: If _U_ is 0 outside _A_ and _c_ on _A_ , then _E_ _p_ _U_ = _c_
_P_ ( _A_ ).

Thus, _E_ _p_ _U_ is a non-zero infinitesimal _β_. But then _β_ < _U_ ( _n_ )
for all _n_ , and so by Binary Wagers and Dominance, _β_ < _E_ _p_ _U_ , a
contradiction.

But we can’t. For suppose we have it. Let _U_ ( _n_ ) = 1/(2 _n_ ). Fix a
positive integer _m_. Let _U_ 1( _n_ ) be 2 for _n_ ≤ _m_ \+ 1 and 0
otherwise. Let _U_ 2( _n_ ) be 1/ _m_ for _n_ > _m_ \+ 1 and 0 for _n_ ≤ _m_
\+ 1. Then by Binary Wagers and by the fact that each ticket has infinitesimal
probability, _E_ _p_ _U_ 1 is an infinitesimal _α_ (since the probability of
any finite set will be infinitesimal). By Binary Wagers and Dominance, _E_ _p_
_U_ 2 ≤ 1/( _m_ +1). Thus by Disjoint Additivity, _E_ _p_ ( _U_ 1+ _U_ 2) ≤
_α_ \+ 1/( _m_ +1) < 1/ _m_. But _U_ < _U_ 1 \+ _U_ 2 everywhere, so by
Dominance we have _E_ _p_ _U_ < 1/ _m_. Since 0 < _U_ everywhere, by Dominance
and Binary Wagers we have 0 < _E_ _p_ _U_.

Suppose we have a finitely-additive probability assignment _p_ (perhaps real,
perhaps hyperreal) for a countably infinite lottery with tickets 1, 2, ... in
such a way that each ticket has infinitesimal probability (where zero counts
as an infinitesimal). Now suppose we want to calculate the expected value or
previsio _E_ _p_ _U_ of any bounded wager _U_ on the outcome of the lottery,
where we think of the wager as assigning a value to each ticket, and the wager
is bounded if there is a finite _M_ such that | _U_ ( _n_ )| < _M_ for all
_n_.

But clearly heaven, purgatory and hell in the interim state is something we
should care about.

First, the soul isn't the part of me that normally feels pain. The soul is the
part of me by virtue of which *I* feel pain. When I feel pain, there is only
one thing that feels pain--me, not me and my soul.  
  
Second, imagine that materialism is true, and the pain center of your brain is
removed from your head and put in a vat. Then that pain center is stimulated.
Should you specially care? Not at all! It's formerly your pain center--it was
that by which you feel pain--but it's not connected in the right way to the
whole, so what happens to it is irrelevant. Or suppose that we have a version
of materialism on which during a cerebrum transplant you stay with the
cerebrumless body (e.g., some versions of animalism). Your cerebrum is removed
and pain-stimulated in a vat. In terms of special care, this is surely
irrelevant.

Isn't the reason that you wouldn't care what happens to your body after you
die is not just that it's not a part of you when it happens but also that the
part of you that would normally feel the pain (if you were
burned/decayed/dissected) doesn't.  
  
With the soul you (run with me for a little on this) would
suffer/struggle/enjoy the afterlife. Now obviously you on curroptionism
wouldn't suffer/struggle/enjoy but the part of you that normally would feel
the pain does. And so the special care may still be applied under
curroptionism.

According to Catholic corruptionists, when I die, my soul will continue to
exist, but I won’t; then at the Resurrection, I will come back into existence,
receiving my soul back. In the interim, however, it is my soul, not I, who
will enjoy heaven, struggle in purgatory or suffer in hell.

Of course, for any thing that enjoys heaven, strugges in purgatory or suffers
in hell, I should care that it does so. But should I have that kind of special
care that we have about things that happen to ourselves for what happens to
the soul? I say not, or at most slightly. For suppose that it turned out on
the correct metaphysics that my matter continues to exist after death. Should
I care whether it burns, decays, or is dissected, with that special care with
which we care about what happens to ourselves? Surely not, or at most
slightly. Why not? Because the matter won’t be a part of me when this happens.
(The “at most slightly” flags the fact that we can care about “dignitary
harms”, such as nobody showing up at our funeral, or us being defamed, etc.)

Totality, reflexivity, transitivity and strong _G_ -invariance for value
distributions follows from the same conditions for subsets of _Ω_. Regularity
of ≼ on the subsets of _Ω_ and additivity implies that if _A_ ⊂ _B_ then _A_ ≺
_B_. The Pareto condition for ≼ on the value distributions follows since if
_f_ and _g_ satisfy are such that _f_ ( _x_ ) ≤ _g_ ( _x_ ) for all _x_ with
strict inequality for some _x_ , then _f_ * ⊂ _g_ *. Finally, the complicated
sameness independence condition follows from additivity.

**Theorem:** Assume the Axiom of Choice. Suppose ≤ on _V_ is reflexive,
transitive and non-trivial in the sense that it contains two values _v_ and
_w_ such that _v_ < _w_. There exists a reflexive, transitive preference
ordering ≼ on the value distributions satisfying (4)–(6) if and only if there
is such an ordering that is total if and only if _G_ has locally finite action
on _X_.

Finally, we want to have some sort of symmetries on the population. The most
radical would be that the value distributions don’t care about permutations of
people, but more moderate symmetries may be required. For this we need a group
_G_ of permutations acting on _X_.

**Proof of Therem:** Suppose that _G_ has locally finite action. Define _Ω_ =
_X_ × _V_. By Theorem 2 of my invariance of [non-classical probabilities
paper](https://arxiv.org/abs/2010.07366), there is a strongly _G_ -invariant
regular (i.e., ⌀ ≺ _A_ if _A_ is non-empty) qualitative probability ≼ on _Ω_.
Given a value distribution _f_ , let _f_ * = {( _x_ , _v_ ) : _v_ ≤ _f_ ( _x_
)} be a subset of _Ω_. Define _f_ ≼ _g_ iff _f_ * ≼ _g_.

Here, _f_ ∘ _g_ is the value distribution where site _x_ gets _f_ ( _g_ ( _x_
)).

Strong _G_ -invariance: if _g_ ∈ _G_ and _f_ is a value distribution, then _f_
∘ _g_ ≈ _f_.

Sameness independence: if _f_ 1, _f_ 2, _g_ 1, _g_ 2 are value distributions
and _A_ ⊆ _X_ is such that (a) _f_ 1 ≼ _f_ 2, (b) _f_ 1( _x_ ) = _f_ 2( _x_ )
and _g_ 1( _x_ ) = _g_ 2( _x_ ) if _x_ ∉ _A_ , (c) _f_ 1( _x_ ) = _g_ 1( _x_ )
and _f_ 2( _x_ ) = _g_ 2( _x_ ) if _x_ ∈ _A_.

In other words, the mutual ranking between two value distributions does not
depend on what the two distributions do to the people on whom the
distributions agree. If it’s better to give $4 to Jones than to give $2 to
Smith when Kowalski is getting $7, it’s still better to give $4 to Jones than
to give $2 to Smith when Kowalski is getting $3. There is probably some other
name in the literature for this property, but I know next to nothing about
social choice literature.

The trick to the proof of the Theorem is to reduce preferences between
distributions to comparisons of subsets of _X_ × _V_ and to reduce comparisons
of subsets of _X_ to preferences between binary distributions.

Note that while it is natural to think of _X_ has just a set of people or of
locations, [inspired by Kenny
Easwaran](https://www.google.com/url?q=https%3A%2F%2Fwww.dropbox.com%2Fs%2Fdklfwsl2ql1rt6s%2FAggregation.pdf%3Fraw%3D1&sa=D&sntz=1&usg=AOvVaw2nfKx0sldlPHYVX-
lddt22) one can also think of it as a set _Q_ × _Ω_ where _Ω_ is a probability
space and _Q_ is a population, so that _f_ ( _x_ , _ω_ ) represents the value
_x_ gets at location _ω_. In that case, _G_ might be defined by symmetries of
the population and/or symmetries of the probability space. In such a setting,
we might want a weaker Pareto principle that supposes additionally that _f_ (
_x_ , _ω_ ) < _g_ ( _x_ , _ω_ ) for some _x_ and _all_ _ω_. With that weaker
Pareto principle, the proof that the existence of a _G_ -invariant preference
of the right sort on the distributions implies local finiteness of action does
not work. However, I think we can still prove local finiteness of action in
that case if the symmetries in _G_ act only on the population (i.e., for all
_x_ and _ω_ there is an _y_ such that _g_ ( _x_ , _ω_ ) = ( _y_ , _ω_ )). In
that case, given a subset _A_ of the population _Q_ , we define _A_ † to be
the distribution that gives _w_ to all the persons in _A_ with certainty
(i.e., everywhere on _Ω_ ) and gives _v_ to everyone else, and the rest of the
proof should go through, but I haven’t checked the details.

Write _f_ ≈ _g_ when _f_ ≼ _g_ and _g_ ≼ _f_ , and _f_ ≺ _g_ when _f_ ≼ _g_
but not _g_ ≼ _f_. Similarly for values _v_ and _w_ , write _v_ < _w_ if _v_ ≤
_w_ but not _w_ ≤ _v_.

If _X_ is finite, then local finiteness of action is trivial. If _X_ is
infinite, then it will be satisfies in some cases but not others. For
instance, it will be satisfied if _G_ is permutations that only move a finite
number of members of _X_ at a time. It will on the other hand fail if _X_ is a
infinite bunch of people regularly spaced in a line and _G_ is shifts.

Of course, for a general social choice principle we need more than just a
decision whether to give one and the same good to the members of some set. But
we can still formalize those questions in terms of something pretty close to
qualitative probabilities. For a general framework, suppose a population set
_X_ (a set of people or places in spacetime or some other sites of value) and
a set of values _V_ (this could be a set of types of good, or the set of real
numbers representing values). We will suppose that _V_ comes with a transitive
and reflexive (preorder) preference relation ≤. Now let _Ω_ = _X_ × _V_. A
value distribution is a function _f_ from _X_ to _V_ , where _f_ ( _x_ ) = _v_
means that _x_ gets something of value _v_.

For instance, in the past I’ve proved theorems on qualitative probabilities. A
qualitative probability is a relation ≼ on the subsets of some sample space
_Ω_ such that:

Now suppose there is a (not necessarily total) strongly _G_ -invariant
reflexive and transitive preference ordering ≼ on the value distributions
satisfying (4)–(6). Given a subset _A_ of _X_ , define _A_ † to be the value
distribution that gives _w_ to all the members of _A_ and _v_ to all the non-
members, where _v_ < _w_. Define _A_ ≼ _B_ iff _A_ † ≼ _B_ †. This will be a
strongly _G_ -invariant reflexive and transitive relation on the subsets of
_X_. It will be regular by the Pareto condition. Finally, additivity follows
from the sameness independence condition. Local finiteness of action of _G_
then follows from Theorem 2 of my paper. ⋄

A comment by a referee of a recent paper of mine that one of my results in
decision theory didn’t actually depend on numerical probabilities and hence
could extend to social choice principles made me realize that this may be true
for some other things I’ve done.

I forgot to say that G acts on X x V by acting on the first component in the
proof.

But need not think of _Ω_ as a space of possibilities and of ≼ as a
probability comparison. We could instead think of it as a set of people who
are candidates for getting some good thing, with _A_ ≼ _B_ meaning that it’s
at least as good for the good thing to be distributed to the members of _B_ as
to the members of _A_. Axioms (1) and (2) are then obvious. And axiom (3) is
an independence axiom: whether it is at least as good to give the good thing
to the members of _B_ as to the members of _A_ doesn’t depend on whether we
give it to the members of a disjoint set _C_ at the same time.

Pareto: If _f_ ( _x_ ) ≤ _g_ ( _x_ ) for all _x_ with _f_ ( _x_ ) < _g_ ( _x_
) for some _x_ , then _f_ ≺ _g_.

We want to generate a reflexive and transitive preference ordering ≼ on the
set _V_ _X_ of value distributions.

if _A_ ∩ _C_ = _B_ ∩ _C_ = ⌀, then _A_ ≼ _B_ iff _A_ ∩ _C_ ≼ _B_ ∩ _C_
(additivity).

A group of symmetries _G_ has locally finite action a set _X_ provided that
for each finite subset _H_ of _G_ and each _x_ ∈ _X_ , applying finite
combinations of members of _G_ to _x_ generates only a finite subset of _X_.
(More precisely, if ⟨ _H_ ⟩ is the subgroup generated by _G_ , then ⟨ _H_ ⟩
_x_ is finite.)

Animalists think humans are animals. Suppose I am an animalist and I think
that I go with my cerebrum in cerebrum-transplant cases. That may seem weird.
But suppose we make an equal opportunity claim here: all animals that have
cerebra go with their cerebra. If your dog Rover’s cerebrum is transplanted
into a robotic body, then the cerebrumless thing is not Rover. Rather, Rover
inhabits a robotic body or that body comes to be a part of Rover, depending on
views about prostheses. And the same is true for any animal that has a
cerebrum.

Finally, compare this. Suppose Snaky a rattlesnake stretched along a line in
space. Now suppose we simultaneously annihilate everything in Snaky. Now,
“simultaneously” is presumably defined with respect to some reference frame
_F_ 1. Let _z_ be a point in Snaky’s rattle located just prior (according to
_F_ 1) to Snaky’s destruction. Then Snaky is partly present at _z_. But with a
bit of thought, we can see that there is another reference frame _F_ 2 where
the only parts of Snaky simultaneous with _z_ are parts of the rattle: all the
non-rattle parts of Snaky have already been annihilated at _F_ 2, but the
rattle has not. Then in _F_ 2 the following is true: there is a time at which
Snaky exists but nothing outside of Snaky’s rattle exists. Hence Snaky can
exist as just a rattle, albeit for a very, very short period of time.

But compare this: Some animals can partly exist in spatial locations where
they have no living cells, and others cannot. The outer parts of my hairs are
parts of me, but there are no living cells there. If my hair is in a room,
then I am partly in that room, even if no living cells of mine are in the
room. But on the other hand, there are some animals (at least the unicellular
ones, but maybe also some soft invertebrates) that can only exist where they
have a living cell.

The person who thinks survival reduced to a cerebrum is implausible for an
animal might, however, say that this is what’s odd about it. An animal reduced
to cerebrum lacks internal life support organs (heart, lungs, etc.) It is odd
to think that some animals can survive without internal life support and
others cannot.

It initially seems weird to say that some animals can survive reduced to a
cerebrum and others cannot. But it’s not that weird when we add that the ones
that can’t survive reduced to a cerebrum are animals that don’t _have_ a
cerebrum.

Hence even a snake can exist without its life-support organs, but only for a
short period of time.

One might object that the spatial case and the temporal case are different,
because in the spatial case we are talking of partial presence and in the
temporal case of full presence. But a four-dimensionalist will disagree. To
exist at a time is to be partly present at that time. So to a four-
dimensionalist the analogy is pretty strict.

But perhaps we could have a subtler story on which goods reduce not just to
reasons to promote them, but to reasons to “stand for them” (taken as the
opposite of “standing against them”), where promotion is one way of “standing
for” a good, but there are others, such as celebration. It does not make sense
to promote the existence of God, the existence of agents, or the Pythagorean
theorem, but celebrating these goods makes sense.

But there seem to be goods that give no one a reason to promote them. Consider
the good fact that there exist (in the eternalist sense: existed, exist now,
will exist, or exist timelessly) agents. No agent can promote the fact that
there exist agents: that good fact is part of the agent’s thrownness, to put
it in Heideggerese.

Perhaps, though, we can modify the story in terms of goods-for- _x_ , and say
that _G_ is good-for- _x_ to the extent that _x_ should stand for _G_. But
that doesn’t seem right, either. I should stand for justice for all, and not
merely to the degree that justice-for-all is good-for-me. Moreover, there
goods that are good for non-agents, while a non-agent does not have a reason
to do anything.

Maybe, though, this isn’t quite right. If Alice is an agent, then Alice’s
existence is a good, but the fact that some agent or other exists isn’t a good
as such. I’m not sure. It seems like a world with agents is better for the
existence of agency, and not just better for the particular agents it has.
Adding _another_ agent to the world seems a lesser value contribution than
just ensuring that there is agency at all. But I could be wrong about that.

The simplest story would be that goods reduce to reasons to promote them.

However, while it might be the case that something is good just in case an
agent should “stand for it”, it does not seem right to think that it is good
_to the extent that_ an agent should “stand for it”. For the degree to which
an agent should stand for a good is determined not just by the magnitude of
the good, but the agent’s relationship to the good. I should celebrate my
children’s accomplishments more than strangers’.

I love reductions. But alas it looks to me like reasons and goods are not
reducible in either direction.

Another family of goods, though, are necessary goods. That God exists is good,
but it is necessarily true. That various mathematical theorems are beautiful
is necessarily true. Yet no one has reason to promote a necessary truth.

The map from axiology to moral reasons is quite complex, contextual, and
heavily agent-centered. The hope of reducing moral reasons to axiology is very
slim indeed.

In 2018, the Belgians beat the Brazilians 2-1 in the 2018 World Cup soccer
quarterfinals. There are about 18 times as many Brazilians and Belgians in the
world. This raises a number of puzzles in value theory, if for simplicity we
ignore everyone but Belgians and Brazilians in the world.

However, these considerations seem to me to depend to some degree on which
decisions one is making. If Daniel is on the soccer team and deciding how hard
to work, it makes little difference whether he is on the Belgian or Brazilian
team. But suppose instead that Daniel is has two talents: he could become an
excellent nurse or a top soccer player. As a nurse, he would help relieve the
suffering of a number of patients. As a soccer player, in addition to the
intrinsic goods of the sports, he would contribute to his fellow citizens’
pleasure and desire satisfaction. In _this_ decision, it seems that the number
of fellow citizens _does_ matter. The number of people Daniel can help as a
nurse is not very dependent on the total population, but the number of people
that his soccer skills can delight varies linearly with the total population,
and if the latter number is large enough, it seems that it would be quite
reasonable for Daniel to opt to be a soccer player. So we could have a case
where if Daniel is Belgian he should become a nurse but if Brazilian then a
soccer player (unless Brazil has a significantly greater need for nurses than
Belgium, that is). But once on the team, it doesn’t seem to matter much.

You might think that the good of the many outweighs the good of the few, and
Belgians are few. But, clearly, the above facts gave very little moral reason
to the Belgian players to lose. One might respond that the above facts gave
lots of reason to the Belgians to lose, but these reasons were outweighed by
the great value of victory to the Belgian players, or perhaps the significant
intrinsic value of playing a sport as well as one can. Maybe, but if so then
just multiply both countries’ populations by a factor of ten or a hundred, in
which case the difference between the goods (desire satisfaction, pleasure and
truth of belief) is equally multiplied, but still makes little or no moral
difference to what the Belgian players should do.

That said, I do think that the larger population of Brazil imbues the
Brazilians’ games and practices with _some_ not insignificant additional moral
weight than the Belgians’. It would be odd if the pleasure, desire
satisfaction and expectations of so many counted for _nothing_. But on the
other hand, it should make no significant difference to the Belgians whether
they are playing Greece or Brazil: the Belgians shouldn’t practice less
against the Greeks on the grounds that an order of magnitude fewer people will
be saddened when the Greeks lose than when Brazilians do.

An order of magnitude more people _wanted_ the Brazilians to win, and getting
what one wants is good. An order of magnitude more people would have felt
significant and appropriate _pleasure_ had the Brazilians won, and an
appropriate pleasure is good. And given both wishful thinking as well as
reasonable general presumptions about there being more talent available in a
larger population base, we can suppose that a lot more people _expected_ the
Brazilians to win, and it’s good if what one thinks is the case is in fact the
case.

Or consider this from the point of view of the Brazilian players. Imagine you
are one of them. Should the good of Brazil—around two hundred million people
caring about the game—be a crushing weight on your shoulders, imbuing
everything you do in practice and in the game with a great significance? No!
It’s still “just a game”, even if the value of the good is spread through two
hundred million people. It would be weird to think that it is a minor
pecadillo for a Belgian to slack off in practice but a grave sin for a
Brazilian to do so, because the Brazilian’s slacking hurts an order of
magnitude more people.

As an Aristotelian who believes in individual forms, I’m puzzled about cases
of species-level flourishing that don’t seem reducible to individual
flourishing. On a biological level, consider how some species (e.g., social
insects, slime molds) have individuals who do not reproduce. Nonetheless it is
important to the flourishing of the _species_ that the species include some
individuals that do reproduce.

We might handle this kind of a case by attributing to other individuals their
_contribution_ to reproduction of the species. But I think this doesn’t solve
the problem. Consider a non-biological case. There are things that are
achievements of the human species, such as having reached the moon, having
achieved a four minute mile, or having proved the Poincaré conjecture. It
seems a stretch to try to individualize these goods by saying that we all
contributed to them. (After all, many of us weren’t even alive in 1969.)

However, there is still a puzzle. If it is a part of every human’s good that
“I am a member of a species that landed on the moon”, does that mean the good
is multiplied the more humans there are, because there are more instances of
this external flourishing? I think not. External flourishing is tricky this
way. The goods don’t always aggregate summatively between people in the case
of external flourishing. If external flourishing were aggregated summatively,
then it would have been better if Russia rather than Poland produced
Copernicus, because there are more Russians than Poles, and so there would
have been more people with the external good of “being a citizen of a country
that produced Copernicus.” But that’s a mistake: it is a good that each Pole
has, but the good doesn’t multiply with the number of Poles. Similarly, if
Belgium is facing off Brazil for the World Cup, it is not the case that it
would be way better if the Brazilians won, just because there are a lot more
Brazilians who would have the external good of “being a fellow citizen with
the winners of the World Cup.”

I think a good move for an Aristotelian who believes in individual forms is to
say that “No man or bee is an island.” There is an external flourishing in
virtue of the species at large: it is a part of _my_ flourishing that humans
landed on the moon. Think of how members of a social group are rightly proud
of the achievements of some famous fellow-members: we Poles are proud of
having produced Copernicus, Russians of having launched humans into space, and
Americans of having landed on the moon.

But now back to infinity. In the interpersonal moral Satan’s Apple, we have
infinitely many agents choosing between _A_ and _B_. But now instead of the
threshold being a finite number, the threshold is an infinite cardinality (one
can also make a version where it’s a co-cardinality). And this threshold has
the property that other people’s choices can _never_ be such that your choice
will put things above the threshold—either the threshold has already been met
without your choice, or your choice can’t make it hit the threshold. In the
finite case, it depended on the numbers involved whether you should choose _A_
or _B_. But the exact same reasoning as in the finite case, but now without
_any_ statistical inputs being needed, shows that you should choose _B_. For
it literally cannot make any difference to whether a disaster happens, no
matter what other people choose.

Let me take another look at the [interpersonal moral Satan’s
Apple](http://alexanderpruss.blogspot.com/2022/11/the-interpersonal-satans-
apple.html), but start with a finite case.

But in the infinite case, no matter what strategy other people adopt, whether
pure or mixed, choosing _B_ is better.

Intuitively, you should do some sort of expected utility calculation based on
your best estimate of the probability _p_ that among the _N_ − 1 people other
than you, _M_ − 1 will choose _B_. For if fewer or more than _M_ − 1 of them
choose _B_ , your choice will make no difference, and you should choose _B_.
If _F_ is the difference between the utilities of _B_ and _A_ , e.g., the
utility of feeding the apple to the hungry child (assumed to be fairly
positive), and _D_ is the utility of the disaster (very negative), then you
need to see if _p_ _D_ \+ _F_ is positive or negative or zero. Modulo some
concerns about attitudes to risk, if _p_ _D_ \+ _F_ is positive, you should
choose _B_ (feed the child) and if its negative, you shouldn’t.

In my previous post, I suggested that the interpersonal moral Satan’s Apple
was a reason to embrace causal finitism: to deny that an outcome (say, the
disaster) can causally depend on infinitely many inputs (the agents’ choices).
But the finite cases make me less confident. In the case where _N_ is large,
and our best estimate of the probability of another agent choosing _B_ is a
value _p_ not close to the threshold ratio _q_ , it still seems
counterintuitive that you should morally choose _B_ , and so should everyone
else, even though that yields the disaster.

But you might not have a uniform distribution. You might, for instance, have a
reasonable estimate that a proportion _p_ of other people will choose _B_
while the threshold is _M_ ≈ _q_ _N_ for some fixed ratio _q_ between 0 and 1.
If _q_ is not close to _p_ , then facts about the binomial distribution show
that the probability that _M_ − 1 other people choose _B_ goes approximately
exponentially to zero as _N_ increases. Assuming that the badness of the
disaster is linear or at most polynomial in the number of agents, if the
number of agents is large enough, choosing _B_ will be a good thing. Of
course, you might have the unlucky situation that _q_ (the ratio of threshold
to number of people) and _p_ (the probability of an agent choosing _B_ ) are
approximately equal, in which case even for large _N_ , the risk that you’re
near the threshold will be too high to allow you to choose _B_.

_"But now instead of the threshold being a finite number, the threshold is an
infinite cardinality (one can also make a version where it’s a co-
cardinality). And this threshold has the property that other people’s choices
can **never** be such that your choice will put things above the threshold —
either the threshold has already been met without your choice, or your choice
can’t make it hit the threshold."_  
  
Such a bad formulation.  
Rather than pointing out, what's "never" the case, one should rather point
out, what's always the case there. The case with such a threshold is, that
such a threshold is either met or not met with or without your choice. Or in
other words such a threshold is met with or without your choice or is not met
with or without your choice.  
So specifically and particularly you making a choice doesn't matter for the
conditions of such a threshold either being met or being not met. But what
matters for the conditions of such a threshold either being met or being not
met is the actual state of affairs of a specific and particular kind of set,
in which you might be contained or not contained.  
It doesn't matter, which particular and specific bricks are used to obtain a
specific and particular wall. But what matters for the wall is if the set of
all bricks making out that wall is corresponding to the set of all natural
numbers.  
As for you as a single and specifical or particular brick - well, you can now
choose between being a part of that infinte wall or being not a part of that
infinte wall - holding out terrorists, illigal immigrants AND legal
immigrants, such as your ancesters were at one point in time and history, OR
NOT doing that - you can give an apple to a child OR NOT do that.  
It’s your choice. You kinda have to make that choice and also have to leave
with the consequences resulting from that made choice of yours.

If you have a uniform distribution over the possible number of people other
than you choosing _B_ , the probability that this number is _M_ − 1 will be 1/
_N_ (since the number of people other than you choosing _B_ is one of 0, 1,
..., _N_ − 1). Now, we assumed that the benefits of _B_ are such that they
don’t outweigh the disaster even if everyone chooses _B_ , so _D_ \+ _N_ _F_ <
0. Therefore (1/ _N_ ) _D_ \+ _F_ < 0, and so in the uniform distribution case
you shouldn’t choose _B_.

Well, I think, that regardless of these probability considerations there is a
good reason - independent of your probability analysis - for making a choice
for option B here.  
I mean, that a feed and alive child in a prison is always better to have than
having the same [child dieing from
hunger](https://www.theworldcounts.com/challenges/people-and-poverty/hunger-
and-obesity/how-many-people-die-from-hunger-each-year).  
Sooo... I guess, that option B is objectively a better choice than option A
utility wise regardless of what else of "threshold"-this and
"probability"-that.  
But I guess, that's just my personal opinion here.

But I think in the finite case one can remove the counterintuitiveness. For
there are mixed strategies that if adopted by everyone are better than
everyone choosing _A_ or everyone choosing _B_. The mixed strategy will
involve choosing some number 0 < _p_ best < _q_ (where _q_ is the threshold
ratio at which the disaster happens) and everyone choosing _B_ with
probability _p_ best and _A_ with probability 1 − _p_ best, where _p_ best is
carefully optimized allow as many people to feed hungry children without a
significant risk of disaster. The exact value of _p_ best will depend on the
exact utilities involved, but will be close to _q_ if the number of agents is
large, as long as the disaster doesn’t scale exponentially. Now our
statistical reasoning shows that when your best estimate of the probability of
other people choosing _B_ is _not_ close to the threshold ratio _q_ , you
should just straight out choose _B_. And the worry I had is that everyone
doing that results in the disaster. But it does not seem problematic that in a
case where your data shows that people’s behavior is not close to optimal,
i.e., their behavior propensities do not match _p_ best, you need to act in a
way that doesn’t universalize very nicely. This is no more paradoxical than
the fact that when there are criminals, we need to have a police force, even
though ideally we wouldn’t have one.

For instance, maybe _B_ is feeding an apple to a hungry child, and _A_ is
refraining from doing so, but there is an evil dictator who likes children to
be miserable, and once enough children are not hungry, he will throw all the
children in jail.

Consider a situation where a _finite_ number _N_ of people independently make
a choice between _A_ and _B_ and some disastrous outcome happens if the number
of people choosing _B_ hits a threshold _M_. Suppose further that if you fix
whether the disaster happens, then it is better you to choose _A_ than _B_ ,
but the disastrous outcome outweighs all the benefits from all the possible
choices of _B_.

If causal finitism is the solution, then it is at least a little interesting
that the domain of moral obligations is smaller than the logically possible
even though it extends beyond the physically possible. (I’m taking it as given
that causal finitism doesn’t just follow from the PNC.)  
  
Actually, now that I think about it, is causal finitism a solution? Let's
grant that it is impossible for one effect to have infinitely many causes.
Assume I am ignorant about this fact. It surely isn’t impossible for me to
intend to do something that I mistakenly believe to be such a cause. And won’t
that mistaken belief generate a similar paradox?

Consider a moral interpersonal version of [Satan’s Apple](http://philsci-
archive.pitt.edu/1595/1/15.1.bayesbind.pdf): infinitely many people
independently choose whether to give a yummy apple to a (different) hungry
child, and if infinitely many choose to do so, some calamity happens to
everyone, a calamity outweighing the hunger the child suffers. You’re one of
the potential apple-givers and you’re not hungry yourself. The disaster
strikes if and only if infinitely many people _other than you_ give an apple.
Your giving an apple makes no difference whatsoever. So it seems like you
_should_ give the apple to the child. After all, you relieve one child’s
hunger, and that’s good whether or not the calamity happens.

Interesting. Mistaken belief can generate the belief that you are IN the
paradox, but it doesn't seem to generate the paradox itself. For it's not
going to be true that everyone doing the right thing (feeding hungry children)
results in disaster, just that we think it will.

_"Your giving an apple makes no difference whatsoever."_  
  
And yet the previous proposition of that _"interpersonal Satan's Apple"_
suggests otherwise to be the case, such that **you giving an apple MIGHT make
a difference WHATSOEVER**.  
Hm. I guess, that this nonsensical intuition comes from the false
presupposition of _"causal finitism"_.  
So don't wonder if you giving an apple might make a difference whatsoever,
such that you might find that apple of yours in the hands of a [hungry child
sitting behind bars alone or separated from his or her parents behind
bars](https://en.wikipedia.org/wiki/Trump_administration_family_separation_policy).  
  
Besides that, one might change the past. Well, not the past of your own
timeline. But if there are multiple similar timelines, then one might be able
to jump from one to another one, such that a change in the other timeline
doesn't alter anything in the previous timeline, like in [Dragon
Ball](https://dragonball.fandom.com/wiki/Time_Machine).  
One might call this phenomenon of multiple timelines independent or branching
timelines to be the "multiverse". Just a suggestion of mine.

Now, we deontologists are used to situations where a disaster happens because
one did the right thing. That’s because consequences are not the only thing
that counts morally, we say. But in the moral interpersonal Satan’s Apple,
there seems to be no deontology in play. It seems weird to imagine that
disaster could strike because everyone did what was consequentialistically
right.

One way out is causal finitism: Satan’s Apple is impossible, because the
disaster would have infinitely many causes.

One can also do the same thing within [Buchak’s REU
theory](https://smile.amazon.com/Risk-Rationality-Lara-Buchak/dp/0198801289),
since that theory is equivalent to applying LSI↑ with a probability
transformed by a monotonic map of [0,1] to [0,1] keeping endpoints fixed,
which is exactly what I did when moving from _P_ to _P_ _ϵ_.

In [yesterday’s post](http://alexanderpruss.blogspot.com/2022/11/how-to-
discount-small-probabilities.html), I argued that there is something
problematic about the idea of discounting small probabilities, given that in a
large enough lottery _every_ possibility with has a small probability. I then
offered a way of making sense of the idea by “trimming” the utility function
at the top and bottom.

Of course, _P_ _ϵ_ is not in general a probability, but it does satisfy the
Zero, Non-Negativity, Normalization and Monotonicity axioms, and we can now
use LSI↑ [level-set
integral](http://alexanderpruss.com/papers/InconsistentCredences.pdf) to
calculate utilities with _P_ _ϵ_.

If _U_ _ϵ_ is the “trimmed” utility function from my previous post, then LSI↑
_P_ _ϵ_ ( _U_ ) = _E_ ( _U_ 2 _ϵ_ ), so the two approaches are equivalent.

Fubini's theorem applies to expected values defined with respect to a measure.
The credence function P_e is not a measure in general, because in general it
fails finite additivity. Thus, the standard Lebesgue integral with respect to
P_e is undefined. I don't know what a "block integral" is.  
  
The point of level-set integrals for me is that they allow one to define a
fairly well-behaved expectation or prevision with respect to credence
assignments that are not probabilities because instead of additivity they only
satisfy monotonicity (P(A) is less than or equal to P(B) if A is a subset of
B).

Why calculate the expected utility via _"level-set integrals"_ , when
according to [Fubini](https://proofwiki.org/wiki/Fubini%27s_Theorem) you can
also calculate the expected utility with "block integrals" or any other
appropriate transformations of the x and y coordinates gaining the same exact
result?  
Why not calculate it with [polar
coordinates](https://en.wikipedia.org/wiki/Polar_coordinate_system)?  
Sure, that would be more difficult to do without having any rotational
symmetries here.  
But you can do that and by doing that properly you or we should gain the same
result for the expected utility.  
Sooo...  
What exactly makes _"level-set integrals"_ so special here?!?  
I don't see any particular good reason for this specific approach for
calculating expected utilities over "block integrals" here.

This morning, however, I noticed that one can also take the idea of
discounting small probabilities more literally and still get the exact same
results as by trimming utility functions. Specifically, given a probability
function _P_ and a probability discount threshold _ϵ_ , we form a credence
function _P_ _ϵ_ by letting _P_ _ϵ_ ( _A_ ) = _P_ ( _A_ ) if _ϵ_ ≤ _P_ ( _A_ )
≤ 1 − _ϵ_ , _P_ _ϵ_ ( _A_ ) = 0 if _P_ ( _A_ ) < _ϵ_ and _P_ _ϵ_ ( _A_ ) = 1
if _P_ ( _A_ ) > 1 − _ϵ_. This discounts close-to-zero probabilities to zero
and raises close-to-one probabilities to one. (We shouldn’t forget the second
or things won't work well.)

Ah, "block" is my term. :-)  
  
Here's the background for why I am interested in expected values with respect
to non-probabilities. The credences or degrees of belief of real human beings
are unlikely to be consistent. In particular, they are unlikely to satisfy the
axioms of probability, especially additivity. At the same time, real human
beings need a way of making predictions. Mathematical expectation is out,
because that requires at least a finitely-additive measure (normally Lebesgue
integrals are defined with respect to a countably-additive measure but they
can also be defined with respect to a finitely-additive one). So we need some
other method for making predictions or generating expectations when the
credences do not satisfy the axioms of probability.

The result is a precise theory (given the mysterious threshold _ϵ_ ). It
doesn’t neglect all possibilities with small probabilities, but rather it
trims low-probability outliers. The trimming procedure respects the fact that
often utility functions are defined up to positive affine transformations.

Here is my friendly proposal. Let _U_ be the utility function we want to
evaluate the value of. Let _T_ be the smallest value such that _P_ ( _U_ > _T_
) ≤ _ϵ_ /2. (This exists: _T_ = inf { _λ_ : _P_ ( _U_ > _λ_ ) ≤ _ϵ_ /2}.) Let
_t_ be the largest value such that _P_ ( _U_ < _t_ ) ≤ _ϵ_ /2 (i.e., _t_ = sup
{ _λ_ : _P_ ( _U_ < _λ_ ) ≤ _ϵ_ /2}). Take _U_ and replace any values bigger
than _T_ with _T_ and any values smaller than _t_ with _t_ , and call the
resulting utility function _U_ _ϵ_. We now replace _U_ with _U_ _ϵ_ in our
expected value calculations. (In the lottery example, we will be trimming from
both ends at the same time.)

Suppose throughout this post that _ϵ_ > 0 counts as our threshold of “very
small probabilities”. No doubt _ϵ_ < 1/100.

In this post I want to offer a precise and friendly amendment to the solution
of neglecting small probabilities. But first why we need an amendment.
Consider a game where an integer _K_ is randomly chosen between  − 1 and _N_
for some large fixed positive _N_ , so large that 1/(2+ _N_ ) < _ϵ_ , and you
get _K_ dollars. The game is clearly worth playing. But if you discount
“possibilities that have very small probabilities”, you are left with
_nothing_ : every possibility has a very small probability!

A very intuitive solution to a variety of problems in infinite decision theory
is that “for possibilities that have very small probabilities of occurring, we
should discount those probabilities down to zero” when making decisions
([Monton](https://quod.lib.umich.edu/cgi/p/pod/dod-idx/how-to-avoid-
maximizing-expected-utility.pdf?c=phimp;idno=3521354.0019.018;format=pdf)).

Moreover, the trimming procedure can yield an answer to what I think is the
biggest objection to small-probability discounting, namely that in a long
enough run—and everyone should think there is a non-negligible chance of
eternal life—even small probabilities can add up. If you are regularly offered
the same small chance of a gigantic benefit during an eternal future, and you
turn it down each time because the chance is negligible, you’re almost surely
missing out on an infinite amount of value. But we can apply the trimming
procedure at the level of choice of policies rather than of individual
decisions. Then if small chances are offered often enough, they won’t all be
trimmed away.

Perhaps this is uncharitable. Maybe the idea is not that we discount to zero
_all_ possibilities with small probabilities, but that we discount such
possibilities until the total discount hits the threshold _ϵ_. But while this
sounds like a charitable interpretation of the suggestion, it leaves the
theory radically underdetermined. For _which_ possibilities do we discount? In
my lottery case, do we start by discounting the possibilities at the low end (
− 1, 0, 1, ...) until we have hit the threshold? Or do we start at the high
end ( _N_ , _N_ − 1, _N_ − 2, ...) or somewhere in the middle?

There are [many](http://philsci-archive.pitt.edu/1595/1/15.1.bayesbind.pdf)
paradoxes of infinite sequences of decisions where the sequence of individual
decisions that maximize expected utility is unfortunate. Perhaps the most
vivid is Satan’s Apple, where a delicious apple is sliced into infinitely many
pieces, and Eve chooses which pieces to eat. But if she greedily takes
infinitely many, she is kicked out of paradise, an outcome so bad that the
whole apple does not outweigh it. For any set of pieces Eve eats, another
piece is only a plus. So she eats them all, and is damned.

If at each time you are choosing between a finite number of betting portfolios
fixed in advance, with the betting portfolio in each decision being tied to a
set of events wholly independent of all the later or earlier events or
decisions, with the overall outcome being just the sum or aggregation of the
outcomes of the betting portfolios, and with the utility of each portfolio
well-defined given your information, then you should at each time maximize
utility.

If so, then I guess, that any and every finite amount of slices will do for
Eve, as long as she doesn't go for any amount of slices with cardinality equal
to the cardinality of the set of all natural numbers. If she is ought to
maximise her utility AND there is no certain bound or limit to that
maximisation of a finite amount of slices of Satan's apple, then go figure,
what such a "maximum" in this case might be.  
 **As for me I will take an arbitrary amount of percentage below 100% of that
pie, I mean, of that _"Satan's apple"_ with an arbitrary finite amount of
slices.**  
Thank you very much.

In Satan’s Apple, for instance, the overall outcome is not just the sum of the
outcomes of the individual decisions to eat or not to eat, and so Satan’s
Apple is not a counterexample to (1). In fact, few of the paradoxes of
infinite sequences of decisions are counterexamples to (1).

Yup, in the story, taking all the even-numbered slices, or all the prime-
numbered slices, or all the power-of-two-numbered slices will get you kicked
out of paradise.

_"But if she greedily takes infinitely many, she is kicked out of paradise, an
outcome so bad that the whole apple does not outweigh it."_  
  
How about taking only the slices with even numbers?  
Is she then also doomed to leaving paradise, just because she took an infinite
amount of slices from Satan's apple, but not exactly the whole (100%) apple?!?  
 **∑n∈ℕ(1/2^n)/2=1/(1-1/2)·1/2=1 (=100% of Satan's apple)  
≠ ∑m∈ℕ(1/2^(2m))/2=∑m∈ℕ(1/4^m)/2=1/(1-1/4)·1/2=2/3 (≈66.67% of Satan's
apple)**  
  
Is Hilbert's Hotel not capable of accommodating new guests, just because all
and every room is currently occupied?  
From the wiki article for ["Hilbert's paradox of the Grand
Hotel"](https://en.wikipedia.org/wiki/Hilbert%27s_paradox_of_the_Grand_Hotel):  
 _ **Analysis**  
Hilbert's paradox is a veridical paradox: it leads to a counter-intuitive
result that is provably true. The statements "there is a guest to every room"
and "no more guests can be accommodated" are not equivalent when there are
infinitely many rooms...._  
  
And these are the problems/propositions, which philosophers are hung up on
these days.

I don’t know if there is something particularly significant about a paradox
violating (1). I think there is, but I can’t quite put my finger on it. On the
other hand, (1) is such a complex principle that it may just seem _ad hoc_.

Of course, infinite fair lotteries are dubious. So I don’t set much store by
this example.

Imagine a lottery where some positive integer _n_ is picked at random, with
all numbers equally likely, and if _n_ is picked, then you get 1/ _n_ units of
value. Should you play this lottery for free?

Professor Pruss,  
  
Here's a theological/philosophical question that I thought you might be
interested in. Speaking to the multitudes, John the Baptist says that "God is
able from these stones to raise up children to Abraham" (Luke 3:8). Setting
aside any questions of historicity (e.g. whether Abraham was a historical
figure, and so on), it can be safely assumed that John the Baptist (and those
to whom he was speaking) regarded the Judeans as literal, biological
descendants of Abraham. So it seems that, if taken at face value, he is saying
that God could turn the stones into literal, biological descendants of
Abraham. I wonder how this might bear on, for instance, essentiality of
origins.  
  
Of course, I think the solution is to avoid this sort of strict literalism,
but either way, it's fun to think about.

But what if someone offers you a random amount of positive value for free.
Strict dominance principles say it’s irrational to refuse it. But I am not
completely sure.

The expected value of the lottery is zero with respect to any finitely-
additive real-valued probability measure that fits the description (i.e.,
assign equal probablity to each number). And for any positive number _x_ , the
probability that you will get less than _x_ is one. It’s not clear to me that
it’s worth going for this.

True. Even still, one wonders what it would mean for God to turn a stone into
a child of Abraham; it wouldn't be a literal biological descendent, nor would
it have undergone a conversion. I suppose just creating a person and declaring
them to be under the Abrahamic covenant could suffice?  
  
Pointless overthinking, of course.

Suppose someone offers you, at no cost whatsoever, something of specified
positive value. However small that value, it seems irrational to refuse it.

If you like infinitesimals, you might say that the expected value of the
lottery is infinitesimal and the probability of getting less than some
positive number _x_ is 1 − _α_ for an infinitesimal _α_. That makes it sound
like a better deal, but it’s not all that clear.

But there is a difference: Bob pursues friendship because of the particular
ineffable “thick” kind of value that friendship has. Alice doesn’t know what
“thick” kind of value friendship has, but on the basis of Bob’s testimony, she
knows that it has some such value or other, and that it is a great and
significant value. As long as Alice knows what kinds of actions friendship
requires, she can pursue friendship without that knowledge, though it’s
probably more difficult for her, perhaps in the way that it is more difficult
for a tone-deaf person to play the piano, though in practice the tone-deaf
person could learn what kinds of finger movements result in aesthetically
valuable music without grasping that aesthetic value.

The Aristotelian tradition makes the grasp of the particular thick kind of
value involved in a virtuous activity be a part of the full possession of that
virtue. On that view, Alice cannot have the full virtue of friendship. There
is something she is missing out on, just as the tone-deaf pianist is missing
out on something. But she is not, I think, less praiseworthy than Bob. In fact
Alice’s pursuit of friendship involves the exercise of a virtue which Bob’s
does not: the virtue of faith, as exhibited in Alice’s trust in Bob’s
testimony about the value of friendship.

Suppose Alice is blind to the intrinsic value of friendship and Bob can see
the intrinsic value of friendship. Bob then told Alice that friendship is
intrinsically valuable. Alice justifiedly trusts Bob in moral matters, and so
Alice concludes that friendship has intrinsic value, even though she can’t
“see” it. Alice and Bob then both pursue friendship for its own sake.

1) You can indeed seek something for its own sake while also seeking it for
the sake of something else, but that implies there are two motives properly
distinct from the other; and one could then perhaps say that what defines the
motive of seeking something for its own sake is to not seek it for the sake of
something else AND to seek it for the intrinsic value it has. This motive,
having such a structure, would still be properly distinct from the other
motive which DOES seek something for the sake of something else.  
  
2) So about seeking a thing of intrinsic value instrumentally, I think this
just reifies (or is that the wrong word to use?) or reduces the intrinic value
of a thing to just a means - you could literally just replace it and have the
rich man tell you he's gonna give you much money if you find something
completely red today.  
  
The redness in this case, just like the intrinsic value in the other, is just
an identifier that you're looking for in order to gain something else. So I
think there's a confusion of meaning going on when someone says they pursue X
because of the intrinsic value it has. One could take this in an instrumental
sense, or one could instead take this in a sense similar to how one loves
others for their own sake, or oneself for one's own sake.  
  
When one seeks the good of another person for the other's own sake, I guess
one is thereby recognising the intrinsic axiology or value-ness of the person
and doing the action on the basis of that.  
  
One sees the value of the other and recognises that benefitting the other
person itself, taking the person as the end because they're valuable simply as
such (axiologically I guess?) is good. So benefitting them is just
intrinsically worth seeking of itself, with the end being the person, and the
grounds being the value & worth of the person properly distinct from any other
end.  
  
  
3) As for achieving X for the sake of Y, I think the person doing the
achieving is also crucial. For the person wanting to achieve X for the sake of
Y, Y is the final cause of **their achieving** X, not X by itself simpliciter.  
  
So it seems one could say that, for the person wanting to achieve X, if he
wanted to do this for its own sake, he'd be taking X as the final cause of the
very achieving itself, or the seekig to achieve. There's no problem in taking
X as the final cause of itself then, since it's not about a final cause
inhering in X itself.  
  
  
4) I'd also love to know the difference between seeking X for X's sake and
seeking it for its own sake, because those two seem identical to me - what is
"its own sake" in regards to X? How could it not be, well.....X itself? Since
the "own" is self-referential to X?  
  

It is tempting to say that you pursue a thing for its own sake provided that
you pursue it because of the intrinsic value you take it to have. But that,
too, is incorrect. For suppose that a rich benefactor tells you that they will
give you a ton of money if you gain something of intrinsic value today. You
know that truth is valuable for its own sake, so you find out something. In
doing so, you find out the truth _because_ the truth is intrinsically
valuable. But your pursuit of that truth is entirely instrumental, despite
your reason being the intrinsic value.

Perhaps it is impossible to do something for no reason. But even if it is
impossible to do something for no reason, it is incorrect to _define_ pursuing
something for its own sake as pursuing it not for the sake of something else.
For that you _pursue something for its own sake_ states something positive
about your pursuit, while that you _don’t pursue it for the sake of anything
else_ states something negative about your pursuit. There is a kind of valuing
of the thing for its own sake that is needed to pursue the thing for its own
sake.

1) Maybe seeking something for its own sake is a combination of both? Not
seeking it for the sake of something else, and also seeking it because of the
intrinsic value it has?  
  
2) As for the example of seeking the truth (which has intrinsic value, as
specified) for the money you'll be given, I think the usage of reason in _"But
your pursuit of that truth is entirely instrumental, despite your reason being
the intrinsic value."_ is a bit incomplete, since most people would use the
word "reason" to describe the actual goal they have in mind for which seeking
a true fact is purely instrumental.  
  
The intrinsic value of the truth then is kinda like any other property any
other thing might have that has utility - the intrinsic value is subordinated
and viewed in the light of the use it has for giving you money.  
  
You might as well be talking about seeking the proper tools to rob a bank with
a vast sum of money.

If there is a primitive notion in the vicinity, wouldn't it just be the three-
place predicate "x pursues y for the sake of z"? From here, we can analyze "x
pursues y for its own sake" as "x pursues y for the sake of y," and we can
analyze "x pursues y for the sake of something else" as "for some z, x pursues
y for the sake of z & z is not y."  
  
(Maybe there would be Frege-puzzle problems with this proposal, e.g., where y
= z but the agent doesn't know this, and pursues y for the sake of z?)

I suspect that pursuing a thing for its own sake is a primitive concept.

Wesley:  
  
1\. But you can seek something for its own sake while seeking it for the sake
of something else as well.  
  
2\. So, this is the weird thing about my example: the non-instrumental value
is being instrumentally pursued. (It kind of reminds me of Frege's infamous
"The concept horse is not a concept".) But the point remains that the non-
instrumental value is indeed a goal one has, just as when one is seeking to
rob a bank, the obtaining of the tools is a goal one has. Sure, you can use
"goal" or "reason" in such a way as to indicate the ultimate goal which one
non-instrumentally pursues, but then the account of non-instrumental pursuit
becomes circular: you non-instrumentally pursue X iff X is your non-
instrumentally pursued goal  
  
By the way, one can combine my truth and "volitional inertia" examples.
Suppose that a rich eccentric is paying me each time I get something of
intrinsic value. I am greedy and generally lacking in virtue, so I pursue all
sorts of things of intrinsic value solely for the sake of money. Then the
eccentric withdraws the offer. Out of volitional inertia, I continue to pursue
the things of intrinsic value, and do so because they have intrinsic value,
but I don't suddenly come to pursue them for their own sake. So in this
example, I pursue something because of its intrinsic value, and for no other
reason, and yet I do not pursue it for its own sake.  
  
I still think there is no way out of these cases other than to make the
concept of pursuit of a thing for its own sake primitive.

Hence, to pursue a thing for its own sake is not the same as to pursue it
because it has intrinsic value. Nor is it to pursue it not for the sake of
something else.

Here's another reason to think there is a difference. If I achieve x for the
sake of y, then y is a final cause of x. But if I achieve x for its own sake,
then x is not its own final cause. So to achieve x for its own sake is not the
same as to achieve x for the sake of x. And what goes for achievement probably
goes for pursuit.  
  
Now, you might say that if I achieve x for the sake of y AND x and y are
distinct, then y is a final cause of x. But now it looks like there is a
serious structural difference between achieving x for its own sake and
achieving x for the sake of y: in the latter case we have final causation and
in the former we don't. But now it seems "x for the sake of y" claims are
disjunctive in nature.

Brian:  
  
That's an option, but it seems to me that pursuing y for the sake of y is
different from pursuing y for its own sake, in the same way that x knowing
themselves is not the same thing as x knowing x, and similarly for other kinds
of reflexive actions. For x to play chess by themselves is not the same as for
x to play chess with x. The game is essentially different because when you
play chess by yourself you know what you're planning. (One could imagine a
case where Brian plays chess with Brian, without it being Brian playing chess
by himself, by supposing time-travel.) The Frege puzzles capture a part of the
difference, but I am not sure they capture all of it.

Suppose you pursue truth for its own sake. As we learn from Aristotle, it does
not follow that you don’t pursue truth for the sake of something else. For the
most valuable things are both intrinsically and instrumentally valuable, and
so they are typically pursued both for their own sake and for the sake of
something else.

What if you pursue something, but not for the sake of something else. Does it
follow that you pursue the thing for its own sake? Maybe, but it’s not as
clear as it might seem. Imagine that you eat fiber for the sake of preventing
colon cancer. Then you hear a study that says that fiber doesn’t prevent colon
cancer. But you continue to eat fiber, out of a kind of volitional inertia,
without any reason to do so. Then you are pursuing the consumption of fiber
not for the sake of anything else. But merely losing the instrumental reason
for eating fiber doesn’t give you a non-instrumentally reason. Rather, you are
now eating fiber irrationally, for no reason.

Alex  
  
You beg the question in your argument.  
Your premise 1 is only true if materialisme is false, but it is false if
materialism is true. Because in that case, a typical human being has just as
much intrinsic value as the arrangement of atoms because he is thé arrangement
of atoms.  
Intrinsic values depend on how things (can) behave, not on their constituants.  
And this particular arrangement of atoms behaves like a human being, hence has
the intrinsic value of a human being.

1\. The material aspect of the typical human being is an 80 kg atomic
arrangement. (Known by science.)  
2\. If materialism is true, a typical human being is identical with its
material aspect. (By definition of materialism)  
3\. So, materialism is false or a typical human being is identical with its
material aspect. (From 2 by definition of material conditional)  
4\. So, materialism is false or a typical human being is identical with an 80
kg atomic arrangement. (From 1 and 3)  
5\. So, if materialism is true, a typical human being is identical with an 80
kg atomic arrangement. (From 4 by definition of material conditional)  
  
Which step do you dispute?

The justification is empirical: The material aspect of the human being is
scientifically known to be an 80 kg atomic arrangement. If materialism is
true, the material aspect is the only aspect. So, if materialism is true, the
human being is an 80kg atomic arrangement.

On reflection, it is easy enough to get around my worry just by eliminating
the word ‘much’ from the first premise. I think the argument would go through
just as well. But I’m still puzzled about how to think of intrinsic value in
parts.

If they are merely weakly emergent, I don't see them making the value large
enough to contradict 1. It's still just an arrangement of atoms, a really cool
one, admittedly.  
  
If strongly emergent, then it's hard to say if we still have materialism.

Michael:  
  
I assume that the typical materialist thinks that statements about enzyme
activity and the like are statements about how atoms are arranged.  
  
Imagine a perfect computer simulation of the behavior of the atoms in a human
body. Either that simulation would include a simulation of enzyme activity or
not. If it does not, then we have weird top-down laws that ensure that the
microphysical laws have exceptions. I am open to that possibility but the
typical materialist is not. But if automatically the simulation of the
behavior of the atoms includes a simulation of enzyme activity, then the
materialist has a very good case that enzyme activity just is behavior of
atoms.

@Aron 1) Wow, you just buried yourself with your own arguments. If we can't
even be sure we ourselves exist, and all knowledge / perception / thinking
could just be false or irrational or even non-existent, then golly there can't
be such a thing as justification either.  
  
Certainly not individual but even less universal - other people could just not
exist as well, or be illusions, or whatever. Even if other people existed,
universal consensus or being convinced somethign is justified could also just
as well be false or non-existent.  
  
You should thereby become an absolute skeptic of everything.  
  
  
2) As for self-awareness depending on memory...every single memory you have
right now could just be false...but you'd still be aware you have those
memories. All your beliefs could be false & illusory, yet you'd still have
those beliefs.  
  
So unless the Principle of Non-contradiction is false...we can be absolutely
sure we have the memories / beliefs / experiences we actually have.  
  
You either have memories / beliefs / thoughts / experiences or you don't.  
  

Wesley C,  
  
"How do we rule out other forms of justification?" I don't want to rule out
other justifications to start with, that's why I wrote "empirical or
otherwise".  
"And even if one can't justify this universally to others, that doesn't mean
the justification isn't true."  
I don't accept any justification that is not universal. This is a
contradiction in terms. A justification must be repeatable and (in principle)
universally accessible to all.  
"For example, one knows one's own existence immediately and uniquely through
one's personal self-awareness."  
No, you doesn't know that, although this is admittedly tricky. To know is not
an "achievement verb" expressing an instantaneous event, but a "state verb",
which means that it is a state of an organism, and has a certain temporal
duration.  
Knowing, and also self-awareness and self-perception for that matter,
presuppose the correctness of memory, and because memory is fallible, there is
a chance (meaning that you cannot rule it out) that you are mistaken in
believing (and therefore you don't know) that you exist.

If materialism is true, a typical human being is an 80 kg arrangement of
atoms.

If all values are subjective, there is no objective reason to do or believe
anything, because every reason expresses the value of the thing it is a reason
for.

_"Which step do you dispute?”_  
  
I dispute step 3 _"So, materialism is not true. (From 1 "A typical human being
has much more intrinsic value than any 80 kg arrangement of atoms." and 2 "If
materialism is true, a typical human being is an 80 kg arrangement of atoms."_
[- maybe by a [modus tollens](https://en.wikipedia.org/wiki/Modus_tollens)?!?]
_)"_  
from your original post.  
I also reject premise 2 _"If materialism is true, a typical human being is an
80 kg arrangement of atoms."_ here as I reject premise 2 _"If materialism is
true, a typical human being is identical with its material aspect. (By
definition of materialism)"_ from your previous comment of a red herring
deviating from my second very trivial disposition of you being not capable of
making and bringing together a simple
[syllogism](https://en.wikipedia.org/wiki/Syllogism).  
It's your "definition" and a straw man of materialism and not mine and besides
that:  
  
1\. The material aspect of the typical human being is an 80 kg atomic
arrangement. (Known by science.)  
2\. If the material aspect of the typical human being is an 80 kg atomic
arrangement, then materialism is true. (Trivially and self-evidently true)  
3\. So, materialism is true. (From 1 and 2 by [modus
ponens](https://en.wikipedia.org/wiki/Modus_ponens))  
  
Do you like it? No?  
Then how about a "compromise"? How about the Truth with the capitol T?  
  
1\. The material aspect of the typical human being is an 80 kg atomic
arrangement. (Known by science.)  
 **2\. Materialism is true,[if and only
if](https://en.wikipedia.org/wiki/Logical_biconditional) the material aspect
of the typical human being is an 80 kg atomic arrangement. (By dogmatic belief
of physicalism)**  
3\. So, if materialism is true, then the material aspect of the typical human
being is an 80 kg atomic arrangement. (From 2 by biconditional implication)  
4\. So, if the material aspect of the typical human being is an 80 kg atomic
arrangement, then materialism is true (From 2 by biconditional implication)  
5\. So, materialism is true. (From 1 and 4 by modus ponens)  
  
Sooo...  
 _ **Which step do you dispute?**_

Alexander:  
  
Sure. "If X, then Y." in your arguments is a material conditional, which is
logically equivalent to "Not X or Y." by material implication and also
logically equivalent to "It's not, that X and not Y." by basically De Morgan's
law.  
Further some material conditionals are true as some material conditionals are
not true.  
So is your material conditional _"If materialism is true, a typical human
being is an 80 kg arrangement of atoms."_ or "Materialism is not true or a
typical human being is an 80 kg arrangement of atoms." or "It's not, that
materialism is true and a typical human being is not an 80 kg arrangement of
atoms."?!?  
Is it justified or substantiated in any given way?!?  
I don't know and I don't see that being here is the case or made to be the
case.  
  
On the other hand my material conditional "If any human is an arrangement of
atoms, then materialism must be necessarily true." is self-evidently true as
the material conditional "If a drawn quadrilateral is a square, then that
drawn quadrilateral is also a rectangle." is self-evidently true.  
  
Also where exactly is ¬Y - a typical human being is not an 80 kg arrangement
of atoms - in your argument here?  
Your premise 1 _“A typical human being has much more intrinsic value than any
80 kg arrangement of atoms.”_ doesn't appear to constitute such a claim and
statement by itself.  
Otherwise how are you exactly concluding from those two premises of yours,
that ¬X is the case - that materialism is not true?!?  
Or is your argument supposed to be not a “Denying the consequent” argument?  
If so, what kind of an argument is it then?!?

Pruss: Does the materialist at least grant that the vocabulary of atomic
physics is insufficient to say everything that can truthfully be said? For
example, statements about enzymes catalyzing particular reactions (or, worse
yet, being "life-sustaining") are not sayable in the vocabularly of atomic
physics.

Kratsch:  
  
Unless otherwise specified, or contextually required, "if ... then ..." in my
arguments is a material conditional. No claim is made that it *follows* from
materialism that a human is an 80 kg arrangement of atoms.

I think the main problem with this argument is the supposition that there can
be such a thing as "intrinsic value", and it is independent of whether
materialism is true or not. All values are by definition subjective (because
they admit no justification, empirical or otherwise). They are not something
humans come to know, but rather, humans make (conscious or subconscious)
decisions to evaluate things according to their subjective perspective,
emotions and preferences, and then project the values they generate onto
entities.

@Aron Bean Also it's kinda weird to say that there can be **no such thing** as
intrinsic value...and any intrinsic value is just us evaluating things with a
subjective perspective and projecting it to them. It's weird to think we
**actually have** the ability to conceptualise something as simple and
foundational as the idea of intrinsic value...all the while such a thing
literally can't exist. Not just doesn't, but as a whole the realm of reality
doesn't and even can't have such a thing in principle...  
  
Because if the idea is by definition subjective, then we should be aware of
this. Just as we know other subjective things as subjective, like preferences,
because we know what a preference is, and know it doesn't inhere in all things
even without knowing other persons with different preferences.  
  
Yet stangely most if not all see intrinsic value not as something in the same
category as preference, but as something found out and known in reality
itself.

Is this supposed to be a [Denying the consequent [(X→Y∧¬Y) ⇒
¬X]](https://askaphilosopher.org/2013/11/19/denying-the-consequent/) argument?  
Then where is the premise with ¬Y - a typical human being is not an 80 kg
arrangement of atoms?!?  
  
Otherwise from materialism doesn't follow a typical human being necessarily an
80 kg arrangement of atoms. But from humans being an arrangement of any amount
of atoms follows, that materialism must be necessarily true.  
So if any human is an arrangement of atoms (- yes, even little or small humans
not weighing typically 80 kg), then materialism must be necessarily true.  

Pruss: I suspect I just don't know what "Materialism" means (which I already
suspected, and now I'm more convinced). I'm not even entirely sure what
"material aspect" means in a statement like "the material aspect of a human
being is... an 80kg atomic arrangement"....  
  
Does the Materialist have to believe the following (which I'll call "M1")?  
  
 _The only accurate statements about a human being are statements that
describe the particular arrangement of atoms in question._  
  
If so, then the materialist surely cannot think that statements about, say,
enzyme activity or blood pressure or DNA transcription are true of humans
either, can she?

No, Alexander. That's basically MY justification for MY material conditional
and not yours.  
Well, my material conditional is self-evidently true, but if one has to give
an external justification for it, then because the material aspect of the
human being is scientifically known to be an 80 kg atomic arrangement,
therefore "if any human is an arrangement of atoms, then materialism must be
necessarily true".  
It is also logically equivalent to "It's NOT, that any human is an arrangement
of atoms and matter AND materialism is NOT true.", which is of course in
itself true - this should be trivially and obviously true.  
  
On the other hand your material conditional is logically equivalent to "It's
NOT, that materialism is true AND a typical human being is NOT an 80 kg
arrangement of atoms.", which is not necessarily true in the sense, that there
are of course instances/"possible worlds", where it's true, that materialism
is true AND [a typical human being is NOT an 80 kg arrangement of
atoms](https://en.wikipedia.org/wiki/Human_body_weight).  
Sooo... You are either question begging here with this material conditional
and argument of yours here and or you are straw manning materialism.  
If you want to critique materialism as being bad for an ontological dogmatic
description or explanation of reality, then please, critique it properly. But
also please don't straw man it and don't question beg in this bad way with
that material conditional of yours.  
If "any" person should know it by now, how to do this properly, then you,
Alexander, should know this by now.  
  
Apropos knowing things. I still don't know, which of your premises
constitutes, that "a typical human being is NOT an 80 kg arrangement of
atoms'', such that you could validly conclude with that dubious material
conditional of yours and by a "denying the consequent" argument, that
materialism is not true.  
So which premise of those two premises of yours here constitutes such a
claim?!?

A typical human being has much more intrinsic value than any 80 kg arrangement
of atoms.

what about emergent properties? ex. atoms, neurons, Brians, consciousness.
Similar to other parts for what makes a human being

I think materialism is false, and I’m not sure what to think about premise 1.
What kind of value does my body (the arrangement of atoms) have on a
hylomorphic view? Does it mainly have instrumental value? In general, if W is
a whole with intrinsic value, do its parts (the xs) mainly have instrumental
value, since they are for the sake of W? Or does the intrinsic value of the
whole bleed into all of the parts?

P1 seems to beg the question, no? If materialism (which I'm taking to mean
"objects have no parts in addition to their material/atomic parts") is true,
then P1 would be like saying "a typical human being has much more intrinsic
value than any human-sized object"....

Alexander R Pruss: "there is no objective reason to do or believe anything"  
  
You cannot believe anything at will. Nor are your choices to do something are
ever free, even if they seem to be so. All your beliefs and choices to act are
predetermined by your genetic makeup and past experiences, which express
themselves in the current emotional/cognitive (that is, biochemical) state of
your brain. This hypothesis is simpler than to suppose some immaterial stuff,
whose interaction with material things, including the brain, would be beyond
comprehension.  
Also note that being rational is not somehow "inherently better" than being
irrational or arational. Rather, it's just that rationality is the kind of
attitude that pays off most of the time in the long run. And even this
"paying-off" translates to things that support the survival of the individual,
so it can be expressed in value-neutral terms.  
  
"every reason expresses the value of the thing it is a reason for"  
This is false.  
For example, if you know that p, and also know that "if p, then q", then this
knowledge, together with knowledge of the rule of inference "modus ponens" may
be a reason for you to believe (and, also know) that q. But there is nothing
that expresses the "value of q" in the state of knowing p, "if p, then q", or
in knowing "modus ponens", nor in its application. They are just propositions,
syntactic structures with semantic interpretation according to classical
logic. And there are many other alternative systems of logic, even some where
modus ponens is not a theorem.

@Aron Bean Isn't that just begging the question that values can't be
"justified"? In what sense - just because it's not empirical doesn't mean
there can't be other forms of justification. How do we rule out other forms of
justification? And even if one can't justify this universally to others, that
doesn't mean the justification isn't true. For example, one knows one's own
existence immediately and uniquely through one's personal self-awareness, yet
this type of justification isn't subjective and unjustifiable just because
it's inherently inaccessible to others.

Actually, if we throw relativity into the mix, then we can get an even closer
analogy, assuming still that a magnet’s field is an accident of the magnet.
Imagine that the magnet is annihilated. The magnetic field disappears, but
gradually, starting near the magnet, because all effects propagate at most at
the speed of light. Thus, even when the magnet is destroyed, for a short
period its magnetic field still exists.

But where in space is the substance of the magnet? I would have thought that
it is where it acts, and since the magnetic field is how the magnet acts qua
magnet, it could not extend beyond the substance. The reason it feels like the
magnetic field extends beyond the magnet is that it acts in more than one way
(in the way it acts on the hand when it is held and in the way it acts on
magnetic metals).

Here is an analogy that occurred to me. Consider a magnet. It’s not crazy to
think of the magnet’s magnetic field as an accident of the magnet. But the
magnetic field extends spatially beyond the magnet. Thus, it exists in places
where the magnet does not.

Imagine that there is only one substance in the world, a magnet. It has a
magnetic field extending around it, beyond the magnet, no?  
  
(Of course, one could count the magnetic field as a separate substance. But
it's not clear that that's the right view.)

Now, according to four-dimensionalism, time is rather like space. If so, then
an accident existing _when its substance does not_ is rather like an accident
existing _where its substance does not_. Hence to the four-dimensionalist, the
magnet analogy should be quite helpful.

On Thomistic accounts of transsubstantiation, the accidents of bread and wine
continue to exist even when the substance no longer does (having been turned
into the substance of Christ’s body and blood). This seems problematic.

No, I don’t think so. I mean, I think that in most contexts, it would be fine
to _say_ that there is a single, cube-shaped lodestone (1×1×1) and a field
that extends beyond it. But I have real doubts that this way of talking works
here.  
  
Here is an analogy. The question of where I am could be taken to be asking
either where my principal activity takes place or else where all of my
activities take place. According to the first way, I am where my brain is.
According to the second way, I am where my entire body is. But it is the
latter, holistic answer, that is more fundamental.  
On this analogy, the cube-shaped lodestone is like the brain; its magnetic
field is like the whole body. The substance as a whole has one small,
localized activity and another activity that is more spread out. The answer to
the question, “Where is the magnet?” depends on which activity you have in
mind, but the more fundamental version of the question concerns all of its
activities rather than some of its limited, localized activities.  
  
What am I missing? How else does a substance get its ‘where’?

That said, I don’t know if the magnet’s field is an accident of it. (Rob Koons
in conversation suggested it might be.) But it’s comprehensible to think of it
as such, and hence the analogy makes Thomistic transsubtantiaton
comprehensible, I think.

So, you just gained the belief in the conjunction of _p_ and _q_. (By (5) and
(7))

Before you gained the belief _p_ you didn’t believe the conjunction of _p_ and
_q_. (By (4))

I agree with your eternalism; still, I think this particular worry might
remain. It seems that the idea of a substance's accidents existing at a time
at which the substance itself does not exist pushes against the very same
intuitions that might have bothered us to begin with. In other words, it seems
that the problem can be restated in eternalist terms.  
  
Thanks for the two points on consubstantiation; they're both interesting, and
I'll have to give them more thought.

I don't know that it does. The main worry is that accidents depend on their
substance. But dependence can be cross-temporal, at least if eternalism is
true. See today's post, too.

I am an eternalist. I think it's OK for the accidents of a substance to be
located at a time where the substance does not exist. In one sense this isn't,
however, a case of the accidents existing without the substance, because when
the accidents exist presently, the substance exists, too, albeit pastly.  
  
As for consubstantiation, here are some worries.  
  
1\. When Jesus says "This is my body", it seems like he is pointing to the
visible thing, namely bread. If there is bread there, then he is pointing at
the bread. Thus, if there is bread there, he is making the incorrect or at
least non-literal statement that that thing, the bread, is his body. But the
Tradition likes to take Jesus's words here literally.  
  
2\. According to Scripture and Tradition, we eat Christ's body and drink his
blood. But on a consubstantiation view, it's not clear that this is the right
way to describe it. It seems that what we really eat and drink is the co-
present bread and wine, and the body and blood just happens to come along with
it. Here's my image of consubstantiation. Suppose that magnetic fields are
substances. Then where there is a magnet, we have something like
consubstantiation: there are two substances, a magnet and a magnetic field, in
the same place (I am talking of the magnetic field inside the magnet, not the
one that extends outside of it). But suppose now you foolishly eat the magnet
(DON'T DO IT; children have died from eating two magnets and having them pinch
through intenstines). I don't think it's correct to say that you eat both
substances. It seems that what you eat is the magnet, and the magnetic field
comes along for the ride.

Hi Professor Pruss. My apologies if this is off topic, but I wanted to ask a
question about the Eucharist. Specifically, what do you think of the claim
that it is metaphysically impossible for accidents to exist in the absence of
the substance of which they are accidents? Also, do you have any particular
objections to the consubstantiation view common among Anglo-Catholics
(including myself)?

Abstract: Scoring rules measure the accuracy or epistemic utility of a
credence assignment. A significant literature uses plausible conditions on
scoring rules on finite sample spaces to argue for both probabilism—the
doctrine that credences ought to satisfy the axioms of probabilism—and for the
optimality of Bayesian update as a response to evidence. I prove a number of
formal results regarding scoring rules on infinite sample spaces that impact
the extension of these arguments to infinite sample spaces. A common condition
in the arguments for probabilism and Bayesian update is strict propriety: that
according to each probabilistic credence, the expected accuracy of any other
credence is worse. Much of the discussion needs to divide depending on whether
we require finite or countable additivity of our probabilities. I show that in
a number of natural infinite finitely additive cases, there simply do not
exist strictly proper scoring rules, and the prospects for arguments for
probabilism and Bayesian update are limited. In many natural infinite
countably additive cases, on the other hand, there do exist strictly proper
scoring rules that are continuous on the probabilities, and which support
arguments for Bayesian update, but which do not support arguments for
probabilism. There may be more hope for accuracy-based arguments if we drop
the assumption that scores are extended-real-valued. I sketch a framework for
scoring rules whose values are nets of extended reals, and show the existence
of a strictly proper net-valued scoring rules in all infinite cases, both for
f.a. and c.a. probabilities. These can be used in an argument for Bayesian
update, but it is not at present known what is to be said about probabilism in
this case.

Comments from a user egregiously failing in the civility required in academic
discussion have been deleted and the user has been banned. My responses to
these comments have been deleted as well out of fairness to the user. I
should, however, note for the sake of anybody who read my comments that in one
of my comments I incorrectedly stated that the logarithmic score is not
additive, and the user was right to call me out on it, but did so in a manner
that was uncivil, and failures of civility are not tolerated.  
  
(Specifically, for a subset A of Omega, let s_A(c,t)=0 unless A is a singleton
and t=1. Then let s_{w}(c,1)=log c({w}). Then the logarithmic score of c is
the sum of s_A(c,1_A(w)) as A ranges over the subsets of Omega, and hence is
additive in my sense. I was, however, correct that the logarithmic score is
not strictly proper when we allow non-probability credences, since it ony
depends on the credences at singletons.)

But a different kind of situation comes up for choices of a point on a
spectrum. For instance, suppose I am deciding how much homework to assign, how
hard a question to ask on an exam, or how long a walk to go for. What is going
on there?

Well, here is a model that applies to a number of cases. There are two
incommensurable goods one better served as one goes in one direction in the
spectrum and the other better served as one goes in the other direction in the
spectrum. Let’s say that we can quantify the spectrum as one from less to more
with respect to some quantity _Q_ (amount of homework, difficulty of a
question or length of a walk), and good _A_ is promoted by less of _Q_ and
incommensurable good _B_ is promoted by more of _Q_. For instance, with
homework, _A_ is the student’s having time for other classes and for non-
academic pursuits and _B_ is the student’s learning more about the subject at
hand. With exam difficulty, _A_ may be avoiding frustration and _B_ is giving
a worthy challenge. With a walk, _A_ is reducing fatigue and _B_ is increasing
health benefits. (Note that the claim that _A_ is promoted by less _Q_ and _B_
is promoted by more _Q_ may only be correct within a certain range of _Q_. A
walk that is too long leads to injury rather than health.)

Here is a variant suggestion. Partition the set of options into two ranges _R_
1, consisting of options where _Q_ < _Q_ 1 and _R_ 2, where _Q_ > _Q_ 1. Why
did I choose _Q_ = _Q_ 1? Well, I chose _Q_ over all the choices in _R_ 1
because _Q_ better promotes _B_ than anything in _R_ 1, and I chose _Q_ over
all the choices in _R_ 2 because _Q_ better promotes _A_ than anything in _R_
1.

Note that nothing in the above explanatory stories requires any commitment to
there being some sort of third good, a good of balance or compromise between
_A_ and _B_. There is no commitment to _Q_ 1 being the best way to position
_Q_.

My usual story about how to reconcile libertarianism with the Principle of
Sufficient Reason is that when we choose, we choose on the basis of
incommensurable reasons, some of which favor the choice we made and others
favor other choices. Moreover, this is a kind of constrastive explanation.

Here is one suggestion. Take the choice to make _Q_ equal to _Q_ 1 to be the
conjunction of two (implicit?) choices:

On both approaches, the apparent inconsistency of citing opposed goods
disappears because they are cited to explain different contrasts.

This story, though it has some difficulties, is designed for choices between
options that promote significantly different goods—say, whether to read a book
or go for a walk or write a paper.

So, now, suppose we choose _Q_ = _Q_ 1. Why did one choose that? It is odd to
say that one chose _Q_ on account of reasons _A_ and _B_ that are opposed to
each other—that sounds inconsistent.

Now, we can explain choice (a) in terms of (a) serving good _A_ better than
the alternative, which would be to make _Q_ be bigger than _Q_ 1. And we can
explain (b) in terms of (b) serving good _B_ better than the alternative of
making _Q_ be smaller.

I like to illustrate the evidential force of simplicity by noting that for
about two hundred years people justifiably believed that the force of gravity
was _G_ _m_ 1 _m_ 2/ _r_ 2 even though _G_ _m_ 1 _m_ 2/ _r_ 2 + _ϵ_ fit the
observational data better if a small enough but non-zero _ϵ_. A minor point
about this struck me yesterday. There is doubtless some _p_ ≠ 2 such that _G_
_m_ 1 _m_ 2/ _r_ _p_ would have fit the observational data _better_. For in
general when you make sufficiently high precision measurements, you never find
_exactly_ the correct value. So if someone bothered to collate all the
observational data and figure out exactly which _p_ is the best fit (e.g.,
which one is exactly in the middle of the normal distribution that best fits
all the observations), the chance that that number would be 2 up to the
requisite number of significant figures would be vanishingly small, even if
_in fact_ the true value is _p_ = 2. So simplicity is not merely a tie-
breaker.

Note that our preference for simplicity here is actually infinite. For if we
were to collate the data, there would not just be _one_ real number that fits
the data better than 2 does, but a _range_ _J_ of real numbers that fits the
data better than 2. And _J_ contains uncountably many real numbers. Yet we
rightly think that 2 is more likely than the claim that the true exponent is
in _J_ , so 2 must be infinitely more likely than most of the numbers in _J_.

Ought implies can. Most people can’t do Bayesian reasoning correctly. So
Bayesian reasoning is not how they ought to reason. In particular, a reduction
of epistemic ought to the kinds of probability fcts that are involved in
Bayesian reasoning fails.

Could it be that the epistemic responsibility is to "go where the evidence
points", and that Bayesianism is just the most rigorous form of that? It would
be like saying that we ought to measure carefully when cutting the pieces to
build someone's house, but that we can only do as well as our available
instruments let us, and that that is sufficient. Bayesian reasoning as such
may not be an "available instrument" for most of us, but we ought to
approximate it as much as we can.  
  
I think Steven Pinker just wrote a book in which he equates rationality with
something like Bayesianism. I haven't read it yet, but it's on my list!

But if "Meadow's law" is no longer a law, then why are so many theists and
apologists so fond of [Plantinga's poker
analogy](https://www.youtube.com/watch?v=KDBkmpm-APE&t=757s)?!?  
I guess, because even though they can not correctly and properly Bayesian
reason and therefore shouldn't do it, they are doing it regardless of the
possibility of them being so irrational with it.  

Even if you acquire the basic mathematical skills, keeping track of
probabilities and conditionalizing on all the evidence is simply beyond our
capabilities. I am constantly receiving vast amounts of data. I just can't
conditionalize on it. All I can do is to pick out some small subset of the
data that seems relevant, and conditionalize on that. Take the lab scientist
who sees an instrument display "3.445". Maybe, though even that is a stretch,
they can conditionalize on the instrument displaying "3.445". But that's such
a small part of their evidence: there is, for instance, the particular pattern
of lights and shadows playing over the instrument display, the flow of air
from the vents, etc. Sure, one normally approximates by assuming all that
other stuff is independent of what one cares about in the experiment. But the
fact remains that one is failing to conditionalize on all one's data.

However, it is implausible to think that we humans ought to do something that
nobody has been able to do until recently and even now only a few can do, and
only in limited cases, even if the something is involuntary.

I suppose the main worry with this argument is that perhaps only an ought
governing voluntary activity implies can. But the epistemic life is in large
part involuntary. An eye ought to transmit visual information, but some eyes
cannot—and that is not a problem because seeing is involuntary.

Maybe the relevant ought facts are like this: Even if we can't reason in
Bayesian way, we can acquire that ability, and we ought to. So we ought to do
something such that, if we do it, then we ought to reason in a Bayesian way.  
  
I can imagine someone saying something like, if we ought to phi, and phi-ing
implies that we ought to psi, then we ought psi.  
  
Consider something as plain as it being the case that I ought to place the
item on the shelf (I promised to, or I work at a grocery store). But I can't,
since I haven't picked up the item, and so how can I place the item on the
shelf? Clearly, we say that I can place the item, because I can pick it up
first.  
  
I realise as I type that this example isn't exactly what I started with, so
maybe this example illustrates the following principle:  
  
Principle: If you can and ought do something X, such that by doing X, you can
do something Y, and the ability to do Y is sufficient for it being the case
that you _ought_ to do Y, then you _can_ do Y and you ought to do Y.  
  
In the item-shelving case, X is pick up the item and Y is place the item on
the shelf. In the epistemology case, X is acquire Bayesian reasoning skills
and Y is reasoning in a Bayesian way. Of course, this only applies to those
who can learn, which is probably most adults.  
  
I have no idea how plausibly I find this. I'm just playing around with
possibilities.

I agree and not just people not capable of correctly and properly Bayesian
reasoning shouldn't Bayesian reason, but also people not capable of correctly
and properly estimating things and reasoning in general should not estimate
things and reason in general.  
Just look at what happend in Sally Clark's case:  
["Making A Math Murderer" by Vsauce2](https://youtu.be/mLEWj-61a4I)  
I guess, that for good reasons ["Meadow's
law"](https://en.wikipedia.org/wiki/Meadow%27s_law) is no longer a law.  

If Bayesian reasoning isn’t how we ought to reason, what’s the point of it? I
am inclined to think it is a useful tool for figuring out the truth in those
particular cases to which it is well suited. There are different tools for
reasoning in different situations.

I think you shouldn’t redirect and you should brake. There is something
morally obnoxious about certainly causing death for a highly uncertain benefit
_when the expected values are close_. This complicates the proportionality
condition in the Principle of Double Effect even more, and provides further
evidence against expected-value utilitarianism.

Or suppose you are driving a fire truck to a place where five people are about
to die in a fire, and you know that you have a 1/4 chance of putting out the
fire and saving them if you get there in time. Moreover, there is a person
sleeping on the road in front of the only road to the fire, and if you stop to
remove the person from the road, it will be too late for the five. Do you
brake? Expected utilities:  − 5 lives for braking and  − 1 − 3.75 = − 4.75
lives for continuing to the fire and running over the person on the road.

Suppose a trolley is heading towards five people, and you can redirect it
towards one. But the trolley needs to go up a hill before it can roll down it
to hit the five people, and your best estimate of its probability of making it
up the hill is 1/4. On the other hand, if you redirect it, it’s a straight
path to the one person, who is certain to be killed. Do you redirect? Expected
utilities:  − 1.25 lives for not redirecting and  − 1 lives for redirecting.

Here I want to make a minor observation. The fact that the SLLN applies to
some sequence of independent random variables is itself not sufficient to make
it rational to bet in each case according to the expectations in an infinite
run. Let _X_ _n_ be 2 _n_ / _n_ with probability 1/2 _n_ and − 1/(2 _n_ ) with
probability 1 − 1/2 _n_. Then

In discussions of maximization of expected value, the Law of Large Numbers is
sometimes invoked, at times—especially by me—off-handedly. According to the
Strong Law of Large Numbers (SLLN), if you have an infinite sequence of
independent random variables _X_ 1, _X_ 2, ... satisfying some conditions
(e.g., in the Kolmogorov version ∑ _n_ ( _σ_ _n_ 2/ _n_ 2) < ∞, where _σ_ _n_
2 is the variance of _X_ _n_ ), then with probability one, the average of the
random variables converges to the average of the mathematical expectations of
the random variables. The thought is that in that case, if the expectation of
each _X_ _n_ is positive, it is rationally required to accept the bet
represented by _X_ _n_.

I still feel that the fact that in my examples, almost surely, at some
*finite* point in time the expected utility non-maximizer overtakes the
expected utility maximizer, and after that the gap just increases, seems
significant. But I can't put my finger on what exactly is significant about
it.

Yes, I’d say that, if, in the case of nth expectation greater than c>0,
someone takes SLLN (if it applies) as a reason to accept all the bets, then in
your example they should refuse to accept all the bets – if they reason on the
basis of a ‘with probability 1’ result in one case, they should also do so in
the other. That said, you should take care to note exactly what the various
authors are actually arguing.  
  
Speaking for myself, I don’t think that any result about an actual infinity of
bets, or even just about limits of finite sequences of bets, is in itself a
good reason to do anything. (Though, of course, such results can give useful
hints.) What matters is the likely position when the game ends, as, in the
real world, it must.  
  
In your example, the distribution of partial sums has progressively increasing
variance and skewness. Roughly (if I’m thinking straight), variance of the nth
partial sum grows like n, 3rd moment grows like (n^2)/2. The normalized 3rd
moment (i.e. with the outcome divided by its s.d. to make the variance 1)
grows like (n^(1/2))/2. If I were really offered this sequence of bets, with
the option of choosing in advance how many to accept, I’d feel that for large
n, things would get pretty hairy, way too hairy to justify accepting on the
basis of the positive expectation, which only grows like ln(n)/2. So I’d
choose a smallish n I felt comfortable with.

Now, just as in my previous post, almost surely (i.e., with probability one)
only finitely many of the bets _X_ _n_ will have the positive payoff. Thus,
with a finite number of exceptions, our sequence of payoffs will be the
sequence  − 1/2, − 1/4, − 1/6, − 1/8, .... Therefore, almost surely, the
average of the first _n_ payoffs converges to zero. Moreover, the average of
the first _n_ mathematical expectations converges to zero. Hence the variables
_X_ 1, _X_ 2, ... satisfy the Strong Law of Large Numbers. But what is the
infinite run payoff of accepting all the bets? Well, given that almost surely
there are only a finite number of _n_ such that the payoff of bet _n_ is not
of the form − 1/(2 _n_ ), it follows that almost surely the infinite run
payoff differs by a finite amount from  − 1/2 − 1/4 − 1/6 − 1/8 = − ∞. Thus
the infinite run payoff is negative infinity, a disaster.

In [a recent post](http://alexanderpruss.blogspot.com/2022/10/expected-
utility-maximization.html), showed how in some cases where the Strong Law of
Large Numbers is not met, in an infinite run it can be disastrous to bet in
each case according to expected value.

In [an earlier post](https://alexanderpruss.blogspot.com/2011/11/attitudes-to-
risk-and-law-of-large.html), I suggested that perhaps the Central Limit
Theorem (CLT) rather than the Law of Large Numbers is what one should use to
justify betting according to expected utilities. If the variables _X_ 1, _X_
2, ... satisfy the conditions of the CLT, and have non-negative expectations,
then _P_ ( _X_ 1+...+ _X_ _n_ ≥0) will eventually exceed any number less than
1/2. In particular, we won’t have the kind of disastrous situation where the
overall payoffs almost surely go negative, and so no example like my above one
can satisfy the conditions of the CLT.

_E_ _X_ _n_ = (1/2 _n_ )(2 _n_ / _n_ ) − 1/(2 _n_ )(1−1/2 _n_ ) = (1/ _n_
)(1−(1/2)(1−1/2 _n_ )).

The last example (with a_n = 1/n^2) is very neat. I had been trying to think
of something similar. :-)  
  
The Peköz paper I mentioned in the other post has, in addition to the variance
condition [sum of (nth variance/n^2 finite)], the condition that all the
individual expectations are greater than some strictly positive constant. In
the example, this is violated - the nth expectation is about 1/n. So again,
there’s no formal contradiction. Of course, this is no surprize.

Hence even when the SLLN applies, we can have cases where almost surely there
are only finitely many positive payments, infinitely many negative ones, and
the negative ones add up to  − ∞.

Clearly _E_ _X_ _n_ > 0. So in individual decisions based on expected value,
each _X_ _n_ will be a required bet.

In the above example, while the variables satisfy the SLLN, they do not
satisfy the conditions for the Kolmogorov version of the SLLN: the variances
grows exponentially. It is somewhat interesting to ask if the variance
condition in the Kolmogorov Law is enough to prevent this pathology. It’s not.
Generalize my example by supposing that _a_ 1, _a_ 2, ... is a sequence of
numbers strictly between 0 and 1 with finite sum. Let _X_ _n_ be 1/( _n_ _a_
_n_ ) with probability _a_ _n_ and  − 1/(2 _n_ ) with probability 1 − _a_ _n_.
As before, the expected value is positive, and by Borel-Cantelli (given that
the sum of the _a_ _n_ is finite) almost surely the payoffs are  − 1/(2 _n_ )
with finitely many exceptions, and hence the there is a finite positive payoff
and an infinite negative one in the infinite run.

If you assume that the nth expectation is bigger than c>0, and the Strong Law
of Large Numbers applies, then of course almost surely the person who accepts
all the bets will eventually be better off than the person who rejects all the
bets, and the difference between the two will grow without bound. And the
variance condition is sufficient for the Strong Law.  
  
Do you think this is true: If someone thinks the above result is a good reason
to accept rather than reject all the bets, then they should also think that in
my case we have good reason to reject rather than accept all the bets?

But the variance _σ_ _n_ 2 is less than _a_ _n_ /( _n_ _a_ _n_ )2 \+ 1 = (1/(
_n_ 2 _a_ _n_ )) + 1. If we let _a_ _n_ = 1/ _n_ 2 (the sum of these is
finite), then each variance is at most 2, and so the conditions of the
Kolmogorov version of the SLLN are satisfied.

Nonetheless, even though you know the statement is wrong, it raises the
probability that the textbook’s mass is 1.496854821 kg (to ten significant
figures). For while most of the digits are garbage, the first couple are
likely close. Before you you heard the student’s statement, you might have
estimated the mass as somewhere between one and two kilograms. Now you
estimate it as between 1.45 and 1.55 kg, say. That raises the probability that
in fact, up to ten significant figures, the mass is 1.496854821 kg by about a
factor of ten.

Alex  
  
If the first digits are likely close, what the student says is not completely
wrong. It is inaccurate.  
The reason you learn something from it is because you already have knowledge
about the book's probable weight. Suppose you ask me about the distance
between the earth and the moon and I say it's 400 000 km. That's wrong, but it
is a better answer than, say, 4 million km. Suppose that before you asked me,
you estimated the distance as between 100 000 and 1 million, then now you
estimate it between 300 000 and 500 000.  
The reason you can estimate it is becasue you have a certain confindence in my
claims. Even though you know I cannot have measured the distance accurately
enough to know it's 400 000 km, you are still confident that I at least know
something about it.  
Now compare this to a situation in which I simply tell you a random distance.
The only thing you learn from this is that I am not capable of or willing to
make a genuine effort.  

Even if you know that I am telling you a random distance, you still learn from
it. For your knowledge of such facts as that I am telling you a random
distance is never certain. It may look like I'm just making it up at random,
but there is a chance that my statement is guided by the truth. (There is also
a chance that my statement is guided by falsehood. But I think that, absent
special evidence about me having reason to positively deceive you, that chance
is smaller.) Or I may be filtering particularly ridiculous random answers
(e.g., you ask me how many miles it is from Waco to Los Angeles, and I google
"random number", and get 4; but that's too ridiculous, so I just say 100).

On reflection, the phenomenon in the first sentence of the post isn't odd at
all. Typically if someone tells you something, that is evidence for what they
tell you, even if you know it's not true.

So, you know that what the student says is false, but your credence in the
content has just gone up by a factor of ten.

Alex  
  
Suppose you want to go to a place P. There is only one road that leads to P,
namely the road to the right.  
Now you ask me which way you should go and I tell you that the left road is
the correct one, which is the wrong way.  
What do you learn from this?

Of course, some people will want to turn this story into an argument that you
don’t know that the student’s statement is wrong. My preference is just to
make this statement another example of why _knowledge_ is an unhelpful
category.

Here’s an odd phenomenon. Someone tells you something. You know it’s false,
but their telling it to you raises the probability of it.

For instance, suppose at the beginning of a science class you are
teachingabout your studnts about significant figures, and you ask a student to
tell you the mass of a textbook in kilograms. They put it on a scale
calibrated in pounds, look up on the internet that a pound is exactly
0.45359237 kg, and report that the mass of the object is 1.496854821 kg.

Now, you know that the classroom scale is not accurate to ten significant
figures. The chance that the student’s measurement was right to ten
significant figures is tiny. You _know_ that the student’s statement is wrong,
assuming that it _is_ in fact wrong.

My claim was only that typically you learn something in favor of p by being
told that p is true. There are, of course, exceptions (e.g., if you know that
someone is going to be lying, in which case their saying something is evidence
that they disbelieve it, which in turn is evidence against it).  
  
That said, that an intelligent person believes a clear and explicit
contradiction may be some very slight evidence against the law of
noncontradiction.

Can we rigorously model this kind of an epistemic value assignment? I think
so. Consider the following discontinuous accuracy scoring rule _s_ 1( _x_ ,
_t_ ), where _x_ is a probability and _t_ is a 0 or 1 truth value:

_s_ 1( _x_ , _t_ ) = − _b_ if _r_ < _x_ and _t_ = 0 or if _x_ < 1 − _r_ and
_t_ = 1.

If we think that it’s never rational for a rational agent to refuse free
information, then the above argument can be made rigorous to establish that
any discontinuous rise in the epistemic value of credence at the point at
which knowledge of a truth is reached is exactly mirrored by a discontinuous
fall in the epistemic value of a state of credence where seeming-knowledge of
a falsehood is reached. Moreover, the rise and the fall must be in the ratio 1
− _r_ : _r_ where _r_ is the knowledge threshold. Note that for knowledge, _r_
is plausibly pretty large, around 0.95 at least, and so the ratio between the
special value of knowledge of a truth and the special disvalue of evidence-
sufficient-for-knowledge for a falsehood will need to be at most 1:19. This
kind of a ratio seems intuitively implausible to me. It seems unlikely that
the special disvalue of evidence-sufficient-for-knowledge of a falsehood is an
order of magnitude greater than the special value of knowledge. This
contributes to my scepticism that there is a special value of knowledge.

But that’s not quite right. For from Alice’s point of view, because the
threshold for knowledge is not 1, there is a real possibility that _p_ is
false. But it may be that just as there is a discontinuous gain in epistemic
value when your (rational) credence becomes sufficient for knowledge of
something that is in fact true, it may be that there is a discontinuous loss
of epistemic value when your credence becomes sufficient for knowledge of
something false. (Of course, you can’t know anything false, but you can have
evidence-sufficient-for-knowledge with respect to something false.) This is
not implausible, and given this, by looking at the information, by her lights
Alice also has a chance of a significant gain in value due to losing the
illusion of knowledge in something false.

Plausibly, then, if we imagine Alice has some evidence for a truth _p_ that is
insufficient for knowledge, and slowly and continuously her evidence for _p_
mounts up, when the evidence has crossed the threshold needed for knowledge,
the value of Alice’s state with respect to _p_ will have suddenly and
discontinuously increased.

Suppose there is a distinctive and significant value to knowledge. What I mean
by that is that if two epistemic are very similar in terms of truth, the level
and type of justification, the subject matter and its relevant to life, the
degree of belief, etc., but one is knowledge and the other is not, then the
one that is knowledge has a significantly higher value because it is
knowledge.

This hypothesis initially seemed to me to have an unfortunate consequence.
Suppose Alice has just barely exceeded the threshold for knowledge of _p_ ,
and she is offered a cost-free piece of information that may turn out to
slightly increase or slightly decrease her overall evidence with respect to
_p_ , where the decrease would be sufficient to lose her knowledge of _p_
(since she has only “barely” exceeded the evidential threshold for knowledge).
It seems that Alice should refuse to look at the information, since the
benefit of a slight improvement in credence if the evidence is non-misleading
is outweighed by the danger of a significant and discontinuous loss of value
due to loss of knowledge.

_s_ 1( _x_ , _t_ ) = _a_ if _r_ < _x_ and _t_ = 1 or if _x_ < 1 − _r_ and _t_
= 0

_s_ 1( _x_ , _t_ ) = 0 if 1 − _r_ ≤ _x_ ≤ _r_

Suppose that _a_ and _b_ are positive and _a_ / _b_ = (1− _r_ )/ _r_. Then if
my scribbled notes are correct, it is straightforward but annoying to check
that _s_ 1 is proper, and it has a discontinuous reward _a_ for meeting
threshold _r_ with respect to a truth and a discontinuous penalty  − _a_ for
meeting threshold _r_ with respect to a falsehood. To get a strictly proper
scoring rule, just add to it any strictly proper continous accuracy scoring
rule (e.g., Brier).

Let _α_ > 0 be the “squishiness” of our credences. Let’s say that for one
credence to be definitely bigger than another, their difference has to be at
least _α_ , and that to definitely meet (fail to meet) a threshold, we must be
at least _α_ above (below) it. We assume that our threshold _r_ is definitely
less than one: _r_ \+ _α_ ≤ 1.

(We definitely won’t meet _r_ on a negative test result.) Thus to get (c)
definitely true, we need (3) to hold as well as the probability of a positive
test result to be at least _r_ \+ _α_ :

_P_ ( _p_ | _E_ ) = _P_ ( _p_ )/ _P_ ( _E_ ) = ( _r_ − _α_ )/( _r_ − _α_ + _β_
(1−( _r_ − _α_ ))).

We thus have our joint constraint on the squishiness of our credences: bad
things happen if our credences are so precise as to make (6) true with respect
to a threshold _r_ for which we don’t want (a)–(c) to definitely hold. The
easiest scenario for making (a)–(c) definitely hold will be a binary test with
no false negatives.

We thus have our joint constraint on the squishiness of our credences: bad
things happen if our credences have a level of precision equal to the right-
hand-side of (6). What exactly that says about _α_ depends on where the
relevant threshold lies. If the threshold _r_ is 1/2, the squishiness _α_ is
0.15. That’s surely higher than the actual squishiness of our credences. So if
we are concerned merely with the threshold being more-likely-than-not, then we
can’t avoid the paradox, because there will be cases where our credence is
definitely below the threshold and it’s definitely above the threshold that
examination of the evidence will push us about the threshold.

What does this tell us about _r_ and _α_? We can actually figure this out.
Consider a test for _p_ that have no false negatives, but has a false positive
rate of _β_. Let _E_ be a positive test result. Our best bet to generating a
counterexample to (a)–(c) will be if the priors for _p_ are as close to _r_ as
possible while yet definitely below, i.e., if the priors for _p_ are _r_ −
_α_. For making the priors be that makes (c) easier to definitely satisfy
while keeping (b) definitely satisfied. Since there are no false negatives,
the posterior for _p_ will be:

Let _z_ = _r_ − _α_ \+ _β_ (1−( _r_ − _α_ )) = (1− _β_ )( _r_ − _α_ ) + _β_.
This is the prior probability of a positive test result. We will definitely
meet _r_ on a positive test result just in case we have ( _r_ − _α_ )/ _z_ =
_P_ ( _p_ | _E_ ) ≥ _r_ \+ _α_ , i.e., just in case

Note that by appropriate choice of _β_ , we can make _z_ be anything between
_r_ − _α_ and 1, and the right-hand-side of (3) is at least _r_ − _α_ since
_r_ \+ _α_ ≤ 1. Thus we can make (c) definitely hold as long as the right-
hand-side of (3) is bigger than or equal to the right-hand-side of (4), i.e.,
if and only if:

In [a recent post](http://alexanderpruss.blogspot.com/2023/01/knowing-you-
will-soon-have-enough.html), I noted that it is possible to cook up a Bayesian
setup where you don’t meet some threshold, say for belief or knowledge, with
respect to some proposition, but you do meet the same threshold with respect
to the claim that after you examine a piece of evidence, then you _will_ meet
the threshold. This is counterintuitive: it seems to imply that you can know
that you will have enough evidence to know something even though you don’t
yet. In a comment, Ian noted that one way out of this is to say that beliefs
do not correspond to sharp credences. It then occurred to me that one could
use the setup to probe the question of how sharp our credences are and what
the thresholds for things like belief and knowledge are, perhaps
complementarily to the considerations in [this
paper](https://philarchive.org/rec/PRUBSA).

It’s in fact not hard to see that (6) is necessary and sufficient for the
existence of a case where (a)–(c) definitely hold.

Note that the squishiness of our credences likely varies with where the
credences lie on the line from 0 to 1, i.e., varies with respect to the
relevant threshold. For we _can_ tell the difference between 0.999 and 1.000,
but we probably can’t tell the difference between 0.700 and 0.701. So the
squishiness should probably be counted relative to the threshold. Or perhaps
it should be correlated to log-odds. But I need to get to looking at grad
admissions files now.

For suppose we have a credence threshold _r_ and that our intuitions agree
that we can’t have a situation where:

we meet _r_ with respect to the proposition that we will meet the threshold
with respect to _p_ after we examine evidence _E_.

But what’s a reasonable threshold for belief? Maybe something like 0.9 or
0.95. At _r_ = 0.9, the squishiness needed for paradox is _α_ = 0.046. I
suspect our credences are more precise than that. If we agree that the
squishiness of our credences is less than 4.6%, then we have an argument that
the threshold for belief is _more_ than 0.9. On the other hand, at _r_ = 0.95,
the squishiness needed for paradox is 2.4%. At this point, it becomes more
plausible that our credences lack that kind of precision, but it’s not clear.
At _r_ = 0.98, the squishiness needed for paradox dips below 1%. Depending on
how precise we think our credences are, we get an argument that the threshold
for belief is something like 0.95 or 0.98.

In cases where doing wrong and suffering wrong are of roughly the same order
of magnitude, it is very intuitive that we should prevent the suffering of
wrong rather than the doing of wrong. Imagine that Alice is drowning while at
the same time Bob is getting ready to assassinate a politician, but we know
for sure that Bob’s bullets have all been replaced with blanks. If our choice
is whether to try to dissuade Bob from attempting murder or keep Alice from
drowning, we should keep Alice from drowning, evne if on the Socratic view the
harm to Bob from attempting murder will be greater than that to Alice from
drowning. (I am assuming that in this case the two harms are nonetheless of
something like the same order of magnitude.)

A reasonable optimism says that in most cases most people’s consciences are
correct. Thus typically we would expect that most violators of a legitimate
law will not be acting out of conscience—for a necessarily condition for the
legitimacy of a law is that it does not conflict with a correct conscience.
Thus, even if there is the rare murderer acting from mistaken conscience, most
murderers act against conscience, and by incentivizing abstention from murder,
in most cases the law _helps_ people follow their conscience, and the small
number of other cases can be tolerated as a side effect. Thus the
considerations of conscience favor intolerant laws in such cases. Nonetheless,
there are cases where most violators of a law would likely be acting from
conscience. Thus, if we had a law requiring eating meat, we would expect that
most of the violators would be conscientious. Similarly, a law against
something—say, the wearing of certain clothes or symbols—that is rarely done
except as a religious practice would likely be a law most violators of which
would be conscientious.

Now, acting against one’s conscience _is_ always wrong, and is almost always
_culpably_ wrong. For the most common case when doing something wrong isn’t
culpable is that one is ignorant of the wrongness, but when one acts against
one’s conscience one surely isn’t ignorant that one is acting against
conscience, and that we ought follow our conscience is obvious.

The difficult case, however, is when people’s consciences are mistaken to such
a degree that conscience requires them to do something that unjustly harms
others. (A less problematic mistake is when conscience is mistaken to such a
degree that conscience requires them to do something that’s permissible, but
not wrong. In those cases, tolerance is clearly called for. We shouldn’t
pressure vegetarians to eat animals even if their conscientious objection to
eating animals happens to be mistaken.)

In a just defensive war, to refuse to fight to defend one’s fellow citizens
without special reason (perhaps priests and doctors should not kill) is wrong.
But a grave harm is done to a conscientious objector who is gotten to fight by
legal incentives. Let’s think through the five considerations above. The first
mainly applies to laws prohibiting a behavior rather than ones requiring a
behavior. Short of brainwashing, it is impossible to _make_ someone fight. (We
could superglue their hands to a gun, and then administer electric shocks
causing their fingers to spasm and fire a bullet, but that wouldn’t count as
_fighting_.) The second applies somewhat: we do need to weigh the harms to
innocent citizens from enemy invaders, harms that might be prevented if our
conscientious objector fought. But note that there is something rather
speculative about these harms. Someone who fights contrary to conscience is
unlikely to be a very effective fighter, and it is far from clear that their
military activity would actually prevent any actual harm to innocents. Now,
regarding the third consideration, one can design a conscription law with an
exemption that few who aren’t conscientious objectors would take advantage of.
One way to do this is to require evidence of one’s conscience’s objection to
fighting (e.g., prior membership in a pacifist organization). Another way is
to impose non-combat duties on conscientious objectors that are as onerous and
maybe as dangerous as combat would be. Regarding the fourth consideration, it
seems unlikely that a typical conscientious objector’s objections to war would
be changed by legal penalties. And the fifth seems a weak consideration in
general. Putting all these together, we do not outweigh the _prima facie_
considerations against pressuring conscientious objectors to act against their
(mistaken) conscience from the harms in going against conscience.

One might think that what I said earlier implies that in this difficult case
the state should always allow people to follow their conscience, because after
all it is worse to do wrong—and violating conscience is wrong—than to have
wrong done to one. But that would be absurd and horrible—think of a racist
murderer whose faulty conscience requires them to kill.

When someone’s conscience mistakenly requires something that violates an
objective moral rule, there is a two-fold benefit to that person from a law
incentivizing following the moral rule. The law is a teacher, and the state’s
disapproval may change one’s mind about the matter. And even if it a harm to
one to violate conscience, it is also a harm to one to do something wrong even
inculpably. Thus, the harm of violating conscience is somewhat offset by the
benefit from not doing something else that is wrong.

That said, I think a qualification is plausible. Some wrongdoings are minor,
and in those cases the harm to the wrongdoer may be minor as well. But in any
case, to get someone to act against their conscience in a matter that
according to their conscience is major is to do them grave harm, a harm not
that different from death.

The harm of violating one’s conscience only happens to one if one _willingly_
violates one’s conscience. If law enforcement physically prevents me from
doing something that conscience requires from me, then I haven’t suffered the
harm. Thus, interestingly, the consideration I sketched against violating
one’s conscience does not apply when one is literally forced (fear of
punishment, unless it is severe enough to suspend one’s freedom of will, does
not actually _force_ , but only incentives).

One of the central insights of Western philosophy, beginning with Socrates,
has been that few if any things are as bad for an individual as culpably doing
wrong. It is better, we are told through much of the Western philosophical
tradition, that it is better to suffer than do injustice.

Now, the state, just like individuals, should _ceteris paribus_ avoid causing
grave harm. Hence, the state should generally avoid getting people to do
things that violate their conscience in major matters.

In some cases the person of mistaken conscience will still do the wrong deed
despite the law’s contrary incentive. In such a case, both the perpetrator and
the victim may be slightly better off for the law. The victim has a dignitary
benefit from the very fact that the state says that the harm was unlawful.
That dignitary benefit may be a cold comfort if the victim suffered a grave
harm, but it is still a benefit. And the perpetrator is slightly better off,
because following one’s conscience _against_ external pressure has an element
of admirability even when the conscience is mistaken.

Nonetheless, there will be cases where these considerations do not suffice,
and the law should be tolerant of mistaken conscience.

In fact, the same thing can be said about knowledge, assuming there is
knowledge in lottery situations. For suppose that I am just the slightest bit
short of the evidence needed for _knowledge_ that I have _C_. Then I can set
up the story such that:

I don’t know that I have _C_ but I know that I will know.

To see that (1) is true, note that the test is certain to come out positive if
I have _C_ and has a significant probability of coming out positive even if I
don’t have _C_. Hence, the probability of a positive test result will be
significantly higher than the probability that I have _C_. But I am just the
slightest bit short of the evidence needed for belief that I have _C_ , so the
evidence that the test would be positive (let’s suppose a deterministic
setting, so we have no worries about the sense of the subjunctive conditional
here) is sufficient for belief.

I’d take this as an argument against the view that belief is (or is justified
by) credence above a threshold.  
  
But that view seems implausible in any case. It’s pretty much only in gambling
setups with well-defined chances that we have sharp credences. But we all have
vast numbers of beliefs (taken in an informal ‘folk’ sense). So it seems that
some other approach is needed.  
  
A thorough Bayesian would be untroubled by this example. She would have her
credences, and that would be enough. She would not care whether they exceeded
some arbitrary threshold for belief. She would be similarly untroubled by the
lottery paradox. But no one in practice is a thorough Bayesian, or even close.
So again, it seems that some other approach is needed.

Ian,  
  
I hadn't thought about this in the context of reflection. But technically
there is no counterexample here to van Fraassen's reflection principle, which
only says that P(H | credence at t will be p) = p. My examples satisfy that,
assuming full transparency about my own credences and full certainty that I
will conditionalize (cf.: https://jonathanweisberg.org/pdf/C_R_and_SKv2.SP.pdf
).  
  
The original reflection principle implies that my credence should be the
expected value of my future credences (at a fixed future time; we might also
generalize to any martingale stopping time). This is not a problem in my
cases, because although it is likely that my future credence will be bigger
than it is now, there is a small chance that my future credence will plummet
to zero due to a negative test result, as William notes.  
  
Upon further thought, I think it can be quite reasonable to say: "I know that
tomorrow I will have knowledge-level evidence for p, but I don't yet." And if
so, then my subsequent post on squishiness, even if technically correct, is
moot.

As long as the test could be a true negative, you could always be surprised
and update to P of 0.0 if the test is negative. So waiting on the test before
you update is justified.

Still, there is something odd about (5). It’s a bit like the line:

Ian:  
  
I don't think the worries about sharpness of credences apply. For we can
suppose that the cases we apply the argument to are precisely ones with
sufficiently sharp credences. They might be cases concerning gambling
scenarios, or they could be medical cases where the only relevant data one has
is statistics from the most recent publications on the subject.  
  
I think all we really need for the argument is that in certain kinds of cases
meeting a credence threshold is what makes the difference between not having a
belief (or a justified belief) and having a belief (or a justified belief).  
  
By the way, suppose we think that the argument doesn't work for sharpness
reasons. Let's say that u is a level of sharpness such that it only makes
sense to say that you've met the threshold if you are at r+u or higher and
that you haven't met it if you are at r-u or lower, and if you're between r-u
and r+u it's indeterminate. Then working through the Bayesian details should
provide one with an interesting joint constraint on r and u. And then we can
ask see if it's reasonable to think that r and u actually satisfy the joint
constraint.

I don’t yet have enough evidence to know that I have _C_ , but I know that in
a moment I will.

To see that (2) is true, note that given that the false negative rate is zero,
and the false positive rate is not close to one, I will indeed have non-
negligible evidence for _C_ if the test is positive.

I have enough evidence to _know_ that the test would come out positive,

Suppose I am just the slightest bit short of the evidence needed for belief
that I have some condition _C_. I consider taking a test for _C_ that has a
zero false negative rate and a middling false positive rate—neither close to
zero nor close to one. On reasonable numerical interpretations of the previous
two sentences:

**Appendix:** Suppose the threshold for belief or knowledge is _r_ , where _r_
< 1. Suppose that the false-positive rate for the test is 1/2 and the false-
negative rate is zero. If _E_ is a positive test result, then _P_ ( _C_ | _E_
) = _P_ ( _C_ ) _P_ ( _E_ | _C_ )/ _P_ ( _E_ ) = _P_ ( _C_ )/ _P_ ( _E_ ) = 2
_P_ ( _C_ )/(1+ _P_ ( _C_ )). It follows by a bit of algebra that if my prior
_P_ ( _C_ ) is more than _r_ /(2− _r_ ), then _P_ ( _C_ | _E_ ) is above the
threshold _r_. Since _r_ < 1, we have _r_ /(2− _r_ ) < _r_ , and so the story
(either in the belief or knowledge form) works for the non-empty range of
priors strictly between _r_ /(2− _r_ ) and _r_.

If I am rational, my beliefs will follow the evidence. So if I am rational, in
a situation like the above, I will take myself to have a way of bringing it
about that I believe, and do so rationally, that I have _C_. Moreover, this
way of bringing it about that I believe that I have _C_ will itself be
perfectly rational if the test is free. For of course it’s rational to accept
free information. So I will be in a position where I am rationally able to
bring it about that I rationally believe _C_ , while not yet believing it.

If the test comes out positive, I will have enough evidence to know that I
have _C_.

I have enough evidence to believe that the test would come out positive.

If the test comes out positive, it will be another piece of evidence for the
hypothesis that I have _C_ , and it will push me over the edge to belief that
I have _C_.

But (6) is absurd: if I know that I will know something, then I am in a
position to know that the matter is so, since that I will know _p_ entails
that _p_ is true (assuming that _p_ doesn’t concern an open future). However,
there is no similar absurdity in (5). I may know that I will have enough
evidence to know _C_ , but that’s not the same as knowing that I will _know_
_C_ or even be in a position to know _C_. For it is possible to have enough
evidence to know something without being in a position to know it (namely,
when the thing isn’t true or when one is Gettiered).

To be clear, I don’t doubt that the example works. It does, and it illustrates
a genuine problem with the view that belief is (or is justified by) credence
above a threshold. Specifically, this violates an apparently natural
reflection principle, that if I rationally believe that I will rationally come
to believe, then I should believe now. Issues like this seem unavoidable with
a threshold (other than 1). The lottery paradox illustrates a different one:
violation of logical closure. Of course, you may be prepared to live with such
issues - maybe beliefs don’t have to be logically consistent.  
  
  
My problem with the threshold approach is more basic. If I don’t have a
credence, I can’t apply the threshold test. If I do have a credence, then
noting whether it is above or below a threshold adds nothing useful. One
response to this is straight Bayesianism, which rejects any role for belief
(beyond credence 1). Another is that credence and belief are separate and
related, but not straightforwardly. In the example, I would say that I
_believe_ the relevant medical information about the chance that I have C and
about the reliability of the test, and set my _credences_ accordingly.

In other words, oddly enough, just prior to getting the test results I can
reasonably say:

So, if any event _E_ has an explanation, it has an explanation going all the
way back to something self-explanatory. (1,2)

I have a problem with (2). (2) only follows if a complete explanation is
possible, which is kind of what you want to prove. So, it seems to me (2) begs
the question.  
If the 'water' is a brute fact, 'the earth floats on water' is as far as an
explanation can get. In a way it is merely partial but it isn't actually part
of something complete because there isn't something complete.  
That's why (1) is 'kind of right' but it isn't completely right.  
  
  
  

A partial explanation is one that is a part of a complete explanation.

I am not sure I buy (1). But it sounds kind of right to me now. Additionally,
(3) kind of sounds correct on its own. If _A_ causes _B_ and _B_ causes _C_
but there is no explanation of _A_ , then it seems that _B_ and _C_ are really
unexplained. Aristotle notes that there was a presocratic philosopher who
explained why the earth doesn’t fall down by saying that it floats on water,
and he notes that the philosopher failed to ask the same question about the
water. I think one lesson of Aristotle’s critique is that if it is unexplained
why the water doesn’t fall down it is unexplained why the earth falls down.

An explanation going back to something self-explanatory involves the activity
of a necessary being.

Any explanation for an event _E_ that does not go all the way back to
something self-explanatory is merely partial.

Dr Pruss, do you think that Bradley type regresses must also stop in some sort
of self explanatory fact? For example instead of having an infinite regress of
relations being related to their relata maybe it is somehow self explanatory
that the relation is related to its relata?

But the same is not true for any other attribute besides “human” among those
of the first paragraph. I can be good and a badminton player while not being a
good badminton player, and I can be a good herring-eater without being good
and a herring-eater. And I have no idea what it is to be a good six-footer,
but perhaps the fact that I am a quarter of an inch short of six feet right
now makes me not be one. (In some cases one direction may hold. It may be that
if I am good and a human, then I am a good human.)

**Response:** I deny that it’s absurd. It’s just that all the electrons we
meet _are_ good at electronicity.

Necessarily, I am good and a virtuous human if and only if I am a good
virtuous human.

_x_ is “centrally, really, deep-down, essentially” _F_ just in case what it is
for _x_ to be good is for _x_ to be a good _F_.

Necessarily, I am good and a human if and only if I am a good human.

On the "male human" point: it does seem like I can say "I am good and a man
iff I am a good man." But then (if we take the above approach) it seems like I
am centrally (really, deep-down) a man, not merely a human. This seems to sit
less comfortably with the Athanasian point. Perhaps this depends on the notion
of gender-specific norms? I'm not enough of a philosopher of gender to know
the literature on things like that.  
  

**Objection 2:** “Good” is attributive, and hence there is no such thing as
being good _simpliciter_.

What am I? A herring-eater, a husband, a badminton player, a philosopher, a
father, a Canadian, a six-footer and a human are all correct answers. There is
an ancient form of the “What is _x_?” question where we are looking for a
“central” answer, and of course “a human” is usually taken to be that one.
What makes for that answer being central?

Necessarily, if I am human, what it is for me to be good is for me to be a
good human.

In other words, that which I am centrally, really, deep-down and essentially
is that which sets the norms for me to be good _simpliciter_.

is not _generally_ logically valid. But some instances of a schema can be
logically valid even if the schema is not logically valid in general.

In other words, if I am a human, being a good human explains my being good. On
the other hand, even if I were a virtuous human, my being a good virtuous
human would not explain my being good. For redundancy is to be avoided in
explanation, and “good virtuous human” is redundant.

So the sense of the “What am I centrally, really, deep-down, essentially?”
question isn’t just modal. What is it?

Maybe, but can you say: What it is for you to be good is for you to be a good
man?  
  
Maybe the issue is that there is an ambiguity. A man (in the relevant sense)
is an adult male human. But "good adult male human" is a bit ambiguous: is one
good at the adulting, the maleness or the humanness, or at all three?  
  
I would say that what makes you be good is that you are good at being human,
and given that you are a man, this may have certain implications that it
wouldn't have if you weren't a man. (E.g., that you are a man makes it bad for
you to say "I am not a man".)

So our initial account of what is being asked for is that we are asking for an
attribute _F_ such that:

But perhaps we can do better. The necessary biconditional (2) holds in the
case “virtuous human”, but in a kind of trivial way: “a good virtuous human”
is repetition. I think that, as often, we need to pass from a modal to a
hyperintensional characterization. Consider that not only is (1) true, but
also:

Andrew:  
  
Nice paper!  
  
But I think classification is not what I am getting at. I am aptly classified
as a male human, a human, a mammal, a vertebrate, an animal, an organism, and
an enmattered thing. However, I think only "human" is the answer to the
question I am after, the "what am I really" question. I am looking for a
particular "deep" classification. This is neither the most general
classification nor the most specific one, but the one that captures what most
deeply I am.  
  
On Athanasian theology, what makes me be saved is that the second person of
the Trinity became human just as I am human. He also became a male human just
as I am a male human and he became a vertebrate just as I am a vertebrate. But
these things do not save me. What saves me is his co-humanity, not his co-
vertebricity or co-animality or co-maleness. There is something deeper about
humanity than about any of these other things.  
  
This may be ethically important, too. Should I care more for a squid or a
mouse, if their intelligence is equal? The mouse is a fellow vertebrate, but
so what? It makes no difference. Similarly, that someone is a fellow male
matters not a whit. However, that someone is a fellow human, that seems to
matter.

Another interpretation of the question of what we are is that it is asking for
*classification*. Generics can be useful here, because they (a) are apt
answers to classification questions and (b) still allow for exceptions. The
former feature makes them substantive. The latter makes them modest and immune
to certain styles of counterexample. For more on this (with respect to the
question of what *we* are, but the tools at play are widely applicable), see:
https://andrewmbailey.com/GenericAnimalism.pdf

Sometimes the word “essential” is thrown in: I am _essentially_ human. But
what does that mean? In contemporary analytic jargon, it just means that I
cannot exist without being human. But the “central” answer to “What am I?” is
not just an answer that states a property that I cannot lack. There are, after
all, many such properties essential in such a way that do not answer the
question. “Someone conceived in 1972” and “Someone in a world where 2+2=4”
attribute properties I cannot lack, but are not the central answers.

**Objection 1:** Some things are “centrally” electrons, but something’s being
good at electronicity is absurd.

Necessarily, _x_ is a good _F_ if and only if _x_ is good.

_x_ is good and _x_ is _F_ if and only if _x_ is a good _F_

And yet “a virtuous human” would not be the answer to the ancient “What am I
centrally, really, deep-down, essentially?” question even if I were in fact a
virtuous human.

Alex  
  
As Ian says, (1) applies to finite creatures, it doesn't apply to an
omniscient being. On open theism, God knows everything that can be known, but
libertarian free choices cannot be known in advance. If God were to "guess"
what I will do tomorrow, of course on open theism, he could turn out to be
wrong, but it would be a guess and not a belief.

I’d reject (1) in any case. Perfectly rational beings would have no use for
belief. They would have consistent credences (which could be 0 or 1) for
everything. On this view, beliefs are a shortcut that we, with our limited
mental capacity, use to reduce processing load.

I suppose the best way out is for the open theist to deny (1).

Consider worlds created by God that contain four hundred people, each of whom
has an independent 1/2 chance of freely choosing to eat an orange tomorrow
(they love their oranges). Let _p_ be the proposition that at least one of
these 400 people will freely choose to eat an orange tomorrow. The chance of
not- _p_ in any such world will be (1/2)400 < 1/10100. Assuming open theism,
so God doesn’t just directly know whether _p_ is true or not, God will take
the probability of _p_ in any such world to be bigger than 1 − (1/10100) and
by (1) God will believe _p_ in these worlds. But in some of these worlds, that
belief will turn out to be false—no one will freely eat the orange. And this
violates (3).

A rational being believes anything that they take to have probability bigger
than 1 − (1/10100) given their evidence.

Speaking of "God's beliefs" seems about as coherent as discussing what God ate
for lunch. He's not a finite creature.

Ian:  
  
I am somewhat inclined to say that a high credence just *is* what a belief is.  
  
THToD:  
  
Like you and Alston, I am inclined to agree that "belief" is at best an
awkward way to describe God's cognition. However, I think a major part of the
reason why God doesn't fundamentally have beliefs is that there is no room for
a gap in God between cognition and reality. Whether an open theist can have
the "no gap" view of God depends on the variety of open theism. If the open
theist is an open futurist, who thinks there are no facts in reality about
future contingents, then they can have the "no gap" view. But an open theist
like Swinburne or van Inwagen who thinks there are facts about future
contingents and God doesn't know them thinks there is a gap between God's
knowledge and reality, and now God is sounding very much like us -- there is
stuff that God is uncertain about.

These three theses, together with some auxiliary assumptions, yield a serious
problem for open theism.

This is interesting action-theoretically. The follow-through appears
pointless, because the agent’s interest is in what happens _before_ the
follow-through, the impact’s having the right physical properties, and yet
there is surely no backwards causation here. But there not appear to be an
effective way to reliably secure these physical properties of the impact
except by trying for the follow-through. So the follow-through itself is
pointless, but one’s _aiming at_ or _trying for_ the follow-through very much
has a point. And here the order of causality is respected: one swings aiming
at the follow-through, which causes an impact with the right physical
properties, and the swing then continues on to the “pointless” follow-through.

As far as I know, in all racquet sports players are told to follow-through: to
continue the racquet swing after the ball or shuttle have left the racquet.
But of course the ball or shuttle doesn’t care what the racquet is doing at
that point. So what’s the point of follow-through? The usual story is this: by
aiming to follow-through, one hits the ball or shuttle better. If one weren’t
trying to follow-through, the swing’s direction would be wrong or the swing
might slow down.

But there is a problem here. For even if there is a “success value” in
accomplishing a self-set goal, the strength of the reasons for pursuing the
follow-through is also proportioned to facts independent of this exercise of
normative power. Rather, the reasons for pursuing the follow-through will
include the internal and external goods of victory (winning as such, prizes,
adulation, etc.), and these are independent of one’s setting follow-through as
one’s goal.

Maybe we should say this. Even if all intentional action is end-directed,
there are two kinds of reasons for an action: the reasons that come from the
value of the end and the reasons that come from the value of the pursuit of
that end. In the case of follow-through, there may be a fairly trivial success
value in the follow-through—a success value that comes from one’s exercise of
normative power in adopting the follow-through as one’s end—but that success
value provides only fairly trivial reasons. However, there can be
significantly non-trivial reasons for one’s pursuing that end, reasons
independent of that end.

Another move is this. We actually have a normative power to make something
_be_ an end. And then it becomes genuinely worth pursuing, because we have
adopted it as an end. So the player first exercises the normative power to
make follow-through be an end, and then pursues that end as an end.

Clearly the follow-through is _intended_ —it’s consciously planned, aimed at,
etc. But it need not be a means to anything one cares about in the game
(though, of course, in some cases it can be a means to impressing the
spectators or intimidating an opponent). But is it an end? It seems pointless
as an end!

Yet it seems that whatever is intended is intended as a means or an end. One
might reject this principle, taking follow-through to be a counterexample.

I'm not sure that the creaturely changes are necessarily "destroying" (or
creating) parts of God (beliefs) on the open theists' assumption. I think they
are determining them. I will assume open theists who believe God is maximally
knowledgeable, that is, knows all that can be known, which does not include
perfect knowledge of the future according to them.  
  
On this set of assumptions God will still, however, have perfect foreknowledge
of all possible outcomes as he can understand all possible cause/effect
branches as First Cause. He just doesn't know which path along the branching
options will eventuate before the time that they do. This means his knowledge
grows only in the sense that he is able to pick out which of each fully
foreseen branch is actualised. In other words, the only way his knowledge
grows is by the steady lengthening of a "track" through a foreknown "space" of
possible world-states over time. In such a scenario, creatures with free will
would determine the direction of the track and thus the precise determination
of God’s knowledge of it, but they would not be adding to or subtracting from
God's beliefs in any quantitative sense. The addition that would occur comes
about automatically as time flows, no matter what finite agents decide.  
  
Please note, I do not hold this position, but it seems to me it is not guilty
of the exact absurdity alleged here, whatever its other flaws.  

And of course by standing up, I bring it about that a new divine belief
exists. So:

It depends on the details of how they think beliefs work. But suppose they
hold to this picture: for each proposition p that God knows, there is God's
act of believing p. Moreover, these opponents of simplicity think that God's
act of believing p is a different act for each different p that God believes.
These different acts are on that view something like accidents of God. Thus,
by raising one's arm one brings it about that God's belief that one's arm is
in a lowered state does not exist and that God's belief that one's arm is in a
raised state does exist, and so one destroys a part of God and creates a new
one. (It is a part of the package that I am criticizing that propositions are
tensed, and so it's not enough for God to eternally know that at t1 the arm is
lowered and at t2 the arm is raised.)

I agree with Walter. It seems to me that 2 is more directly incompatible with
God's atemporality than with his lack of complexity as such. An atemporal God
who sees Creation across its history in toto as a block, whether He is simple
or complex, has only true beliefs about everything that has been or will be,
and about when each fact will be true for finite creatures who are temporal.
But his beliefs don't change.  
  
Although a God who changed with and experienced time such that his beliefs
also changed might be thus considered complex across time by having temporal
parts, that would seem to be assuming an A-theory of time for all (including
God), then importing a B perspective at the end, incoherently. So, simplicity
of essence (at any point in time) and atemporality could be distinguished I
suppose, such that a person could affirm both simplicity and temporality of
God. I'm not saying that any open theist holds this opinion, or should, but it
seems a logical possibility from within that framework.

Alex  
  
How does it follow from the idea that God is not simple that God’s beliefs
change as the reality they are about changes?  
  
If (2) is true, isn't that a problem for divine simplicity as well? For on
divine simplicity, God id identical to God's beliefs, so if (2) is true it
seems that God changes as the reality changes, which conradicts divine
simplicity as well as divine immutability.

How? Easy. I am now sitting, and God knows that. So, a part of God is the
belief that I am sitting. But I can destroy that belief of God’s by standing
up! For as soon as I stand up, the belief that I am sitting will no longer
exist. But on the view in question, God’s beliefs are parts of him. So by
standing up, I would bring it about that a part of God doesn’t exist.

I think one can hold that God's beliefs are accidents of his which are not
identical to him (rejecting a strong doctrine of divine simplicity) without
holding that they are "parts" of him in any sense that makes (3) or (4)
absurd. Even for created beings like us, the accident-substance relationship
does not appear to be at all the same as the part-whole relationship as common
sense conceives it. (I would find it weird to say that my beliefs are a part
of me in the same sense that my arm is a part of my body, for instance.)

God is not simple, and in particular God’s beliefs are proper parts of God.

I agree that one could hold 1 without 2. But my argument is against people who
hold the package of 1 and 2. Think here of process theologians, and their more
orthodox cousins.

There seems to be a legitimate reason to allow evil choices to exist and to
allow those evil choices to affect other people. Seemingly, actions wouldn't
mean much (such as freely loving God) if we were all in cubicles and couldn't
actually interact with people negatively. Free love and free hatred seems to
be two sides of the same coin. I would also say that there are legitimate
reasons for suffering to exist. Perhaps the world would have less good in it
if there is no suffering. Would God permit suffering to allow more good? I
don't see a reason as to why He wouldn't. Theodicies and defenses are very
interesting, though it seems to me that, intuitively at least, most theodicies
seem to deal with the problem of trivial evils.

Perhaps not negatively, but certainly 'without love'. It seems to me that God
did not have to create you and thus God did not have to love you. If God were
forced to love you or loved you randomly, I wouldn't consider God to be a
moral being. I also wouldn't consider love to be a moral good if it were
forced. Therefore, it seems possible that evil actions are merely privations
of good actions, and thus one cannot choose to do "nothing" because "nothing"
is a lack of good, which seems to be some sort of evil. Perhaps one could make
a distinction between evils, but it seems that this might be a better
distinction that the one I previously drew.

Moreover, we might ask whether our ignorance of goods higher up in the
hierarchy of goods beyond the ordinary goods is not itself evidence against
the existence of such goods. Here, I think the answer is that it is very
little evidence. We would expect any particular finite being to be able to
recognize only a finite number of types of good, and thus the fact that there
are only a finite number of goods that we recognize is very little evidence
against the hypothesis of the upward hierarchy of goods.

So if God exists, we would expect there to be unknown possible finite goods
that are related to the known horrendous evils in something like the
proportion in which the known great finite goods are related to the known
trivial evils. Thus, if God exists, there very likely is  
an upward hierarchy of possible goods to which the horrendous evils of this
life stand like a mosquito bite to the courage of a Socrates. If we believe in
this hierarchy of goods, then it seems we should be no more impressed by the
atheological evidential force of horrendous evils than the ordinary person is
by the atheological evidential force of trivial evils.

However, if God exists, we would _expect_ there to be an unbounded upward
qualitative hierarchy of possible finite goods. God is infinitely good, and
finite goods are participations in God, so we would expect a hierarchy of
qualitatively greater and greater types of good that reflect God’s infinite
goodness better and better.

Nobody seriously runs an argument against the existence of God from _trivial_
evils, like hangnails or mosquito bites.

There is, however, a difference between the cases. Many great ordinary goods
that dwarf trivial evils, like the courage of a Socrates, are known to us. Few
if any finite goods that dwarf horrendous evils are known to us. Nonetheless,
if theism is true, it is very likely that such goods are possible. And since
the argument from evil is addressed against the theist, it seems fair for the
theist to invoke that hierarchy.

The reason why few people seriously run an argument from trivial evils is
simply because the case is much more obvious in the case of horrendous evil.  
But a truly opnipotent being should be able to accomplish Evert good without
any evil, unless this is logically impossible  
So, unless there is a convincing argument for why a certain good cannot
posdibly be accomplished without permitting evil, the default position should
be that it is possible.  

Unknown  
  
So, God's actions do not mean much if He can't interact with people
negatively?

Why not? Here is a hypothesis. There are so very many possible much greater
goods—goods qualitatively and not just quantitatively much greter—that it
would be easy to suppose that God’s permitting the trivial evil could promote
or enhance one of these goods to a degree sufficient to yield justification.

On the other hand, if we think of _horrendous_ evils, like the torture of
children, it is difficult to think of much greater goods. Maybe with
difficulty we can come up with one or two possibilities, but not enough to
make it _easy_ to suppose a justification for God’s permission of the evil in
terms of the goods.

Unknown  
  
I wouldn't say "forced" is the correct word here, but it is true that God
_necessarily_ loves you. So, in a sense, God _did/does_ have to love you.

1) I think the participationism explanation is problematic, since by that same
logic one could maybe deny one truly possesses being or goodness or value; one
could even deny one causes anything at all, concluding occasionalism. But we
do have our own being & goodness, so it's not the divine being & goodness in
us, per Aquinas.  
  
  
2) In fact, there's a real sense in which our actions & thereby
accomplishments are uniquely our own in a way they aren't God's precisely
because our actions are rooted in our secondary causality, which is distinct
from God's causality. And secondary causality being distinct from primary
causality, the causal responsibility for secondary causal acts is in the
creature, since the primary causality of God in sustaining our actions in
existence only determins them insofar as they exist, but we determine them
insofar as whether we cause them to occur (the occurrence of them is distinct
from their basic existence) and what we cause to occur.  
  
3) Additionally, there may be a difference between bragging and general pride
- some moral theology manuals seem to suggest being proud of one's
accomplishments isn't sinful, because that's distinct from bragging morally.
And one can't be proud of things one doesn't truly possess or cause in some
way.  
  
Of course, other sources say any form of pride is sinful, so this isn't a
clear issue it seems - but if one did take the route that some forms of pride
ARE morally okay, then this makes the thesis we don't possess anything,
whether being or our actions, unlikely.

Walter:  
  
Good point.  
  
I propose to revise premise 2 to read:  
2*. There are some highly accomplished individuals such that for them humility
is an appropriate attitude only if God exists.  
  
Then we don't even need premise 3.  
  
To respond to your point, now, I suggest that we think of highly accomplished
individuals who did not have many of the kinds of advantages you list, and who
were hindered in various ways (e.g., by racism, sexism, poverty, war, etc.).
While no doubt everyone got some help from other humans, in some cases a
realistic appraisal of the degree of that help, especially when combined with
the degree to which fellow humans hindered the individual, will not suffice
for humility to be appropriate *if* God does not exist.

Wesley:  
  
"by that same logic one could maybe deny one truly possesses being or goodness
or value"  
  
There is precedent for saying something like that. "No one is good but God
alone" (Mark 10:8).  
  
There has got to be a sense, and a very important one, in which what Jesus
says is true. But there is also a sense in which we are good--but only good-
by-participation.

Here’s another way to think about it. Given (1) and (3), we need an
explanation of how it is that humility is an appropriate attitude for a highly
accomplished individual. Classical theism’s doctrine of participation provides
such an explanation: all the efforts and all the accomplishments are not truly
theirs but a participation in God’s perfection.

This seems question-begging. It's not because highly accomplished individuals
have a lot to brag about that they can brag about everything.  
  
Suppose you think this is a great argument, you certainly have reason to brag
about it, but even if there is no God, you also have reason to be humble,
because you may have been the first to come up with this particular argument,
but you could only have built a successful argument because of your education,
because of what you have studied, read, even the way your parents raised you.  
Nothing we do is exclusively our own accomplishment.  
Now if you don't agree with this, you don't have grounds for premise (1),
because if something really is your own accomplishment, humililty is not
appropriate because there is nothing wrong with being proud of what you have
accomplished.  
  
  
  

Alex  
  
I have already answered this in my first reply. For the individuals you
describe, humililty is not appropriate.

Premise (1) is controversial. The ancient Greeks would have denied it. But I
think the reason they denied it is that they didn’t have the examples that the
Christian tradition does, highly attractive examples examples of accomplished
lives of great humility.

Here’s the thought behind premise (2). If there is no God, then highly
accomplished individuals have much to brag about. Many of their
accomplishments are primarily _theirs_.

@Alex I think an important nuance here is that in this specific view of
participation, goodness-by-participation isn't actually **true or intrinsic**
goodness. But this is problematic because we do have other verses affirming
the true goodness of creatures, such as Genesis 1 explicitly affirms this, as
does 1 Timothy 4 (first four verses on marriage & food) and Matthew 10
(sparrows).  
  
The main problem would be that this view of participationism _wouldn't just
make it impossible_ to take pride or "brag" about something we did (which I
can concede is wrong), but that it also makes it impossible to actually say we
are good or have value as well. In the **same manner & for the same reasons**
we can't take pride in or brag about our actions, we also can't affirm we are
**good,** or have being, and this seems to be a big problem with that specific
account.  
  
And it's also important to point out that Scripture often uses **hyperbolic
negation** or hyperbolic merism - God for example in Jeremiah 7:22 apparently
denies He ever commanded the Hebrews to do sacrifice, yet that's not a literal
negation but a hyperbolic one to point out that loving God is more important -
rhetorically DENYING one thing to point out the greater importance of another,
without intending to truly deny the importance of the secondary. Same thing
with loving Christ & hating one's parents - intentionally hyperbolic contrasts
that actually convey a hierarchy of love, but not pure exclusivity.  
  
So basing a very specific view of participationism (because not all models of
participationism would agree that we aren't actually truly good) on a phrase
that is likely using intentional rhetorical hyperbole is more speculative than
solid.

Suppose your end is irrationality. Is it really true that you _should_ adopt
the means to that, such as reasoning badly? Surely not! Instead, you should
reject the end.

I am inclined to think (1) is false if by “end” is meant the end the agent
actually adopts, as opposed to a natural end of the agent. If your ends are
sufficiently irrational, adopting means appropriate to them may be less
rational than adopting means inappropriate to them.

But what is wrong with being such that you adopt means inappropriate to your
ends is not necessarily the means—it could be the ends.

Unjust laws have no normative force, and stupid ends have no normative force,
either.

[Adapting](https://www.instructables.com/Playing-NES-Power-Pad-Games-in-
Emulation/) Dance Dance Revolution and other mat controllers to work as NES
Power Pad controllers for emulation.

The amount of things one person can do with enough time is insane. I wouldn't
believe you if you told me that one person can get two PhDs, teach classes in
metaphysics, run a blog with constantly new and exciting arguments and
positions, is called one of the foremost Christian philosophers currently
alive, and STILL has time to measure bicycle energy output. I am convinced
that Dr. Pruss is some sort of superhuman.

The title of this post contradicts the title of [another recent
post](http://alexanderpruss.blogspot.com/2022/12/the-right-cannot-be-derived-
from-good.html), but the contents do not.

This is compatible with there being cases where it is bad for one to do the
right thing. Thus, refraining from stealing the money that one would need to
sign up for a class on virtue is right and noninstrumentally good, but if the
class is really effective then stealing the money might be instrumentally good
for one, though noninstrumentally ba.

I think (1) is something that everyone should accept. Even consequentialists
can and should accept (1) (though utilitarian consequentialists have too
shallow an axiology to make (1) true). But natural law theorists might add a
further claim to (1): the left-hand-side is true because the right-hand-side
is true.

An action is right (respectively, wrong) if and only if it is
noninstrumentally good (respectively, bad) to do it.

There is a way to connect the right and wrong with the good and bad:

Moreover, note that it is very plausible that what range of variation of
priors is good for the community depends on the species of rational animal we
are talking about. Rational apes like us are likely more epistemically
cooperative than rational sharks would be, and so rational sharks would
benefit less from variation of priors, since for them the good of the
community would be closer to just the sum of the individual goods.

It is epistemically better for the human community if human beings do not all
have the same (ur-) priors.

This could well be true because differences in priors lead to a variety of
lines of investigation, a greater need for effort in convincing others, and
less danger of the community as a whole getting stuck in a local epistemic
optimum. If this hypothesis is true, then we would have an interesting story
about why it would be good for our community if a range of priors were
rationally permissible.

Of course, that it would be _good_ for the community if some norm of
individual rationality obtained does not prove that the norm obtains.

I think it does. I have been trying to defend a natural law account of
rationality on which just as our moral norms are given by what is natural for
the will, our epistemic norms are given by what is natural for our intellect.
And just as our will is the will of a particular kind of deliberative animal,
so too our intellect is the intellect of a particular kind of investigative
animal. And we expect a correlation between what a social animal’s nature
impels it to do and what is good for the social animal’s community. Thus, we
expect a degree of harmony between the norms of epistemic rationality—which on
my view are imposed by the nature of the animal—and the good of the community.

At the same time, the harmony need not be perfect. Just as there may be times
when the good of the community and the good of the individual conflict in
respect of non-epistemic flourishig, there [may be such
conflict](http://alexanderpruss.blogspot.com/2011/01/epistemic-self-sacrifice-
and-prisoner.html) in epistemic flourishing.

I am grateful to Anna Judd for pointing me to a possible connection between
permissivism and natural law epistemology.

Panteleology holds that teleology is ubiquitous. Every substance aims at  
some end.

That said, I think the quantum realm provides room for saying that things
don’t “just do what they do”. If an electron is in a mixed spin up/down state,
it seems right to think about it as having a directedness at a pure spin-up
state and a directedness at a pure spin-down state, and only one of these
directednesses will succeed.

The main objection to panteleology is the same as that to panpsychism: the
incredulous stare. I think a part of the puzzlement comes from the thought
that things that are neither biological nor artifactual “just do what they
do”, and there is no such thing as failure. But this seems to me to be a
mistake. Imagine a miracle where a rock fails to fall down, despite being
unsupported and in a gravitational field. It seems very natural to say that in
that case the rock failed to do what rocks should do! So it may be that away
from the biological realm (namely organisms and stuff made by organisms)
failure takes a miracle, but the logical possibility of such a miracle makes
it not implausible to think that there really is a directedness.

Panteleology seems to be exactly what we would expect in a world created by
God. Everything _should_ glorify God.

I think many theists would admit that God has created every substance for a
purpose, but that'd be an extrinsic teleology, and would be less
controversial. You've said elsewhere that you think that every substance has a
teleology. Are you meaning to argue that every substance has intrinsic
teleology? That'd be more controversial.  
  
  
  

Couldn't one relate teleology to causal powers and the possible effects they
could accomplish? Final causality exists as long as anything has a causal
power TOWARDS anything, and this directedness of power - without needing to be
active even - is itself a real example of teleology.  
  
For any agent that has the power to cause any effect in any way, it must be
directed towards that end at least insofar as any POWER only makes coherent
sense insofar as it has an EFFECT which it includes within itself and thereby
points.

Panteleology is also entailed by a panpsychism that follows Leibniz in
including the ubiquity of “appetitions” and not just perceptions. And it seems
to me that if we think through the kinds of reasons people have for
panpsychism, these reasons extend to appetitions—just as a discontinuity in
perception is mysterious, a discontinuity in action-driving is mysterious.

I don’t think we can derive (2) in accordance with the strictures in (1). If a
kidney were a lot more valuable than 20% of lifetime income, we would have
some hope of deriving (2) from descriptive facts, non-rightness value facts,
and abstract moral principles, for we might have some abstract moral principle
prohibiting the government from forcibly and non-punitively taking something
above some value. But a kidney is not a lot more valuable than 20% of lifetime
income. Indeed, if it would cost you 20% of your lifetime income to prevent
the destruction of one of your kidneys, it need not be unreasonable for you to
refuse to pay. Indeed, it seems that either 20% of lifetime income is
incommensurable with a kidney, or in some cases it is more valuable than a
kidney.

Consider the following thesis that both Kantians, utilitarians and New Natural
Law thinkers will agree on:

The restriction to non-rightness good and bad is to avoid triviality. By
“rightness value” here, I mean only the value that an action or character has
in virtue of its being right or wrong to the extent that it is.

I don’t have a good definition of “abstract moral principle”, but I want them
to be highly general principles about moral agency such as “Choose the greater
over the lesser good”, “Do not will the evil”, etc.

All facts about rightness and wrongness can be derived from descriptive facts,
facts about non-rightness value, and a small number of fundamental abstract
moral principles.

If loss of a kidney were to impact one’s autonomy significantly more than loss
of 20% of your lifetime income, then again there would be some hope for a
derivation of (2). But whether loss of a kidney is more of an autonomy impact
than loss of 20% of income will differ from person to person.

It is not wrong for the government to forcibly and non-punitively take 20% of
your lifetime income, but it is wrong for the government to forcibly and non-
punitively take one of your kidneys.

One might suppose that among the small number of fundamental abstract moral
principles one will have some principles about respect for bodily integrity. I
doubt it, though. Respect for bodily integrity is an immensely complex area of
ethics, and it is very unlikely that it can be encapsulated in a small number
of abstract moral principles. Respect for bodily integrity differs in very
complex ways depending on the body part and the nature of the relationship
between the agent and the patient.

I should note that the above argument fails against divine command theories.
Divine command theorists will say that about rightness and wrongness are
identified with descriptive facts about what God commands, and these facts can
be very rich and hence include enough data to determine (2). For the argument
against (1) to work, the “descriptive facts” have to be more like the facts of
natural science than like facts about divine commands.

I think Utilitarians might try to say something like, "If governments took
kidneys, this would have a different psychological effect on society than if
they took 20% of our income. Because of this accidental feature of our
psychologies, it actually would be more harmful for them to take our kidneys,
and look here I can show how this extra harm is unjustifiable via my abstract
moral principles."

The code uses [webpxmux.js](https://github.com/sumimakito/webpxmux.js), though
it was a little bit tricky because in-browser Javascript may not have enough
memory to store all the uncompressed images that webpxmux.js needs to generate
an animation. So instead I encode each frame to WebP using webpxmux.js,
extract the compressed ALPH and VP8 chunks from the WebP file, and store only
the compressed chunks, writing them all at the end. (It would be even better
from the memory point of view to write the chunks one by one rather than
storing them in memory, but a WebP file has a filesize in its header, and
that’s not known until all the compressed chunks have been generated. One
could get around this limitation by generating the video twice, but that would
be twice as slow.)

When making my [Guinness application record
video](http://alexanderpruss.blogspot.com/2022/12/a-new-but-uncertified-world-
record.html), I wanted to include a time display in the video and Guinness
also required a running count display. I ended up writing a [Python
script](https://gist.github.com/arpruss/3a705c859d4ea4a482c4e47e96a08f79)
using OpenCV2 to generate a video of the time and lap count, and overlaid it
with the main video in Adobe Premiere Rush.

Since then, I wrote a [web-based tool](https://arpruss.github.io//webpanim)
for generating a WebP animation of a timer and text synchronized to a set of
times. The timer can be in seconds or tenths of a second, and you can specify
a list of text messages and the times to display them (or to hide them). You
can then overlay it on a video in Premiere Rush or Pro. There is alpha
support, so you can have a transparent or translucent background if you like,
and a bunch of fonts to choose from (including the geeky-looking Hershey font
that I used in my Python script.)

People often talk of moral norms as overriding. The paradigm kind of case
seems to be like this:

However, there is another story possible. Perhaps in the case where the moral
considerations are at too low a level to override the _N_ -prohibition, we can
still have _moral_ permission to _ϕ_ , but that permission no longer overrides
the _N_ -prohibition. On this story, there are two kinds of cases, in both of
which we have moral permission, but in one case the moral permission comes
along with sufficiently strong moral considerations to override the _N_
-prohibition, while in the other it does not. On this story, moral requirement
always overrides non-moral reasons; but whether moral considerations override
non-moral considerations depends on the relative strengths of the two sets of
considerations.

I don’t want to say that all norms are moral norms. But it may well be that
all norms governing the functioning of the will are moral norms.

But this would be quite interesting. It would imply that in the absence of
sufficient moral considerations in favor of _ϕ_ ing, an _N_ -prohibition would
automatically generate a _moral_ prohibition. But this means that the real
normative upshot in all three cases is given by morality, and the _N_ -norms
aren’t actually doing any independent normative work. This suggests strongly
that on such a picture, we should take the _N_ -norms to be simply a species
of moral norms.

So far so good. Moral norms can override non-moral norms in two ways: by
creating a moral requirement contrary to the non-moral norms or by creating a
moral permission contrary to the non-moral norms.

where “ _N_ ” is some norm like that of prudence or etiquette. In this case,
the moral requirement of _ϕ_ ing overrides the _N_ -prohibition on _ϕ_ ing.
Thus, you might be rude to make a point of justice or sacrifice your life for
the sake of justice.

But now consider this. What happens if the moral considerations are at an even
lower level, a level insufficient to override the _N_ -prohibition? (E.g.,
what if to save someone’s finger you would need to sacrifice your arm?) Then,
it seems:

Still, consider this. The judgment whether moral considerations override the
non-moral ones seems to be an eminently _moral_ judgment. It is the person
with _moral_ virtue who is best suited to figuring out whether such overriding
happens. But what happens if morality says that the moral considerations do
not override the _N_ -prohibition? Is that not a case of morality giving its
endorsement to the _N_ -prohibition, so that the _N_ -prohibition would rise
to the level of a moral prohibition as well? But if so, then that pushes us
back to the previous story where it is reasonable to take _N_ -considerations
to be subsumed into moral considerations.

But if there are cases like (1), there will surely also be cases where the
moral considerations in favor of _ϕ_ ing do not rise to the level of a
requirement, but are sufficient to override the _N_ -prohibition. In those
cases, presumably:

Cases of supererogation look like that: you are morally permitted to do
something contrary to prudential norms, but not required to do so.

It is true that if Alice expects Bob to expect her to keep her promise, then
Alice will expect Bob to raise his right hand, and hence she should raise her
right hand. But since she’s known to be an amoral egoist, there is no reason
for Bob to expect Alice to keep her promise. And the same vice versa.

Take first the case where they are both perfect amoral egoists. Amoral egoists
don’t care about promises. So the fact that an amoral egoist promised to raise
the right hand is no evidence at all that they will raise the right hand,
unless there is something in it for them. But is there anything in it for
them? Well, if Bob raises his right hand, then there is something in it for
Alice to raise her right hand. But note that this conditional is true
_regardless_ of whether they’ve made any promises to each other, and it is
equally true that if Bob raises his left hand, then there is something in it
for Alice to raise her left hand.

The promise is simply irrelevant here. It is true that in normal
circumstances, it makes sense for egoists to keep promises in order to fool
people into thinking that they have morality. But I’ve assumed full shared
knowledge of each other’s tendencies here, and so no such considerations apply
here.

They confer before the game and promise to one another to raise the right
hand. They go into their separate rooms. And what happens next?

Here's a further line of thought. Suppose that Alice and Bob are not fully
utilitarian, but they incorporate into their ethics (which they follow
perfectly) an anti-conventionalist element (and that's also a part of their
shared knowledge). Thus, they think that the fact that one has promised p is a
fairly weak reason, of degree epsilon, against performing p. Then, if
epsilon>0, then it seems that by their lights Alice and Bob should lift their
left hands rather than their right, if they promised to lift their right,
since their behavioral bias is anti-promissory, and each knows the other to
have such a bias. Very well. Now take the limit as epsilon (the strength of
the reason they think favors breaking promises) to zero. For every epsilon>0,
they should lift their left hands rather than their right. It seems to be a
reasonable continuity conclusion that for epsilon=0, they should either be
neutral between lifting their left hands rather than their right or should
still prefer the left, but definitely should not prefer their right. And yet
epsilon=0 is just the full utilitarian case.

My feeling is it's not hard to solve -- as long as you don't place artificial
restrictions on what counts as a reason, in the way the utilitarian or egoist
does.

What if they are utilitarians? It makes no difference. Since in this case both
always get the same outcome, there is no difference between utilitarians and
amoral egoists.

This means that in cases like this, with full transparency of behavioral
tendencies, utilitarians and amoral egoists will do well to brainwash or
hypnotize themselves into promise-keeping.

Suppose Alice and Bob are perfect utilitarians or perfect amoral egoists in
any combination. They are about to play a game where they raise a left hand or
a right hand in a separate booth, and if they both raise the same hand, they
both get something good. Otherwise, nobody gets that good. Nobody sees what
they’re doing in the game: the game is fully automated. And they both have
full shared knowledge of the above.

In ordinary life, this problem doesn’t arise as much, because as long as at
least one person is more typical, and hence takes promises to have reason-
giving force, or if public opinion is around to enforce promise-keeping, then
the issue doesn’t come up. But I think there is a lesson here and in the
[previous post](http://alexanderpruss.blogspot.com/2022/12/utilitarianism-and-
communication.html): for many ordinary practice, the utilitarian is free-
riding on the non-utilitarians.

This is a fun puzzle. It seems like David Lewis’ convention work, or Thomas
Schelling’s coordination and focal points stuff must be relevant.

But I’ve been trying really hard to figure out how is it that such a
conventional behavior would indicate to Bob that the lion is on the left path.

If the above argument is correct—and I am far from confident of that, since it
makes my head spin—then we have an argument that in order for communication to
be possible, at least one of the agents must be convention-bound. One way to
be convention-bound is to think, in a way utilitarians don’t, that convention
provides non-consequentialist reasons. Another way is to be an akratic
utilitarian, addicted to following convention. Now, the possibility of
communication is essential for the utility of the kinds of social animals that
we are. Thus we have an argument that at least some subjective utilitarians
will have to become convention-bound, either by getting themselves to believe
that convention has normative force or by being akratic.

If Alice were a typical human being, she would have a habit of using
established social conventions to tell the truth about things, except perhaps
in exceptional cases (such as the murderer at the door), and so her use of the
conventional lion-indicating behavior would correlate with the presence of
lions, and would provide Bob with evidence of the presence of lions. But Alice
is not a typical human being. She is a subjectively perfect utilitarian.
Social convention has no normative force for Alice (or Bob, for that matter).
Only utility does.

This is not a refutation of utilitarianism. Utilitarians, following Parfit,
are willing to admit that there could be utility maximization reasons to cease
to be utilitarian. But it is, nonetheless, really interesting if something as
fundamental as communication provides such a reason.

Suppose the lion is on the left path. What should Alice do? Well, if she can,
she should bring it about that Bob takes the right path, because doing so
would clearly maximize utility. How can she do that? An obvious suggestion:
Engage in a conventional behavior indicating a where the lion is, such as
pointing left and roaring, or saying “Hail well-met traveler, lest you be
eaten, I advise you to avoid the leftward leonine path.”

I put this as an issue about communication. But maybe it’s really an issue
about communication but coordination. Maybe the literature on repeated games
might help in some way.

Similarly, if Bob were a typical human being, he would have a habit of forming
his beliefs on the basis of testimony interpreted via established social
conventions absent reason to think one is being misinformed, and so Alice’s
engaging in conventional left-path lion-indicating behavior would lead Bob to
think there is a lion on the left, and hence to go on the right. And while it
woudl still be true that social convention has no normative force for Alice,
Alice would think have reason to think that Bob follows convention, and for
the sake of maximizing utility would suit her behavior to his. But Bob is a
perfect Bayesian. He doesn’t form beliefs out of habit. He updates on
evidence. And given that Alice is not a typical human being, but a
subjectively perfect utilitarian, it is unclear to me why her engaging in the
conventional left-path lion-indicating behavior is more evidence for the lion
being on the left than for the lion being on the right. For Bob knows that
convention carries no normative force for Alice.

Alice and Bob are both perfect Bayesian epistemic agents and subjectively
perfect utilitarians (i.e., they always do what by their lights maximizes
expected utility). Bob is going to Megara. He comes to a crossroads, from
which two different paths lead to Megara. On exactly one of these paths there
is a man-eating lion and on the other there is nothing special. Alice knows
which path has the lion. The above is all shared knowledge for Alice and Bob.

Here is a brief way to put it. For Alice and Bob, convention carries no weight
except as a predictor of the behavior of convention-bound people, i.e., people
who are not subjectively perfect utilitarians. It is shared knowledge between
Alice and Bob that neither is convention-bound. So convention is irrelevant to
the problem at hand, the problem of getting Bob to avoid the lion. But there
is no solution to the problem absent convention or some other tool unavailable
to the utilitarian (a natural law theorist might claim that mimicry and
pointing are _natural_ indicators).

I've wondered about such incompossible goods myself. Weightlifting can reduce
agility, for example, and while being tall is great for basketball, it is not
good for weightlifting. Various niches seem to exist. It does raise the
question of what a perfect human being consists of, however. Are some of these
"perfections" merely accidental in the sense that they exploit what are
normatively speaking (in relation to human nature) flaws? We know some sports
are actually bad for the human body, or neglect the overall health of the
body. Certain perfections seem to be perfections only in an analogical sense,
perhaps something along the lines of being a "good thief" or an "effective
deceiver".  
  
Where intelligence is concerned, I am tempted to argue against incompossible
goods either because intelligence isn't like that or for similar natural law
reasons, but even more strongly, especially on account of the centrality of
intelligence to humanity.  
  
Consider your favorite déformation professionnelle, which, I submit, is more
of a result of imprudence, habit, ignorance, lack of practice using other
methods, and even effeminacy and arrogance. Someone with a rigorous
philosophical education is less likely to try to pigeonhole reality into the
reductive and simplified straitjacket of our physical models in the manner of
at least some physicists who generally lack serious exposure to philosophy and
may even hold it in contempt out of ignorance. A physicist may also be tempted
to pigeonhole simply because of pride; if he isn't any good at metaphysics,
then his thoughts on a metaphysical subject matter aren't likely to be very
valuable or interesting, and that stings the prideful man accustomed to
feeling like a hotshot. There is also the threat of seeing one's own field put
in its methodological place, so to speak, deflating any pretensions to the
kind of ultimacy that metaphysics lays claim to. A competent physics may also
derive greater pleasure from exercising his specialized competence and choose
his methods simply on the basis of what feels good and now what is called for
by a problem. So here the question resurfaces: is there an incompossibility
between being a good physicist and a good metaphysician? I suspect there isn't
intrinsically, even if that is often the case which I suspect is rather a
result of how one's time is spent. But even if it is the case, because general
knowledge is superior and more worthy of human attention than specialized
knowledge, we could argue that competence in physics that occurs at the
expense of philosophical depth is, in fact, a kind of failure to attain human
excellence by failing to devote proportional attention and effort to the kinds
of knowledge that are most essential, and in doing so, risking intellectual
deformation in important and even necessary matters.

There are empirical indications that various skills and maybe even virtues are
pretty domain specific. It seems that being good at reasoning about one thing
need not make one good at reasoning about another, even if the reasoning is
formally equivalent.

BTW, it may depend on how much one keeps to proper form in the different
racquet sports. Someone like me who plays lots of different racquet sports
(regularly: badminton; semi-regularly: tennis, racquetball, table tennis,
pickleball; used to do occasional squash until our university's one court
closed as nobody but my son and I played; used to do crossminton during
Covid), maybe at an upper beginner level, perhaps does not have enough good
form in any one of the sports for it to matter. I hadn't played much tennis
this fall, but I played a fair amount of badminton, and seemed to find my
tennis improved when I got back to it, maybe due to transfer of thinking about
things like "how do I hit the shuttle away from where my opponents are", or
maybe just due to general fitness improvement.

I think that's correct. The more one concentrates on good form, the more
detrimental negative transfer becomes. This also applies in martial arts like
Tae Kwon Do, on the self-defense side, whereby training to 'pull' kicks and
punches (i.e. to safeguard your training partner), negatively transfers to
real-life situations.

I do have a piece of anecdotal data, though. I’ve been doing some endurance-
ish sports. Nothing nearly like a marathon, but things like swimming 2-3 km,
or climbing for an hour, typically (but [not
always](http://alexanderpruss.blogspot.com/2022/12/a-new-but-uncertified-
world-record.html)) competing against myself.

I have no idea if anything like this transfer works for other people.

This is akin to the 'positive transfer' of skills, as discussed with the sport
science. So-called 'negative transfer', its opposite, is to be avoided at all
costs. For example, if I try to play squash to improve my racket skills, I
will ruin my tennis game. The racket skills are subtly different, due to the
'wrist flick' in the squash technique.

And I _have_ noticed some transfer of skills and maybe even of the virtue of
patience both between the various sports and between the sports and other
repetitive activities, such as grading. There is a distinctive feeling I have
when I am half-way through something, and where I am fairly confident I can
finish it, and a kind of relaxation past the half-way point where I become
more patient, and time seems to flow “better”. For instance, I can compare how
tired I feel half-way through a long set of climbs and how tired I feel half-
way through a 2 km swim, and the comparison can give me some strength. Similar
positive thinking can happen while grading, things like “I can do it” or
“There isn’t all that much left.” Though there are also differences between
the sports and the grading, because in grading the quality of the work matters
a lot more, and since I am not racing against myself so there is no point of a
burst of speed at the end if I find myself with an excess of energy. Pacing is
also much less important for grading.

**Final remark:** The argument applies to any exclusive and exhaustive
division of reasons into “simple” (i.e., non-combination) types.

So what should we say? One possibility is to say that there are _only_ reasons
of one type, say the moral. I find that attractive. Then benefits to yourself
also give you _moral_ reason to act, and so you simply have a moral reason to
spin the spinner. Another possibility is to say that in addition to moral and
prudential reasons there is some third class of “mixed” or “combination”
reasons.

**Objection:** The chance _p_ of the spinner landing on red is a prudential
reason and the chance 1 − _p_ of its landing on green is a moral reason. So
you have _two_ reasons, one moral and one prudential.

One might think that reasons for action are exhaustively and exclusively
divided into the moral and the prudential. Here is a problem with this.
Suppose that you have a spinner divided into red and green areas. If you spin
it and it lands into red, something nice happens to you; if it lands on green,
something nice happens to a deserving stranger. You clearly have reason to
spin the spinner. But, assuming the division of reasons, your reason for
spinning it is neither moral nor prudential.

**Response:** That may be right in the simple case. But now imagine that the
“red” set is a saturated nonmeasurable subset of the spinner edge, and the
“green” set is also such. A saturated nonmeasurable subset has no reasonable
probability assignment, not even a non-trivial range of probabilities like
from 1/3 to 1/2 (at best we can assign it the full range from 0 to 1). Now the
reason-giving strength of a chancy outcome is proportionate to the
probability. But in the saturated nonmeasurable case, there is no probability,
and hence no meaningful strength for the red-based reason or for the green-
based reason. But there is a meaningful strength for the red-or-green moral-
cum-prudential reason. The red-or-green-based reason hence does not reduce to
two separate reasons, one moral and one prudential.

Now, one might have technical worries about saturated nonmeasurable sets
figuring in decisions. I do. (E.g., see the Axiom of Choice chapter in my
infinity book.) But now instead of supposing saturated nonmeasurable sets,
suppose a case where an agent subjectively has literally no idea whether some
event _E_ will happen—has no probability assignment for _E_ whatsoever, not
even a ranged one (except for the full range from 0 to 1). The spinner landing
on a set believed to be saturated nonmeasurable might be an example of such a
case, but the case could be more humdrum—it’s just a case of extreme
agnosticism. And now suppose that the agent is told that if they so opt, then
they will get something nice on _E_ and a deserving stranger will get
something nice otherwise.

I think it is sometimes said that it is anachronistic to attribute to the
ancient Greeks the discovery that the square root of two is irrational,
because what they discovered was a properly _geometrical_ fact, that the side
and diagonal of a square are incommensurable, rather than a fact about real
numbers.

It is correct to say that the Greeks discovered an incommensurability fact.
But it is, I think, worth noting that this incommensurability fact is not
really geometric fact: it is a geometric-cum-arithmetical fact. Here is why.
The claim that two line segments are commensurable says that there are
positive integers _m_ and _n_ such that _m_ copies of the first segment have
the same length as _n_ copies of the second. This claim is essentially
arithmetical in that it quantifies over positive integers.

Don't you rather mean _"Th(something)"_ to be our "intended" model of
"something", such that "something" could be the naturals N, such that Th(N) is
our "intended" model of naturals N?!?  
  
Why isn't Th(N) recursively axiomattizaböe though?!?

I was thinking of the decidability of Th(N), where N is our "intended" model
of the naturals, not of the decidability of any particular recursive
axiomatization. (Th(N) is not recursively axiomatizable, of course.)

And because pure (Tarskian) geometry is decidable, while the theory of the
positive integers is not decidable, the positive integers are not definable in
terms of pure geometry, so we cannot eliminate the quantification over
positive integers. In fact, it is known that the rational numbers are not
definable in terms of pure geometry either, so neither the incommensurability
formulation nor theory irrationality formulation is a purely geometric claim.

Suppose there is a distinctive and significant value to knowledge. What I mean
by that is that if two epistemic are very similar in terms of truth, the level
and type of justification, the subject matter and its relevant to life, the
degree of belief, etc., but one is knowledge and the other is not, then the
one that is knowledge has a significantly higher value because it is
knowledge.

_s_ 1( _x_ , _t_ ) = − _b_ if _r_ < _x_ and _t_ = 0 or if _x_ < 1 − _r_ and
_t_ = 1.

Suppose that _a_ and _b_ are positive and _a_ / _b_ = (1− _r_ )/ _r_. Then if
my scribbled notes are correct, it is straightforward but annoying to check
that _s_ 1 is proper, and it has a discontinuous reward _a_ for meeting
threshold _r_ with respect to a truth and a discontinuous penalty  − _a_ for
meeting threshold _r_ with respect to a falsehood. To get a strictly proper
scoring rule, just add to it any strictly proper continous accuracy scoring
rule (e.g., Brier).

If we think that it’s never rational for a rational agent to refuse free
information, then the above argument can be made rigorous to establish that
any discontinuous rise in the epistemic value of credence at the point at
which knowledge of a truth is reached is exactly mirrored by a discontinuous
fall in the epistemic value of a state of credence where seeming-knowledge of
a falsehood is reached. Moreover, the rise and the fall must be in the ratio 1
− _r_ : _r_ where _r_ is the knowledge threshold. Note that for knowledge, _r_
is plausibly pretty large, around 0.95 at least, and so the ratio between the
special value of knowledge of a truth and the special disvalue of evidence-
sufficient-for-knowledge for a falsehood will need to be at most 1:19. This
kind of a ratio seems intuitively implausible to me. It seems unlikely that
the special disvalue of evidence-sufficient-for-knowledge of a falsehood is an
order of magnitude greater than the special value of knowledge. This
contributes to my scepticism that there is a special value of knowledge.

This hypothesis initially seemed to me to have an unfortunate consequence.
Suppose Alice has just barely exceeded the threshold for knowledge of _p_ ,
and she is offered a cost-free piece of information that may turn out to
slightly increase or slightly decrease her overall evidence with respect to
_p_ , where the decrease would be sufficient to lose her knowledge of _p_
(since she has only “barely” exceeded the evidential threshold for knowledge).
It seems that Alice should refuse to look at the information, since the
benefit of a slight improvement in credence if the evidence is non-misleading
is outweighed by the danger of a significant and discontinuous loss of value
due to loss of knowledge.

Plausibly, then, if we imagine Alice has some evidence for a truth _p_ that is
insufficient for knowledge, and slowly and continuously her evidence for _p_
mounts up, when the evidence has crossed the threshold needed for knowledge,
the value of Alice’s state with respect to _p_ will have suddenly and
discontinuously increased.

But that’s not quite right. For from Alice’s point of view, because the
threshold for knowledge is not 1, there is a real possibility that _p_ is
false. But it may be that just as there is a discontinuous gain in epistemic
value when your (rational) credence becomes sufficient for knowledge of
something that is in fact true, it may be that there is a discontinuous loss
of epistemic value when your credence becomes sufficient for knowledge of
something false. (Of course, you can’t know anything false, but you can have
evidence-sufficient-for-knowledge with respect to something false.) This is
not implausible, and given this, by looking at the information, by her lights
Alice also has a chance of a significant gain in value due to losing the
illusion of knowledge in something false.

_s_ 1( _x_ , _t_ ) = _a_ if _r_ < _x_ and _t_ = 1 or if _x_ < 1 − _r_ and _t_
= 0

Can we rigorously model this kind of an epistemic value assignment? I think
so. Consider the following discontinuous accuracy scoring rule _s_ 1( _x_ ,
_t_ ), where _x_ is a probability and _t_ is a 0 or 1 truth value:

_s_ 1( _x_ , _t_ ) = 0 if 1 − _r_ ≤ _x_ ≤ _r_

It’s in fact not hard to see that (6) is necessary and sufficient for the
existence of a case where (a)–(c) definitely hold.

In [a recent post](http://alexanderpruss.blogspot.com/2023/01/knowing-you-
will-soon-have-enough.html), I noted that it is possible to cook up a Bayesian
setup where you don’t meet some threshold, say for belief or knowledge, with
respect to some proposition, but you do meet the same threshold with respect
to the claim that after you examine a piece of evidence, then you _will_ meet
the threshold. This is counterintuitive: it seems to imply that you can know
that you will have enough evidence to know something even though you don’t
yet. In a comment, Ian noted that one way out of this is to say that beliefs
do not correspond to sharp credences. It then occurred to me that one could
use the setup to probe the question of how sharp our credences are and what
the thresholds for things like belief and knowledge are, perhaps
complementarily to the considerations in [this
paper](https://philarchive.org/rec/PRUBSA).

Note that the squishiness of our credences likely varies with where the
credences lie on the line from 0 to 1, i.e., varies with respect to the
relevant threshold. For we _can_ tell the difference between 0.999 and 1.000,
but we probably can’t tell the difference between 0.700 and 0.701. So the
squishiness should probably be counted relative to the threshold. Or perhaps
it should be correlated to log-odds. But I need to get to looking at grad
admissions files now.

For suppose we have a credence threshold _r_ and that our intuitions agree
that we can’t have a situation where:

_P_ ( _p_ | _E_ ) = _P_ ( _p_ )/ _P_ ( _E_ ) = ( _r_ − _α_ )/( _r_ − _α_ + _β_
(1−( _r_ − _α_ ))).

(We definitely won’t meet _r_ on a negative test result.) Thus to get (c)
definitely true, we need (3) to hold as well as the probability of a positive
test result to be at least _r_ \+ _α_ :

Note that by appropriate choice of _β_ , we can make _z_ be anything between
_r_ − _α_ and 1, and the right-hand-side of (3) is at least _r_ − _α_ since
_r_ \+ _α_ ≤ 1. Thus we can make (c) definitely hold as long as the right-
hand-side of (3) is bigger than or equal to the right-hand-side of (4), i.e.,
if and only if:

Let _z_ = _r_ − _α_ \+ _β_ (1−( _r_ − _α_ )) = (1− _β_ )( _r_ − _α_ ) + _β_.
This is the prior probability of a positive test result. We will definitely
meet _r_ on a positive test result just in case we have ( _r_ − _α_ )/ _z_ =
_P_ ( _p_ | _E_ ) ≥ _r_ \+ _α_ , i.e., just in case

Let _α_ > 0 be the “squishiness” of our credences. Let’s say that for one
credence to be definitely bigger than another, their difference has to be at
least _α_ , and that to definitely meet (fail to meet) a threshold, we must be
at least _α_ above (below) it. We assume that our threshold _r_ is definitely
less than one: _r_ \+ _α_ ≤ 1.

We thus have our joint constraint on the squishiness of our credences: bad
things happen if our credences have a level of precision equal to the right-
hand-side of (6). What exactly that says about _α_ depends on where the
relevant threshold lies. If the threshold _r_ is 1/2, the squishiness _α_ is
0.15. That’s surely higher than the actual squishiness of our credences. So if
we are concerned merely with the threshold being more-likely-than-not, then we
can’t avoid the paradox, because there will be cases where our credence is
definitely below the threshold and it’s definitely above the threshold that
examination of the evidence will push us about the threshold.

We thus have our joint constraint on the squishiness of our credences: bad
things happen if our credences are so precise as to make (6) true with respect
to a threshold _r_ for which we don’t want (a)–(c) to definitely hold. The
easiest scenario for making (a)–(c) definitely hold will be a binary test with
no false negatives.

we meet _r_ with respect to the proposition that we will meet the threshold
with respect to _p_ after we examine evidence _E_.

But what’s a reasonable threshold for belief? Maybe something like 0.9 or
0.95. At _r_ = 0.9, the squishiness needed for paradox is _α_ = 0.046. I
suspect our credences are more precise than that. If we agree that the
squishiness of our credences is less than 4.6%, then we have an argument that
the threshold for belief is _more_ than 0.9. On the other hand, at _r_ = 0.95,
the squishiness needed for paradox is 2.4%. At this point, it becomes more
plausible that our credences lack that kind of precision, but it’s not clear.
At _r_ = 0.98, the squishiness needed for paradox dips below 1%. Depending on
how precise we think our credences are, we get an argument that the threshold
for belief is something like 0.95 or 0.98.

What does this tell us about _r_ and _α_? We can actually figure this out.
Consider a test for _p_ that have no false negatives, but has a false positive
rate of _β_. Let _E_ be a positive test result. Our best bet to generating a
counterexample to (a)–(c) will be if the priors for _p_ are as close to _r_ as
possible while yet definitely below, i.e., if the priors for _p_ are _r_ −
_α_. For making the priors be that makes (c) easier to definitely satisfy
while keeping (b) definitely satisfied. Since there are no false negatives,
the posterior for _p_ will be:

One of the central insights of Western philosophy, beginning with Socrates,
has been that few if any things are as bad for an individual as culpably doing
wrong. It is better, we are told through much of the Western philosophical
tradition, that it is better to suffer than do injustice.

When someone’s conscience mistakenly requires something that violates an
objective moral rule, there is a two-fold benefit to that person from a law
incentivizing following the moral rule. The law is a teacher, and the state’s
disapproval may change one’s mind about the matter. And even if it a harm to
one to violate conscience, it is also a harm to one to do something wrong even
inculpably. Thus, the harm of violating conscience is somewhat offset by the
benefit from not doing something else that is wrong.

Now, the state, just like individuals, should _ceteris paribus_ avoid causing
grave harm. Hence, the state should generally avoid getting people to do
things that violate their conscience in major matters.

The difficult case, however, is when people’s consciences are mistaken to such
a degree that conscience requires them to do something that unjustly harms
others. (A less problematic mistake is when conscience is mistaken to such a
degree that conscience requires them to do something that’s permissible, but
not wrong. In those cases, tolerance is clearly called for. We shouldn’t
pressure vegetarians to eat animals even if their conscientious objection to
eating animals happens to be mistaken.)

In some cases the person of mistaken conscience will still do the wrong deed
despite the law’s contrary incentive. In such a case, both the perpetrator and
the victim may be slightly better off for the law. The victim has a dignitary
benefit from the very fact that the state says that the harm was unlawful.
That dignitary benefit may be a cold comfort if the victim suffered a grave
harm, but it is still a benefit. And the perpetrator is slightly better off,
because following one’s conscience _against_ external pressure has an element
of admirability even when the conscience is mistaken.

One might think that what I said earlier implies that in this difficult case
the state should always allow people to follow their conscience, because after
all it is worse to do wrong—and violating conscience is wrong—than to have
wrong done to one. But that would be absurd and horrible—think of a racist
murderer whose faulty conscience requires them to kill.

The harm of violating one’s conscience only happens to one if one _willingly_
violates one’s conscience. If law enforcement physically prevents me from
doing something that conscience requires from me, then I haven’t suffered the
harm. Thus, interestingly, the consideration I sketched against violating
one’s conscience does not apply when one is literally forced (fear of
punishment, unless it is severe enough to suspend one’s freedom of will, does
not actually _force_ , but only incentives).

Now, acting against one’s conscience _is_ always wrong, and is almost always
_culpably_ wrong. For the most common case when doing something wrong isn’t
culpable is that one is ignorant of the wrongness, but when one acts against
one’s conscience one surely isn’t ignorant that one is acting against
conscience, and that we ought follow our conscience is obvious.

A reasonable optimism says that in most cases most people’s consciences are
correct. Thus typically we would expect that most violators of a legitimate
law will not be acting out of conscience—for a necessarily condition for the
legitimacy of a law is that it does not conflict with a correct conscience.
Thus, even if there is the rare murderer acting from mistaken conscience, most
murderers act against conscience, and by incentivizing abstention from murder,
in most cases the law _helps_ people follow their conscience, and the small
number of other cases can be tolerated as a side effect. Thus the
considerations of conscience favor intolerant laws in such cases. Nonetheless,
there are cases where most violators of a law would likely be acting from
conscience. Thus, if we had a law requiring eating meat, we would expect that
most of the violators would be conscientious. Similarly, a law against
something—say, the wearing of certain clothes or symbols—that is rarely done
except as a religious practice would likely be a law most violators of which
would be conscientious.

In cases where doing wrong and suffering wrong are of roughly the same order
of magnitude, it is very intuitive that we should prevent the suffering of
wrong rather than the doing of wrong. Imagine that Alice is drowning while at
the same time Bob is getting ready to assassinate a politician, but we know
for sure that Bob’s bullets have all been replaced with blanks. If our choice
is whether to try to dissuade Bob from attempting murder or keep Alice from
drowning, we should keep Alice from drowning, evne if on the Socratic view the
harm to Bob from attempting murder will be greater than that to Alice from
drowning. (I am assuming that in this case the two harms are nonetheless of
something like the same order of magnitude.)

Nonetheless, there will be cases where these considerations do not suffice,
and the law should be tolerant of mistaken conscience.

That said, I think a qualification is plausible. Some wrongdoings are minor,
and in those cases the harm to the wrongdoer may be minor as well. But in any
case, to get someone to act against their conscience in a matter that
according to their conscience is major is to do them grave harm, a harm not
that different from death.

In a just defensive war, to refuse to fight to defend one’s fellow citizens
without special reason (perhaps priests and doctors should not kill) is wrong.
But a grave harm is done to a conscientious objector who is gotten to fight by
legal incentives. Let’s think through the five considerations above. The first
mainly applies to laws prohibiting a behavior rather than ones requiring a
behavior. Short of brainwashing, it is impossible to _make_ someone fight. (We
could superglue their hands to a gun, and then administer electric shocks
causing their fingers to spasm and fire a bullet, but that wouldn’t count as
_fighting_.) The second applies somewhat: we do need to weigh the harms to
innocent citizens from enemy invaders, harms that might be prevented if our
conscientious objector fought. But note that there is something rather
speculative about these harms. Someone who fights contrary to conscience is
unlikely to be a very effective fighter, and it is far from clear that their
military activity would actually prevent any actual harm to innocents. Now,
regarding the third consideration, one can design a conscription law with an
exemption that few who aren’t conscientious objectors would take advantage of.
One way to do this is to require evidence of one’s conscience’s objection to
fighting (e.g., prior membership in a pacifist organization). Another way is
to impose non-combat duties on conscientious objectors that are as onerous and
maybe as dangerous as combat would be. Regarding the fourth consideration, it
seems unlikely that a typical conscientious objector’s objections to war would
be changed by legal penalties. And the fifth seems a weak consideration in
general. Putting all these together, we do not outweigh the _prima facie_
considerations against pressuring conscientious objectors to act against their
(mistaken) conscience from the harms in going against conscience.

I’d take this as an argument against the view that belief is (or is justified
by) credence above a threshold.  
  
But that view seems implausible in any case. It’s pretty much only in gambling
setups with well-defined chances that we have sharp credences. But we all have
vast numbers of beliefs (taken in an informal ‘folk’ sense). So it seems that
some other approach is needed.  
  
A thorough Bayesian would be untroubled by this example. She would have her
credences, and that would be enough. She would not care whether they exceeded
some arbitrary threshold for belief. She would be similarly untroubled by the
lottery paradox. But no one in practice is a thorough Bayesian, or even close.
So again, it seems that some other approach is needed.

Ian:  
  
I don't think the worries about sharpness of credences apply. For we can
suppose that the cases we apply the argument to are precisely ones with
sufficiently sharp credences. They might be cases concerning gambling
scenarios, or they could be medical cases where the only relevant data one has
is statistics from the most recent publications on the subject.  
  
I think all we really need for the argument is that in certain kinds of cases
meeting a credence threshold is what makes the difference between not having a
belief (or a justified belief) and having a belief (or a justified belief).  
  
By the way, suppose we think that the argument doesn't work for sharpness
reasons. Let's say that u is a level of sharpness such that it only makes
sense to say that you've met the threshold if you are at r+u or higher and
that you haven't met it if you are at r-u or lower, and if you're between r-u
and r+u it's indeterminate. Then working through the Bayesian details should
provide one with an interesting joint constraint on r and u. And then we can
ask see if it's reasonable to think that r and u actually satisfy the joint
constraint.

Still, there is something odd about (5). It’s a bit like the line:

To see that (1) is true, note that the test is certain to come out positive if
I have _C_ and has a significant probability of coming out positive even if I
don’t have _C_. Hence, the probability of a positive test result will be
significantly higher than the probability that I have _C_. But I am just the
slightest bit short of the evidence needed for belief that I have _C_ , so the
evidence that the test would be positive (let’s suppose a deterministic
setting, so we have no worries about the sense of the subjunctive conditional
here) is sufficient for belief.

But (6) is absurd: if I know that I will know something, then I am in a
position to know that the matter is so, since that I will know _p_ entails
that _p_ is true (assuming that _p_ doesn’t concern an open future). However,
there is no similar absurdity in (5). I may know that I will have enough
evidence to know _C_ , but that’s not the same as knowing that I will _know_
_C_ or even be in a position to know _C_. For it is possible to have enough
evidence to know something without being in a position to know it (namely,
when the thing isn’t true or when one is Gettiered).

To be clear, I don’t doubt that the example works. It does, and it illustrates
a genuine problem with the view that belief is (or is justified by) credence
above a threshold. Specifically, this violates an apparently natural
reflection principle, that if I rationally believe that I will rationally come
to believe, then I should believe now. Issues like this seem unavoidable with
a threshold (other than 1). The lottery paradox illustrates a different one:
violation of logical closure. Of course, you may be prepared to live with such
issues - maybe beliefs don’t have to be logically consistent.  
  
  
My problem with the threshold approach is more basic. If I don’t have a
credence, I can’t apply the threshold test. If I do have a credence, then
noting whether it is above or below a threshold adds nothing useful. One
response to this is straight Bayesianism, which rejects any role for belief
(beyond credence 1). Another is that credence and belief are separate and
related, but not straightforwardly. In the example, I would say that I
_believe_ the relevant medical information about the chance that I have C and
about the reliability of the test, and set my _credences_ accordingly.

If I am rational, my beliefs will follow the evidence. So if I am rational, in
a situation like the above, I will take myself to have a way of bringing it
about that I believe, and do so rationally, that I have _C_. Moreover, this
way of bringing it about that I believe that I have _C_ will itself be
perfectly rational if the test is free. For of course it’s rational to accept
free information. So I will be in a position where I am rationally able to
bring it about that I rationally believe _C_ , while not yet believing it.

As long as the test could be a true negative, you could always be surprised
and update to P of 0.0 if the test is negative. So waiting on the test before
you update is justified.

If the test comes out positive, I will have enough evidence to know that I
have _C_.

I don’t know that I have _C_ but I know that I will know.

Ian,  
  
I hadn't thought about this in the context of reflection. But technically
there is no counterexample here to van Fraassen's reflection principle, which
only says that P(H | credence at t will be p) = p. My examples satisfy that,
assuming full transparency about my own credences and full certainty that I
will conditionalize (cf.: https://jonathanweisberg.org/pdf/C_R_and_SKv2.SP.pdf
).  
  
The original reflection principle implies that my credence should be the
expected value of my future credences (at a fixed future time; we might also
generalize to any martingale stopping time). This is not a problem in my
cases, because although it is likely that my future credence will be bigger
than it is now, there is a small chance that my future credence will plummet
to zero due to a negative test result, as William notes.  
  
Upon further thought, I think it can be quite reasonable to say: "I know that
tomorrow I will have knowledge-level evidence for p, but I don't yet." And if
so, then my subsequent post on squishiness, even if technically correct, is
moot.

In other words, oddly enough, just prior to getting the test results I can
reasonably say:

I have enough evidence to believe that the test would come out positive.

To see that (2) is true, note that given that the false negative rate is zero,
and the false positive rate is not close to one, I will indeed have non-
negligible evidence for _C_ if the test is positive.

I have enough evidence to _know_ that the test would come out positive,

**Appendix:** Suppose the threshold for belief or knowledge is _r_ , where _r_
< 1. Suppose that the false-positive rate for the test is 1/2 and the false-
negative rate is zero. If _E_ is a positive test result, then _P_ ( _C_ | _E_
) = _P_ ( _C_ ) _P_ ( _E_ | _C_ )/ _P_ ( _E_ ) = _P_ ( _C_ )/ _P_ ( _E_ ) = 2
_P_ ( _C_ )/(1+ _P_ ( _C_ )). It follows by a bit of algebra that if my prior
_P_ ( _C_ ) is more than _r_ /(2− _r_ ), then _P_ ( _C_ | _E_ ) is above the
threshold _r_. Since _r_ < 1, we have _r_ /(2− _r_ ) < _r_ , and so the story
(either in the belief or knowledge form) works for the non-empty range of
priors strictly between _r_ /(2− _r_ ) and _r_.

In fact, the same thing can be said about knowledge, assuming there is
knowledge in lottery situations. For suppose that I am just the slightest bit
short of the evidence needed for _knowledge_ that I have _C_. Then I can set
up the story such that:

Suppose I am just the slightest bit short of the evidence needed for belief
that I have some condition _C_. I consider taking a test for _C_ that has a
zero false negative rate and a middling false positive rate—neither close to
zero nor close to one. On reasonable numerical interpretations of the previous
two sentences:

I don’t yet have enough evidence to know that I have _C_ , but I know that in
a moment I will.

If the test comes out positive, it will be another piece of evidence for the
hypothesis that I have _C_ , and it will push me over the edge to belief that
I have _C_.

Dr Pruss, do you think that Bradley type regresses must also stop in some sort
of self explanatory fact? For example instead of having an infinite regress of
relations being related to their relata maybe it is somehow self explanatory
that the relation is related to its relata?

A partial explanation is one that is a part of a complete explanation.

So, if any event _E_ has an explanation, it has an explanation going all the
way back to something self-explanatory. (1,2)

An explanation going back to something self-explanatory involves the activity
of a necessary being.

I am not sure I buy (1). But it sounds kind of right to me now. Additionally,
(3) kind of sounds correct on its own. If _A_ causes _B_ and _B_ causes _C_
but there is no explanation of _A_ , then it seems that _B_ and _C_ are really
unexplained. Aristotle notes that there was a presocratic philosopher who
explained why the earth doesn’t fall down by saying that it floats on water,
and he notes that the philosopher failed to ask the same question about the
water. I think one lesson of Aristotle’s critique is that if it is unexplained
why the water doesn’t fall down it is unexplained why the earth falls down.

I have a problem with (2). (2) only follows if a complete explanation is
possible, which is kind of what you want to prove. So, it seems to me (2) begs
the question.  
If the 'water' is a brute fact, 'the earth floats on water' is as far as an
explanation can get. In a way it is merely partial but it isn't actually part
of something complete because there isn't something complete.  
That's why (1) is 'kind of right' but it isn't completely right.  
  
  
  

Any explanation for an event _E_ that does not go all the way back to
something self-explanatory is merely partial.

And yet “a virtuous human” would not be the answer to the ancient “What am I
centrally, really, deep-down, essentially?” question even if I were in fact a
virtuous human.

is not _generally_ logically valid. But some instances of a schema can be
logically valid even if the schema is not logically valid in general.

Maybe, but can you say: What it is for you to be good is for you to be a good
man?  
  
Maybe the issue is that there is an ambiguity. A man (in the relevant sense)
is an adult male human. But "good adult male human" is a bit ambiguous: is one
good at the adulting, the maleness or the humanness, or at all three?  
  
I would say that what makes you be good is that you are good at being human,
and given that you are a man, this may have certain implications that it
wouldn't have if you weren't a man. (E.g., that you are a man makes it bad for
you to say "I am not a man".)

Sometimes the word “essential” is thrown in: I am _essentially_ human. But
what does that mean? In contemporary analytic jargon, it just means that I
cannot exist without being human. But the “central” answer to “What am I?” is
not just an answer that states a property that I cannot lack. There are, after
all, many such properties essential in such a way that do not answer the
question. “Someone conceived in 1972” and “Someone in a world where 2+2=4”
attribute properties I cannot lack, but are not the central answers.

On the "male human" point: it does seem like I can say "I am good and a man
iff I am a good man." But then (if we take the above approach) it seems like I
am centrally (really, deep-down) a man, not merely a human. This seems to sit
less comfortably with the Athanasian point. Perhaps this depends on the notion
of gender-specific norms? I'm not enough of a philosopher of gender to know
the literature on things like that.  
  

Necessarily, I am good and a human if and only if I am a good human.

But perhaps we can do better. The necessary biconditional (2) holds in the
case “virtuous human”, but in a kind of trivial way: “a good virtuous human”
is repetition. I think that, as often, we need to pass from a modal to a
hyperintensional characterization. Consider that not only is (1) true, but
also:

But the same is not true for any other attribute besides “human” among those
of the first paragraph. I can be good and a badminton player while not being a
good badminton player, and I can be a good herring-eater without being good
and a herring-eater. And I have no idea what it is to be a good six-footer,
but perhaps the fact that I am a quarter of an inch short of six feet right
now makes me not be one. (In some cases one direction may hold. It may be that
if I am good and a human, then I am a good human.)

Necessarily, _x_ is a good _F_ if and only if _x_ is good.

**Objection 1:** Some things are “centrally” electrons, but something’s being
good at electronicity is absurd.

So the sense of the “What am I centrally, really, deep-down, essentially?”
question isn’t just modal. What is it?

So our initial account of what is being asked for is that we are asking for an
attribute _F_ such that:

**Response:** I deny that it’s absurd. It’s just that all the electrons we
meet _are_ good at electronicity.

Necessarily, if I am human, what it is for me to be good is for me to be a
good human.

Another interpretation of the question of what we are is that it is asking for
*classification*. Generics can be useful here, because they (a) are apt
answers to classification questions and (b) still allow for exceptions. The
former feature makes them substantive. The latter makes them modest and immune
to certain styles of counterexample. For more on this (with respect to the
question of what *we* are, but the tools at play are widely applicable), see:
https://andrewmbailey.com/GenericAnimalism.pdf

**Objection 2:** “Good” is attributive, and hence there is no such thing as
being good _simpliciter_.

In other words, if I am a human, being a good human explains my being good. On
the other hand, even if I were a virtuous human, my being a good virtuous
human would not explain my being good. For redundancy is to be avoided in
explanation, and “good virtuous human” is redundant.

Andrew:  
  
Nice paper!  
  
But I think classification is not what I am getting at. I am aptly classified
as a male human, a human, a mammal, a vertebrate, an animal, an organism, and
an enmattered thing. However, I think only "human" is the answer to the
question I am after, the "what am I really" question. I am looking for a
particular "deep" classification. This is neither the most general
classification nor the most specific one, but the one that captures what most
deeply I am.  
  
On Athanasian theology, what makes me be saved is that the second person of
the Trinity became human just as I am human. He also became a male human just
as I am a male human and he became a vertebrate just as I am a vertebrate. But
these things do not save me. What saves me is his co-humanity, not his co-
vertebricity or co-animality or co-maleness. There is something deeper about
humanity than about any of these other things.  
  
This may be ethically important, too. Should I care more for a squid or a
mouse, if their intelligence is equal? The mouse is a fellow vertebrate, but
so what? It makes no difference. Similarly, that someone is a fellow male
matters not a whit. However, that someone is a fellow human, that seems to
matter.

What am I? A herring-eater, a husband, a badminton player, a philosopher, a
father, a Canadian, a six-footer and a human are all correct answers. There is
an ancient form of the “What is _x_?” question where we are looking for a
“central” answer, and of course “a human” is usually taken to be that one.
What makes for that answer being central?

Necessarily, I am good and a virtuous human if and only if I am a good
virtuous human.

In other words, that which I am centrally, really, deep-down and essentially
is that which sets the norms for me to be good _simpliciter_.

_x_ is “centrally, really, deep-down, essentially” _F_ just in case what it is
for _x_ to be good is for _x_ to be a good _F_.

_x_ is good and _x_ is _F_ if and only if _x_ is a good _F_

I suppose the best way out is for the open theist to deny (1).

Ian:  
  
I am somewhat inclined to say that a high credence just *is* what a belief is.  
  
THToD:  
  
Like you and Alston, I am inclined to agree that "belief" is at best an
awkward way to describe God's cognition. However, I think a major part of the
reason why God doesn't fundamentally have beliefs is that there is no room for
a gap in God between cognition and reality. Whether an open theist can have
the "no gap" view of God depends on the variety of open theism. If the open
theist is an open futurist, who thinks there are no facts in reality about
future contingents, then they can have the "no gap" view. But an open theist
like Swinburne or van Inwagen who thinks there are facts about future
contingents and God doesn't know them thinks there is a gap between God's
knowledge and reality, and now God is sounding very much like us -- there is
stuff that God is uncertain about.

Consider worlds created by God that contain four hundred people, each of whom
has an independent 1/2 chance of freely choosing to eat an orange tomorrow
(they love their oranges). Let _p_ be the proposition that at least one of
these 400 people will freely choose to eat an orange tomorrow. The chance of
not- _p_ in any such world will be (1/2)400 < 1/10100. Assuming open theism,
so God doesn’t just directly know whether _p_ is true or not, God will take
the probability of _p_ in any such world to be bigger than 1 − (1/10100) and
by (1) God will believe _p_ in these worlds. But in some of these worlds, that
belief will turn out to be false—no one will freely eat the orange. And this
violates (3).

I’d reject (1) in any case. Perfectly rational beings would have no use for
belief. They would have consistent credences (which could be 0 or 1) for
everything. On this view, beliefs are a shortcut that we, with our limited
mental capacity, use to reduce processing load.

Alex  
  
As Ian says, (1) applies to finite creatures, it doesn't apply to an
omniscient being. On open theism, God knows everything that can be known, but
libertarian free choices cannot be known in advance. If God were to "guess"
what I will do tomorrow, of course on open theism, he could turn out to be
wrong, but it would be a guess and not a belief.

A rational being believes anything that they take to have probability bigger
than 1 − (1/10100) given their evidence.

Speaking of "God's beliefs" seems about as coherent as discussing what God ate
for lunch. He's not a finite creature.

These three theses, together with some auxiliary assumptions, yield a serious
problem for open theism.

Another move is this. We actually have a normative power to make something
_be_ an end. And then it becomes genuinely worth pursuing, because we have
adopted it as an end. So the player first exercises the normative power to
make follow-through be an end, and then pursues that end as an end.

But there is a problem here. For even if there is a “success value” in
accomplishing a self-set goal, the strength of the reasons for pursuing the
follow-through is also proportioned to facts independent of this exercise of
normative power. Rather, the reasons for pursuing the follow-through will
include the internal and external goods of victory (winning as such, prizes,
adulation, etc.), and these are independent of one’s setting follow-through as
one’s goal.

As far as I know, in all racquet sports players are told to follow-through: to
continue the racquet swing after the ball or shuttle have left the racquet.
But of course the ball or shuttle doesn’t care what the racquet is doing at
that point. So what’s the point of follow-through? The usual story is this: by
aiming to follow-through, one hits the ball or shuttle better. If one weren’t
trying to follow-through, the swing’s direction would be wrong or the swing
might slow down.

Maybe we should say this. Even if all intentional action is end-directed,
there are two kinds of reasons for an action: the reasons that come from the
value of the end and the reasons that come from the value of the pursuit of
that end. In the case of follow-through, there may be a fairly trivial success
value in the follow-through—a success value that comes from one’s exercise of
normative power in adopting the follow-through as one’s end—but that success
value provides only fairly trivial reasons. However, there can be
significantly non-trivial reasons for one’s pursuing that end, reasons
independent of that end.

Clearly the follow-through is _intended_ —it’s consciously planned, aimed at,
etc. But it need not be a means to anything one cares about in the game
(though, of course, in some cases it can be a means to impressing the
spectators or intimidating an opponent). But is it an end? It seems pointless
as an end!

Yet it seems that whatever is intended is intended as a means or an end. One
might reject this principle, taking follow-through to be a counterexample.

This is interesting action-theoretically. The follow-through appears
pointless, because the agent’s interest is in what happens _before_ the
follow-through, the impact’s having the right physical properties, and yet
there is surely no backwards causation here. But there not appear to be an
effective way to reliably secure these physical properties of the impact
except by trying for the follow-through. So the follow-through itself is
pointless, but one’s _aiming at_ or _trying for_ the follow-through very much
has a point. And here the order of causality is respected: one swings aiming
at the follow-through, which causes an impact with the right physical
properties, and the swing then continues on to the “pointless” follow-through.

How? Easy. I am now sitting, and God knows that. So, a part of God is the
belief that I am sitting. But I can destroy that belief of God’s by standing
up! For as soon as I stand up, the belief that I am sitting will no longer
exist. But on the view in question, God’s beliefs are parts of him. So by
standing up, I would bring it about that a part of God doesn’t exist.

I agree that one could hold 1 without 2. But my argument is against people who
hold the package of 1 and 2. Think here of process theologians, and their more
orthodox cousins.

I agree with Walter. It seems to me that 2 is more directly incompatible with
God's atemporality than with his lack of complexity as such. An atemporal God
who sees Creation across its history in toto as a block, whether He is simple
or complex, has only true beliefs about everything that has been or will be,
and about when each fact will be true for finite creatures who are temporal.
But his beliefs don't change.  
  
Although a God who changed with and experienced time such that his beliefs
also changed might be thus considered complex across time by having temporal
parts, that would seem to be assuming an A-theory of time for all (including
God), then importing a B perspective at the end, incoherently. So, simplicity
of essence (at any point in time) and atemporality could be distinguished I
suppose, such that a person could affirm both simplicity and temporality of
God. I'm not saying that any open theist holds this opinion, or should, but it
seems a logical possibility from within that framework.

Alex  
  
How does it follow from the idea that God is not simple that God’s beliefs
change as the reality they are about changes?  
  
If (2) is true, isn't that a problem for divine simplicity as well? For on
divine simplicity, God id identical to God's beliefs, so if (2) is true it
seems that God changes as the reality changes, which conradicts divine
simplicity as well as divine immutability.

God is not simple, and in particular God’s beliefs are proper parts of God.

I think one can hold that God's beliefs are accidents of his which are not
identical to him (rejecting a strong doctrine of divine simplicity) without
holding that they are "parts" of him in any sense that makes (3) or (4)
absurd. Even for created beings like us, the accident-substance relationship
does not appear to be at all the same as the part-whole relationship as common
sense conceives it. (I would find it weird to say that my beliefs are a part
of me in the same sense that my arm is a part of my body, for instance.)

And of course by standing up, I bring it about that a new divine belief
exists. So:

It depends on the details of how they think beliefs work. But suppose they
hold to this picture: for each proposition p that God knows, there is God's
act of believing p. Moreover, these opponents of simplicity think that God's
act of believing p is a different act for each different p that God believes.
These different acts are on that view something like accidents of God. Thus,
by raising one's arm one brings it about that God's belief that one's arm is
in a lowered state does not exist and that God's belief that one's arm is in a
raised state does exist, and so one destroys a part of God and creates a new
one. (It is a part of the package that I am criticizing that propositions are
tensed, and so it's not enough for God to eternally know that at t1 the arm is
lowered and at t2 the arm is raised.)

I'm not sure that the creaturely changes are necessarily "destroying" (or
creating) parts of God (beliefs) on the open theists' assumption. I think they
are determining them. I will assume open theists who believe God is maximally
knowledgeable, that is, knows all that can be known, which does not include
perfect knowledge of the future according to them.  
  
On this set of assumptions God will still, however, have perfect foreknowledge
of all possible outcomes as he can understand all possible cause/effect
branches as First Cause. He just doesn't know which path along the branching
options will eventuate before the time that they do. This means his knowledge
grows only in the sense that he is able to pick out which of each fully
foreseen branch is actualised. In other words, the only way his knowledge
grows is by the steady lengthening of a "track" through a foreknown "space" of
possible world-states over time. In such a scenario, creatures with free will
would determine the direction of the track and thus the precise determination
of God’s knowledge of it, but they would not be adding to or subtracting from
God's beliefs in any quantitative sense. The addition that would occur comes
about automatically as time flows, no matter what finite agents decide.  
  
Please note, I do not hold this position, but it seems to me it is not guilty
of the exact absurdity alleged here, whatever its other flaws.  

There seems to be a legitimate reason to allow evil choices to exist and to
allow those evil choices to affect other people. Seemingly, actions wouldn't
mean much (such as freely loving God) if we were all in cubicles and couldn't
actually interact with people negatively. Free love and free hatred seems to
be two sides of the same coin. I would also say that there are legitimate
reasons for suffering to exist. Perhaps the world would have less good in it
if there is no suffering. Would God permit suffering to allow more good? I
don't see a reason as to why He wouldn't. Theodicies and defenses are very
interesting, though it seems to me that, intuitively at least, most theodicies
seem to deal with the problem of trivial evils.

There is, however, a difference between the cases. Many great ordinary goods
that dwarf trivial evils, like the courage of a Socrates, are known to us. Few
if any finite goods that dwarf horrendous evils are known to us. Nonetheless,
if theism is true, it is very likely that such goods are possible. And since
the argument from evil is addressed against the theist, it seems fair for the
theist to invoke that hierarchy.

Perhaps not negatively, but certainly 'without love'. It seems to me that God
did not have to create you and thus God did not have to love you. If God were
forced to love you or loved you randomly, I wouldn't consider God to be a
moral being. I also wouldn't consider love to be a moral good if it were
forced. Therefore, it seems possible that evil actions are merely privations
of good actions, and thus one cannot choose to do "nothing" because "nothing"
is a lack of good, which seems to be some sort of evil. Perhaps one could make
a distinction between evils, but it seems that this might be a better
distinction that the one I previously drew.

Nobody seriously runs an argument against the existence of God from _trivial_
evils, like hangnails or mosquito bites.

However, if God exists, we would _expect_ there to be an unbounded upward
qualitative hierarchy of possible finite goods. God is infinitely good, and
finite goods are participations in God, so we would expect a hierarchy of
qualitatively greater and greater types of good that reflect God’s infinite
goodness better and better.

On the other hand, if we think of _horrendous_ evils, like the torture of
children, it is difficult to think of much greater goods. Maybe with
difficulty we can come up with one or two possibilities, but not enough to
make it _easy_ to suppose a justification for God’s permission of the evil in
terms of the goods.

Moreover, we might ask whether our ignorance of goods higher up in the
hierarchy of goods beyond the ordinary goods is not itself evidence against
the existence of such goods. Here, I think the answer is that it is very
little evidence. We would expect any particular finite being to be able to
recognize only a finite number of types of good, and thus the fact that there
are only a finite number of goods that we recognize is very little evidence
against the hypothesis of the upward hierarchy of goods.

Unknown  
  
I wouldn't say "forced" is the correct word here, but it is true that God
_necessarily_ loves you. So, in a sense, God _did/does_ have to love you.

Why not? Here is a hypothesis. There are so very many possible much greater
goods—goods qualitatively and not just quantitatively much greter—that it
would be easy to suppose that God’s permitting the trivial evil could promote
or enhance one of these goods to a degree sufficient to yield justification.

The reason why few people seriously run an argument from trivial evils is
simply because the case is much more obvious in the case of horrendous evil.  
But a truly opnipotent being should be able to accomplish Evert good without
any evil, unless this is logically impossible  
So, unless there is a convincing argument for why a certain good cannot
posdibly be accomplished without permitting evil, the default position should
be that it is possible.  

Unknown  
  
So, God's actions do not mean much if He can't interact with people
negatively?

So if God exists, we would expect there to be unknown possible finite goods
that are related to the known horrendous evils in something like the
proportion in which the known great finite goods are related to the known
trivial evils. Thus, if God exists, there very likely is  
an upward hierarchy of possible goods to which the horrendous evils of this
life stand like a mosquito bite to the courage of a Socrates. If we believe in
this hierarchy of goods, then it seems we should be no more impressed by the
atheological evidential force of horrendous evils than the ordinary person is
by the atheological evidential force of trivial evils.

1) I think the participationism explanation is problematic, since by that same
logic one could maybe deny one truly possesses being or goodness or value; one
could even deny one causes anything at all, concluding occasionalism. But we
do have our own being & goodness, so it's not the divine being & goodness in
us, per Aquinas.  
  
  
2) In fact, there's a real sense in which our actions & thereby
accomplishments are uniquely our own in a way they aren't God's precisely
because our actions are rooted in our secondary causality, which is distinct
from God's causality. And secondary causality being distinct from primary
causality, the causal responsibility for secondary causal acts is in the
creature, since the primary causality of God in sustaining our actions in
existence only determins them insofar as they exist, but we determine them
insofar as whether we cause them to occur (the occurrence of them is distinct
from their basic existence) and what we cause to occur.  
  
3) Additionally, there may be a difference between bragging and general pride
- some moral theology manuals seem to suggest being proud of one's
accomplishments isn't sinful, because that's distinct from bragging morally.
And one can't be proud of things one doesn't truly possess or cause in some
way.  
  
Of course, other sources say any form of pride is sinful, so this isn't a
clear issue it seems - but if one did take the route that some forms of pride
ARE morally okay, then this makes the thesis we don't possess anything,
whether being or our actions, unlikely.

Here’s another way to think about it. Given (1) and (3), we need an
explanation of how it is that humility is an appropriate attitude for a highly
accomplished individual. Classical theism’s doctrine of participation provides
such an explanation: all the efforts and all the accomplishments are not truly
theirs but a participation in God’s perfection.

Wesley:  
  
"by that same logic one could maybe deny one truly possesses being or goodness
or value"  
  
There is precedent for saying something like that. "No one is good but God
alone" (Mark 10:8).  
  
There has got to be a sense, and a very important one, in which what Jesus
says is true. But there is also a sense in which we are good--but only good-
by-participation.

This seems question-begging. It's not because highly accomplished individuals
have a lot to brag about that they can brag about everything.  
  
Suppose you think this is a great argument, you certainly have reason to brag
about it, but even if there is no God, you also have reason to be humble,
because you may have been the first to come up with this particular argument,
but you could only have built a successful argument because of your education,
because of what you have studied, read, even the way your parents raised you.  
Nothing we do is exclusively our own accomplishment.  
Now if you don't agree with this, you don't have grounds for premise (1),
because if something really is your own accomplishment, humililty is not
appropriate because there is nothing wrong with being proud of what you have
accomplished.  
  
  
  

@Alex I think an important nuance here is that in this specific view of
participation, goodness-by-participation isn't actually **true or intrinsic**
goodness. But this is problematic because we do have other verses affirming
the true goodness of creatures, such as Genesis 1 explicitly affirms this, as
does 1 Timothy 4 (first four verses on marriage & food) and Matthew 10
(sparrows).  
  
The main problem would be that this view of participationism _wouldn't just
make it impossible_ to take pride or "brag" about something we did (which I
can concede is wrong), but that it also makes it impossible to actually say we
are good or have value as well. In the **same manner & for the same reasons**
we can't take pride in or brag about our actions, we also can't affirm we are
**good,** or have being, and this seems to be a big problem with that specific
account.  
  
And it's also important to point out that Scripture often uses **hyperbolic
negation** or hyperbolic merism - God for example in Jeremiah 7:22 apparently
denies He ever commanded the Hebrews to do sacrifice, yet that's not a literal
negation but a hyperbolic one to point out that loving God is more important -
rhetorically DENYING one thing to point out the greater importance of another,
without intending to truly deny the importance of the secondary. Same thing
with loving Christ & hating one's parents - intentionally hyperbolic contrasts
that actually convey a hierarchy of love, but not pure exclusivity.  
  
So basing a very specific view of participationism (because not all models of
participationism would agree that we aren't actually truly good) on a phrase
that is likely using intentional rhetorical hyperbole is more speculative than
solid.

Alex  
  
I have already answered this in my first reply. For the individuals you
describe, humililty is not appropriate.

Here’s the thought behind premise (2). If there is no God, then highly
accomplished individuals have much to brag about. Many of their
accomplishments are primarily _theirs_.

Walter:  
  
Good point.  
  
I propose to revise premise 2 to read:  
2*. There are some highly accomplished individuals such that for them humility
is an appropriate attitude only if God exists.  
  
Then we don't even need premise 3.  
  
To respond to your point, now, I suggest that we think of highly accomplished
individuals who did not have many of the kinds of advantages you list, and who
were hindered in various ways (e.g., by racism, sexism, poverty, war, etc.).
While no doubt everyone got some help from other humans, in some cases a
realistic appraisal of the degree of that help, especially when combined with
the degree to which fellow humans hindered the individual, will not suffice
for humility to be appropriate *if* God does not exist.

Premise (1) is controversial. The ancient Greeks would have denied it. But I
think the reason they denied it is that they didn’t have the examples that the
Christian tradition does, highly attractive examples examples of accomplished
lives of great humility.

Suppose your end is irrationality. Is it really true that you _should_ adopt
the means to that, such as reasoning badly? Surely not! Instead, you should
reject the end.

I am inclined to think (1) is false if by “end” is meant the end the agent
actually adopts, as opposed to a natural end of the agent. If your ends are
sufficiently irrational, adopting means appropriate to them may be less
rational than adopting means inappropriate to them.

Unjust laws have no normative force, and stupid ends have no normative force,
either.

But what is wrong with being such that you adopt means inappropriate to your
ends is not necessarily the means—it could be the ends.

[Adapting](https://www.instructables.com/Playing-NES-Power-Pad-Games-in-
Emulation/) Dance Dance Revolution and other mat controllers to work as NES
Power Pad controllers for emulation.

The amount of things one person can do with enough time is insane. I wouldn't
believe you if you told me that one person can get two PhDs, teach classes in
metaphysics, run a blog with constantly new and exciting arguments and
positions, is called one of the foremost Christian philosophers currently
alive, and STILL has time to measure bicycle energy output. I am convinced
that Dr. Pruss is some sort of superhuman.

This is compatible with there being cases where it is bad for one to do the
right thing. Thus, refraining from stealing the money that one would need to
sign up for a class on virtue is right and noninstrumentally good, but if the
class is really effective then stealing the money might be instrumentally good
for one, though noninstrumentally ba.

The title of this post contradicts the title of [another recent
post](http://alexanderpruss.blogspot.com/2022/12/the-right-cannot-be-derived-
from-good.html), but the contents do not.

An action is right (respectively, wrong) if and only if it is
noninstrumentally good (respectively, bad) to do it.

I think (1) is something that everyone should accept. Even consequentialists
can and should accept (1) (though utilitarian consequentialists have too
shallow an axiology to make (1) true). But natural law theorists might add a
further claim to (1): the left-hand-side is true because the right-hand-side
is true.

There is a way to connect the right and wrong with the good and bad:

I think it does. I have been trying to defend a natural law account of
rationality on which just as our moral norms are given by what is natural for
the will, our epistemic norms are given by what is natural for our intellect.
And just as our will is the will of a particular kind of deliberative animal,
so too our intellect is the intellect of a particular kind of investigative
animal. And we expect a correlation between what a social animal’s nature
impels it to do and what is good for the social animal’s community. Thus, we
expect a degree of harmony between the norms of epistemic rationality—which on
my view are imposed by the nature of the animal—and the good of the community.

At the same time, the harmony need not be perfect. Just as there may be times
when the good of the community and the good of the individual conflict in
respect of non-epistemic flourishig, there [may be such
conflict](http://alexanderpruss.blogspot.com/2011/01/epistemic-self-sacrifice-
and-prisoner.html) in epistemic flourishing.

Moreover, note that it is very plausible that what range of variation of
priors is good for the community depends on the species of rational animal we
are talking about. Rational apes like us are likely more epistemically
cooperative than rational sharks would be, and so rational sharks would
benefit less from variation of priors, since for them the good of the
community would be closer to just the sum of the individual goods.

I am grateful to Anna Judd for pointing me to a possible connection between
permissivism and natural law epistemology.

Of course, that it would be _good_ for the community if some norm of
individual rationality obtained does not prove that the norm obtains.

It is epistemically better for the human community if human beings do not all
have the same (ur-) priors.

This could well be true because differences in priors lead to a variety of
lines of investigation, a greater need for effort in convincing others, and
less danger of the community as a whole getting stuck in a local epistemic
optimum. If this hypothesis is true, then we would have an interesting story
about why it would be good for our community if a range of priors were
rationally permissible.

Panteleology holds that teleology is ubiquitous. Every substance aims at  
some end.

Panteleology seems to be exactly what we would expect in a world created by
God. Everything _should_ glorify God.

The main objection to panteleology is the same as that to panpsychism: the
incredulous stare. I think a part of the puzzlement comes from the thought
that things that are neither biological nor artifactual “just do what they
do”, and there is no such thing as failure. But this seems to me to be a
mistake. Imagine a miracle where a rock fails to fall down, despite being
unsupported and in a gravitational field. It seems very natural to say that in
that case the rock failed to do what rocks should do! So it may be that away
from the biological realm (namely organisms and stuff made by organisms)
failure takes a miracle, but the logical possibility of such a miracle makes
it not implausible to think that there really is a directedness.

Panteleology is also entailed by a panpsychism that follows Leibniz in
including the ubiquity of “appetitions” and not just perceptions. And it seems
to me that if we think through the kinds of reasons people have for
panpsychism, these reasons extend to appetitions—just as a discontinuity in
perception is mysterious, a discontinuity in action-driving is mysterious.

That said, I think the quantum realm provides room for saying that things
don’t “just do what they do”. If an electron is in a mixed spin up/down state,
it seems right to think about it as having a directedness at a pure spin-up
state and a directedness at a pure spin-down state, and only one of these
directednesses will succeed.

Couldn't one relate teleology to causal powers and the possible effects they
could accomplish? Final causality exists as long as anything has a causal
power TOWARDS anything, and this directedness of power - without needing to be
active even - is itself a real example of teleology.  
  
For any agent that has the power to cause any effect in any way, it must be
directed towards that end at least insofar as any POWER only makes coherent
sense insofar as it has an EFFECT which it includes within itself and thereby
points.

I think many theists would admit that God has created every substance for a
purpose, but that'd be an extrinsic teleology, and would be less
controversial. You've said elsewhere that you think that every substance has a
teleology. Are you meaning to argue that every substance has intrinsic
teleology? That'd be more controversial.  
  
  
  

All facts about rightness and wrongness can be derived from descriptive facts,
facts about non-rightness value, and a small number of fundamental abstract
moral principles.

Consider the following thesis that both Kantians, utilitarians and New Natural
Law thinkers will agree on:

I think Utilitarians might try to say something like, "If governments took
kidneys, this would have a different psychological effect on society than if
they took 20% of our income. Because of this accidental feature of our
psychologies, it actually would be more harmful for them to take our kidneys,
and look here I can show how this extra harm is unjustifiable via my abstract
moral principles."

It is not wrong for the government to forcibly and non-punitively take 20% of
your lifetime income, but it is wrong for the government to forcibly and non-
punitively take one of your kidneys.

The restriction to non-rightness good and bad is to avoid triviality. By
“rightness value” here, I mean only the value that an action or character has
in virtue of its being right or wrong to the extent that it is.

If loss of a kidney were to impact one’s autonomy significantly more than loss
of 20% of your lifetime income, then again there would be some hope for a
derivation of (2). But whether loss of a kidney is more of an autonomy impact
than loss of 20% of income will differ from person to person.

I don’t have a good definition of “abstract moral principle”, but I want them
to be highly general principles about moral agency such as “Choose the greater
over the lesser good”, “Do not will the evil”, etc.

One might suppose that among the small number of fundamental abstract moral
principles one will have some principles about respect for bodily integrity. I
doubt it, though. Respect for bodily integrity is an immensely complex area of
ethics, and it is very unlikely that it can be encapsulated in a small number
of abstract moral principles. Respect for bodily integrity differs in very
complex ways depending on the body part and the nature of the relationship
between the agent and the patient.

I should note that the above argument fails against divine command theories.
Divine command theorists will say that about rightness and wrongness are
identified with descriptive facts about what God commands, and these facts can
be very rich and hence include enough data to determine (2). For the argument
against (1) to work, the “descriptive facts” have to be more like the facts of
natural science than like facts about divine commands.

I don’t think we can derive (2) in accordance with the strictures in (1). If a
kidney were a lot more valuable than 20% of lifetime income, we would have
some hope of deriving (2) from descriptive facts, non-rightness value facts,
and abstract moral principles, for we might have some abstract moral principle
prohibiting the government from forcibly and non-punitively taking something
above some value. But a kidney is not a lot more valuable than 20% of lifetime
income. Indeed, if it would cost you 20% of your lifetime income to prevent
the destruction of one of your kidneys, it need not be unreasonable for you to
refuse to pay. Indeed, it seems that either 20% of lifetime income is
incommensurable with a kidney, or in some cases it is more valuable than a
kidney.

The code uses [webpxmux.js](https://github.com/sumimakito/webpxmux.js), though
it was a little bit tricky because in-browser Javascript may not have enough
memory to store all the uncompressed images that webpxmux.js needs to generate
an animation. So instead I encode each frame to WebP using webpxmux.js,
extract the compressed ALPH and VP8 chunks from the WebP file, and store only
the compressed chunks, writing them all at the end. (It would be even better
from the memory point of view to write the chunks one by one rather than
storing them in memory, but a WebP file has a filesize in its header, and
that’s not known until all the compressed chunks have been generated. One
could get around this limitation by generating the video twice, but that would
be twice as slow.)

Since then, I wrote a [web-based tool](https://arpruss.github.io//webpanim)
for generating a WebP animation of a timer and text synchronized to a set of
times. The timer can be in seconds or tenths of a second, and you can specify
a list of text messages and the times to display them (or to hide them). You
can then overlay it on a video in Premiere Rush or Pro. There is alpha
support, so you can have a transparent or translucent background if you like,
and a bunch of fonts to choose from (including the geeky-looking Hershey font
that I used in my Python script.)

When making my [Guinness application record
video](http://alexanderpruss.blogspot.com/2022/12/a-new-but-uncertified-world-
record.html), I wanted to include a time display in the video and Guinness
also required a running count display. I ended up writing a [Python
script](https://gist.github.com/arpruss/3a705c859d4ea4a482c4e47e96a08f79)
using OpenCV2 to generate a video of the time and lap count, and overlaid it
with the main video in Adobe Premiere Rush.

Still, consider this. The judgment whether moral considerations override the
non-moral ones seems to be an eminently _moral_ judgment. It is the person
with _moral_ virtue who is best suited to figuring out whether such overriding
happens. But what happens if morality says that the moral considerations do
not override the _N_ -prohibition? Is that not a case of morality giving its
endorsement to the _N_ -prohibition, so that the _N_ -prohibition would rise
to the level of a moral prohibition as well? But if so, then that pushes us
back to the previous story where it is reasonable to take _N_ -considerations
to be subsumed into moral considerations.

But if there are cases like (1), there will surely also be cases where the
moral considerations in favor of _ϕ_ ing do not rise to the level of a
requirement, but are sufficient to override the _N_ -prohibition. In those
cases, presumably:

So far so good. Moral norms can override non-moral norms in two ways: by
creating a moral requirement contrary to the non-moral norms or by creating a
moral permission contrary to the non-moral norms.

But now consider this. What happens if the moral considerations are at an even
lower level, a level insufficient to override the _N_ -prohibition? (E.g.,
what if to save someone’s finger you would need to sacrifice your arm?) Then,
it seems:

People often talk of moral norms as overriding. The paradigm kind of case
seems to be like this:

However, there is another story possible. Perhaps in the case where the moral
considerations are at too low a level to override the _N_ -prohibition, we can
still have _moral_ permission to _ϕ_ , but that permission no longer overrides
the _N_ -prohibition. On this story, there are two kinds of cases, in both of
which we have moral permission, but in one case the moral permission comes
along with sufficiently strong moral considerations to override the _N_
-prohibition, while in the other it does not. On this story, moral requirement
always overrides non-moral reasons; but whether moral considerations override
non-moral considerations depends on the relative strengths of the two sets of
considerations.

I don’t want to say that all norms are moral norms. But it may well be that
all norms governing the functioning of the will are moral norms.

Cases of supererogation look like that: you are morally permitted to do
something contrary to prudential norms, but not required to do so.

But this would be quite interesting. It would imply that in the absence of
sufficient moral considerations in favor of _ϕ_ ing, an _N_ -prohibition would
automatically generate a _moral_ prohibition. But this means that the real
normative upshot in all three cases is given by morality, and the _N_ -norms
aren’t actually doing any independent normative work. This suggests strongly
that on such a picture, we should take the _N_ -norms to be simply a species
of moral norms.

where “ _N_ ” is some norm like that of prudence or etiquette. In this case,
the moral requirement of _ϕ_ ing overrides the _N_ -prohibition on _ϕ_ ing.
Thus, you might be rude to make a point of justice or sacrifice your life for
the sake of justice.

Take first the case where they are both perfect amoral egoists. Amoral egoists
don’t care about promises. So the fact that an amoral egoist promised to raise
the right hand is no evidence at all that they will raise the right hand,
unless there is something in it for them. But is there anything in it for
them? Well, if Bob raises his right hand, then there is something in it for
Alice to raise her right hand. But note that this conditional is true
_regardless_ of whether they’ve made any promises to each other, and it is
equally true that if Bob raises his left hand, then there is something in it
for Alice to raise her left hand.

Here's a further line of thought. Suppose that Alice and Bob are not fully
utilitarian, but they incorporate into their ethics (which they follow
perfectly) an anti-conventionalist element (and that's also a part of their
shared knowledge). Thus, they think that the fact that one has promised p is a
fairly weak reason, of degree epsilon, against performing p. Then, if
epsilon>0, then it seems that by their lights Alice and Bob should lift their
left hands rather than their right, if they promised to lift their right,
since their behavioral bias is anti-promissory, and each knows the other to
have such a bias. Very well. Now take the limit as epsilon (the strength of
the reason they think favors breaking promises) to zero. For every epsilon>0,
they should lift their left hands rather than their right. It seems to be a
reasonable continuity conclusion that for epsilon=0, they should either be
neutral between lifting their left hands rather than their right or should
still prefer the left, but definitely should not prefer their right. And yet
epsilon=0 is just the full utilitarian case.

They confer before the game and promise to one another to raise the right
hand. They go into their separate rooms. And what happens next?

In ordinary life, this problem doesn’t arise as much, because as long as at
least one person is more typical, and hence takes promises to have reason-
giving force, or if public opinion is around to enforce promise-keeping, then
the issue doesn’t come up. But I think there is a lesson here and in the
[previous post](http://alexanderpruss.blogspot.com/2022/12/utilitarianism-and-
communication.html): for many ordinary practice, the utilitarian is free-
riding on the non-utilitarians.

My feeling is it's not hard to solve -- as long as you don't place artificial
restrictions on what counts as a reason, in the way the utilitarian or egoist
does.

This means that in cases like this, with full transparency of behavioral
tendencies, utilitarians and amoral egoists will do well to brainwash or
hypnotize themselves into promise-keeping.

Suppose Alice and Bob are perfect utilitarians or perfect amoral egoists in
any combination. They are about to play a game where they raise a left hand or
a right hand in a separate booth, and if they both raise the same hand, they
both get something good. Otherwise, nobody gets that good. Nobody sees what
they’re doing in the game: the game is fully automated. And they both have
full shared knowledge of the above.

It is true that if Alice expects Bob to expect her to keep her promise, then
Alice will expect Bob to raise his right hand, and hence she should raise her
right hand. But since she’s known to be an amoral egoist, there is no reason
for Bob to expect Alice to keep her promise. And the same vice versa.

The promise is simply irrelevant here. It is true that in normal
circumstances, it makes sense for egoists to keep promises in order to fool
people into thinking that they have morality. But I’ve assumed full shared
knowledge of each other’s tendencies here, and so no such considerations apply
here.

What if they are utilitarians? It makes no difference. Since in this case both
always get the same outcome, there is no difference between utilitarians and
amoral egoists.

This is a fun puzzle. It seems like David Lewis’ convention work, or Thomas
Schelling’s coordination and focal points stuff must be relevant.

Alice and Bob are both perfect Bayesian epistemic agents and subjectively
perfect utilitarians (i.e., they always do what by their lights maximizes
expected utility). Bob is going to Megara. He comes to a crossroads, from
which two different paths lead to Megara. On exactly one of these paths there
is a man-eating lion and on the other there is nothing special. Alice knows
which path has the lion. The above is all shared knowledge for Alice and Bob.

If the above argument is correct—and I am far from confident of that, since it
makes my head spin—then we have an argument that in order for communication to
be possible, at least one of the agents must be convention-bound. One way to
be convention-bound is to think, in a way utilitarians don’t, that convention
provides non-consequentialist reasons. Another way is to be an akratic
utilitarian, addicted to following convention. Now, the possibility of
communication is essential for the utility of the kinds of social animals that
we are. Thus we have an argument that at least some subjective utilitarians
will have to become convention-bound, either by getting themselves to believe
that convention has normative force or by being akratic.

Suppose the lion is on the left path. What should Alice do? Well, if she can,
she should bring it about that Bob takes the right path, because doing so
would clearly maximize utility. How can she do that? An obvious suggestion:
Engage in a conventional behavior indicating a where the lion is, such as
pointing left and roaring, or saying “Hail well-met traveler, lest you be
eaten, I advise you to avoid the leftward leonine path.”

Here is a brief way to put it. For Alice and Bob, convention carries no weight
except as a predictor of the behavior of convention-bound people, i.e., people
who are not subjectively perfect utilitarians. It is shared knowledge between
Alice and Bob that neither is convention-bound. So convention is irrelevant to
the problem at hand, the problem of getting Bob to avoid the lion. But there
is no solution to the problem absent convention or some other tool unavailable
to the utilitarian (a natural law theorist might claim that mimicry and
pointing are _natural_ indicators).

Similarly, if Bob were a typical human being, he would have a habit of forming
his beliefs on the basis of testimony interpreted via established social
conventions absent reason to think one is being misinformed, and so Alice’s
engaging in conventional left-path lion-indicating behavior would lead Bob to
think there is a lion on the left, and hence to go on the right. And while it
woudl still be true that social convention has no normative force for Alice,
Alice would think have reason to think that Bob follows convention, and for
the sake of maximizing utility would suit her behavior to his. But Bob is a
perfect Bayesian. He doesn’t form beliefs out of habit. He updates on
evidence. And given that Alice is not a typical human being, but a
subjectively perfect utilitarian, it is unclear to me why her engaging in the
conventional left-path lion-indicating behavior is more evidence for the lion
being on the left than for the lion being on the right. For Bob knows that
convention carries no normative force for Alice.

If Alice were a typical human being, she would have a habit of using
established social conventions to tell the truth about things, except perhaps
in exceptional cases (such as the murderer at the door), and so her use of the
conventional lion-indicating behavior would correlate with the presence of
lions, and would provide Bob with evidence of the presence of lions. But Alice
is not a typical human being. She is a subjectively perfect utilitarian.
Social convention has no normative force for Alice (or Bob, for that matter).
Only utility does.

I put this as an issue about communication. But maybe it’s really an issue
about communication but coordination. Maybe the literature on repeated games
might help in some way.

But I’ve been trying really hard to figure out how is it that such a
conventional behavior would indicate to Bob that the lion is on the left path.

This is not a refutation of utilitarianism. Utilitarians, following Parfit,
are willing to admit that there could be utility maximization reasons to cease
to be utilitarian. But it is, nonetheless, really interesting if something as
fundamental as communication provides such a reason.

This is akin to the 'positive transfer' of skills, as discussed with the sport
science. So-called 'negative transfer', its opposite, is to be avoided at all
costs. For example, if I try to play squash to improve my racket skills, I
will ruin my tennis game. The racket skills are subtly different, due to the
'wrist flick' in the squash technique.

BTW, it may depend on how much one keeps to proper form in the different
racquet sports. Someone like me who plays lots of different racquet sports
(regularly: badminton; semi-regularly: tennis, racquetball, table tennis,
pickleball; used to do occasional squash until our university's one court
closed as nobody but my son and I played; used to do crossminton during
Covid), maybe at an upper beginner level, perhaps does not have enough good
form in any one of the sports for it to matter. I hadn't played much tennis
this fall, but I played a fair amount of badminton, and seemed to find my
tennis improved when I got back to it, maybe due to transfer of thinking about
things like "how do I hit the shuttle away from where my opponents are", or
maybe just due to general fitness improvement.

There are empirical indications that various skills and maybe even virtues are
pretty domain specific. It seems that being good at reasoning about one thing
need not make one good at reasoning about another, even if the reasoning is
formally equivalent.

I've wondered about such incompossible goods myself. Weightlifting can reduce
agility, for example, and while being tall is great for basketball, it is not
good for weightlifting. Various niches seem to exist. It does raise the
question of what a perfect human being consists of, however. Are some of these
"perfections" merely accidental in the sense that they exploit what are
normatively speaking (in relation to human nature) flaws? We know some sports
are actually bad for the human body, or neglect the overall health of the
body. Certain perfections seem to be perfections only in an analogical sense,
perhaps something along the lines of being a "good thief" or an "effective
deceiver".  
  
Where intelligence is concerned, I am tempted to argue against incompossible
goods either because intelligence isn't like that or for similar natural law
reasons, but even more strongly, especially on account of the centrality of
intelligence to humanity.  
  
Consider your favorite déformation professionnelle, which, I submit, is more
of a result of imprudence, habit, ignorance, lack of practice using other
methods, and even effeminacy and arrogance. Someone with a rigorous
philosophical education is less likely to try to pigeonhole reality into the
reductive and simplified straitjacket of our physical models in the manner of
at least some physicists who generally lack serious exposure to philosophy and
may even hold it in contempt out of ignorance. A physicist may also be tempted
to pigeonhole simply because of pride; if he isn't any good at metaphysics,
then his thoughts on a metaphysical subject matter aren't likely to be very
valuable or interesting, and that stings the prideful man accustomed to
feeling like a hotshot. There is also the threat of seeing one's own field put
in its methodological place, so to speak, deflating any pretensions to the
kind of ultimacy that metaphysics lays claim to. A competent physics may also
derive greater pleasure from exercising his specialized competence and choose
his methods simply on the basis of what feels good and now what is called for
by a problem. So here the question resurfaces: is there an incompossibility
between being a good physicist and a good metaphysician? I suspect there isn't
intrinsically, even if that is often the case which I suspect is rather a
result of how one's time is spent. But even if it is the case, because general
knowledge is superior and more worthy of human attention than specialized
knowledge, we could argue that competence in physics that occurs at the
expense of philosophical depth is, in fact, a kind of failure to attain human
excellence by failing to devote proportional attention and effort to the kinds
of knowledge that are most essential, and in doing so, risking intellectual
deformation in important and even necessary matters.

And I _have_ noticed some transfer of skills and maybe even of the virtue of
patience both between the various sports and between the sports and other
repetitive activities, such as grading. There is a distinctive feeling I have
when I am half-way through something, and where I am fairly confident I can
finish it, and a kind of relaxation past the half-way point where I become
more patient, and time seems to flow “better”. For instance, I can compare how
tired I feel half-way through a long set of climbs and how tired I feel half-
way through a 2 km swim, and the comparison can give me some strength. Similar
positive thinking can happen while grading, things like “I can do it” or
“There isn’t all that much left.” Though there are also differences between
the sports and the grading, because in grading the quality of the work matters
a lot more, and since I am not racing against myself so there is no point of a
burst of speed at the end if I find myself with an excess of energy. Pacing is
also much less important for grading.

I have no idea if anything like this transfer works for other people.

I think that's correct. The more one concentrates on good form, the more
detrimental negative transfer becomes. This also applies in martial arts like
Tae Kwon Do, on the self-defense side, whereby training to 'pull' kicks and
punches (i.e. to safeguard your training partner), negatively transfers to
real-life situations.

I do have a piece of anecdotal data, though. I’ve been doing some endurance-
ish sports. Nothing nearly like a marathon, but things like swimming 2-3 km,
or climbing for an hour, typically (but [not
always](http://alexanderpruss.blogspot.com/2022/12/a-new-but-uncertified-
world-record.html)) competing against myself.

So what should we say? One possibility is to say that there are _only_ reasons
of one type, say the moral. I find that attractive. Then benefits to yourself
also give you _moral_ reason to act, and so you simply have a moral reason to
spin the spinner. Another possibility is to say that in addition to moral and
prudential reasons there is some third class of “mixed” or “combination”
reasons.

Now, one might have technical worries about saturated nonmeasurable sets
figuring in decisions. I do. (E.g., see the Axiom of Choice chapter in my
infinity book.) But now instead of supposing saturated nonmeasurable sets,
suppose a case where an agent subjectively has literally no idea whether some
event _E_ will happen—has no probability assignment for _E_ whatsoever, not
even a ranged one (except for the full range from 0 to 1). The spinner landing
on a set believed to be saturated nonmeasurable might be an example of such a
case, but the case could be more humdrum—it’s just a case of extreme
agnosticism. And now suppose that the agent is told that if they so opt, then
they will get something nice on _E_ and a deserving stranger will get
something nice otherwise.

**Response:** That may be right in the simple case. But now imagine that the
“red” set is a saturated nonmeasurable subset of the spinner edge, and the
“green” set is also such. A saturated nonmeasurable subset has no reasonable
probability assignment, not even a non-trivial range of probabilities like
from 1/3 to 1/2 (at best we can assign it the full range from 0 to 1). Now the
reason-giving strength of a chancy outcome is proportionate to the
probability. But in the saturated nonmeasurable case, there is no probability,
and hence no meaningful strength for the red-based reason or for the green-
based reason. But there is a meaningful strength for the red-or-green moral-
cum-prudential reason. The red-or-green-based reason hence does not reduce to
two separate reasons, one moral and one prudential.

**Final remark:** The argument applies to any exclusive and exhaustive
division of reasons into “simple” (i.e., non-combination) types.

One might think that reasons for action are exhaustively and exclusively
divided into the moral and the prudential. Here is a problem with this.
Suppose that you have a spinner divided into red and green areas. If you spin
it and it lands into red, something nice happens to you; if it lands on green,
something nice happens to a deserving stranger. You clearly have reason to
spin the spinner. But, assuming the division of reasons, your reason for
spinning it is neither moral nor prudential.

**Objection:** The chance _p_ of the spinner landing on red is a prudential
reason and the chance 1 − _p_ of its landing on green is a moral reason. So
you have _two_ reasons, one moral and one prudential.

I think it is sometimes said that it is anachronistic to attribute to the
ancient Greeks the discovery that the square root of two is irrational,
because what they discovered was a properly _geometrical_ fact, that the side
and diagonal of a square are incommensurable, rather than a fact about real
numbers.

And because pure (Tarskian) geometry is decidable, while the theory of the
positive integers is not decidable, the positive integers are not definable in
terms of pure geometry, so we cannot eliminate the quantification over
positive integers. In fact, it is known that the rational numbers are not
definable in terms of pure geometry either, so neither the incommensurability
formulation nor theory irrationality formulation is a purely geometric claim.

I was thinking of the decidability of Th(N), where N is our "intended" model
of the naturals, not of the decidability of any particular recursive
axiomatization. (Th(N) is not recursively axiomatizable, of course.)

It is correct to say that the Greeks discovered an incommensurability fact.
But it is, I think, worth noting that this incommensurability fact is not
really geometric fact: it is a geometric-cum-arithmetical fact. Here is why.
The claim that two line segments are commensurable says that there are
positive integers _m_ and _n_ such that _m_ copies of the first segment have
the same length as _n_ copies of the second. This claim is essentially
arithmetical in that it quantifies over positive integers.

Don't you rather mean _"Th(something)"_ to be our "intended" model of
"something", such that "something" could be the naturals N, such that Th(N) is
our "intended" model of naturals N?!?  
  
Why isn't Th(N) recursively axiomattizaböe though?!?

About half-way through, I ducked into the storage area inside the rock and
changed to a dry shirt.

I was actually really stressed yesterday. I had already beaten the record
unofficially in practice, but with lots of people coming to watch (local
climbers, grad students and colleagues), I didn't want to disappoint.
Contributing to the stress was that the conditions changed from my practice
runs (the auto-belay was down from maintenance and I had to use a manual
belay; this involved lengthening the route slightly, increasing the length of
each set and decreasing the total count, as well as an effective increase in
my weight since the auto-belay subtracts a few pounds due to its spring-
loading), and that I hadn't been able to train on the particular route for
several weeks prior to Wednesday due to the auto-belay being down for
maintenance while on Wednesday's approximately half-distance-at-half-time
practice I was more tired at the end than I should have been due to poor
pacing.

I climbed in sets of 10. The planned pace was 8:18 per set and a 44-45 second
rest between sets (clock runs during rests,), averaging at 49.8 seconds per
climb including descent. I was always ahead of pace, and I occasionally took a
mini break at the mid-point time if I was too far ahead.

On the ground there was a sheet of paper with the start and end times of each
break printed in large letters (calculated by [this
script](https://gist.github.com/arpruss/10e364904dfcc19b043a594232e0acde)), as
well as the mid-point time for each set of 10 to keep me better on pace.

I just noticed that you mentioned capturing video footage for Guinness, so I
suppose that answers my prior question. I apologize for the oversight.
Congrats again!

In the morning I stress-baked pumpkin muffins for myself and the volunteers. I
had the muffins, water and loose chalk on a table for use during breaks.

You should make a living out of that, Alex.  
Would give you a lot less stress.

As we say in Australia, you are 'a gun'! Have you ever thought about writing
philosophically about climbing? Perhaps even just in the vaguely existential
vein that Murakami mines in his 'What I Talk About When I Talk About Running'.

Wow, I had no idea that their confirmation process was so extensive! I wonder
what purpose the "notable moments" list is meant to be playing, especially
since they already have the video itself.

A Kindle Fire running a pre-release version of my [Giant
Stopwatch](https://play.google.com/store/apps/details?id=omegacentauri.mobi.simplestopwatch&hl=en_US&gl=US)
app provided unofficial timing for audience to see and for my pacing. I had to
modify the app to have a periodic beep to meet Guinness's requirements of an
audible stop signal.

And now for something not very philosophical. Today, in front of two witnesses
and two timekeepers and with the help of Levi Durham doing an amazing feat of
belaying me for an hour, I beat the [Guinness World
Record](https://www.guinnessworldrecords.com/world-records/438677-greatest-
vertical-distance-climbed-on-an-artificial-climbing-wall-in-1-hour-indi) in
greatest vertical distance climbed in one hour on an indoor climbing wall. The
previous record was 928.5m and I did 1013.7m (with about half a minute to
spare). On Baylor's climbing wall, this involved 67 climbs divided into sets
of 10 (the last was 7), with about a minute of rest between sets (the clock
kept on running during the rest).

The route was a standard 5.7 grade for most of my training (including when I
unofficially beat the records), with Rock management kindly agreeing to keep
the route up for several months for me. For the final attempt, we added holds
to make the finish at the top of the wall, and changed three other holds to
easier ones. (Guinness has no route grade requirements.)

The top of the wall is 15.13 meters vertically from the ground (as measured by
a geology grad student), at 3.5 degree slab.

I spent over a whole day documenting things for Guinness. They want photos,
attempt video (with running count--there was more python scripting to generate
that, plus looking at the video to figure out the times of all the ascents),
statements from two witnesses and two timekeepers for the attempt, a
measurement statement from a "surveyor or other qualified person" (a geology
grad student in our case), a measurement video, two witnesses for the
measurement, an index to the photos, documentation of timekeeper and witness
qualifications, and a notable moments list for the video.  
  
On the bright side, all this means that their records are probably pretty
reliable.  
  
It's now submitted, and they should respond in three months. Fingers crossed.
For GBP 500, they can do a rush review in five days, but why bother?

I wore moderately worn (one small hole) and comfortable 5.10 Anasazi shoes, a
Camp USA Energy harness, shorts and a T-shirt. (I have not received any
sponsorship.) My belayer used a tube-style device and wore belay gloves.

I trained for about three months, not very heavily. In training did two
unofficial full-length practice runs, and in each I beat the previous record:
in the first one I got 947.1 meters and in the second I got 1004.5, so I was
pretty confident I could beat the 928.5 meters on the official attempt (though
I was still pretty nervous). I also trained by doing a small number of
approximately 1/2 or 1/3 sized practices (maybe three or so), and more regular
shorter runs (1-10 climbs) at fast pace.

Most of my practice was with an auto-belay, and at a shorter distance per
climb (and hence greater number of climbs needed) since the auto-belay makes
it impossible to get to the top of the wall. The auto-belay is also spring
loaded so it effectively decreases body weight (by 7 lbs at the bottom
according to my measurement). Then a couple of weeks ago the auto-belay was
closed by management due to a maintenance issue, and I had a break in training
until the Wednesday before the official attempt when I trained with a manual
belay.

I am guessing that if one is doing one of the longer records, say an 8 hour
one, they aren't going to want to watch all of the video, except maybe at high
speed, and so looking at notable moments might make sense.  
  
I expect they also choose some small fraction of the records to feature on
social media, and I could see them using the notable moments list to extract
video for that.  
  
I pity those who don't have programming skills, though. Inserting a running
count of laps into a video by manually inserting a title into the video at
each point with a video editor program would be as much an endurance sport as
actually doing the laps. It took me a while to type up the times of all the
climb tops, but after that I just had the computer automatically generate a
video track for the inset window with time and counts.

Since Guinness requires video proof in addition to human witnesses, in the
interests of redundancy, I had three cameras pointed at the attempt. The best
footage (above) is from a Sony A7R2 with a zoom lens at 16mm, producing 1080P
at 59.94 fps. Video was processed with Adobe Premiere Rush. The processing
consisted of trimming the start and end, and adding a timing video track I
generated with a [Python OpenCV2
script](https://gist.github.com/arpruss/3a705c859d4ea4a482c4e47e96a08f79),
synchronized with single-frame precision at the 1:00:00 point with the footage
of Giant Stopwatch (barely visible under the table towards the end of the
video; early in the video, glare hides it). For the unofficial version I link
above, I accelerated the middle climbs 10X in Premiere Rush.

I’ve been wondering whether it is possible for a country to count as pacifist
and yet wage a defensive war. I think the answer is positive, as long as one
has a moderate pacifism that is opposed to lethal violence but not to all
violence. I think that a prohibition of all violence is untenable. It seems
obvious that if you see someone about to shoot an innocent person, and you can
give the shooter a shove to make them miss, you presumptively should.

Second, “lethal” weapons can be used less than lethally. For instance, with
modern medicine, abdominal gunshot wounds are only 10% fatal, yet they are no
doubt very effective at stopping an attacker. While it may seem weird to
imagine a pacifist shooting someone in the stomach, when the chance of
survival is 90%, it does not seem unreasonable to say that the pacifist could
be aiming to stop the attacker non-lethally. After all, tasers sometimes kill,
too. They do so less than 0.25% of the time, but that’s a difference of degree
rather than of principle.

Third, we might subdivide moderate pacifists based on whether they prohibit
all violence that foreseeably leads to death or just violence that
intentionally leads to death. If it is only intentionally lethal violence that
is forbidden, then quite a bit of modern warfare can stand. If the enemy is
attacking with tanks or planes, one can intentionally destroy the tank or
plane as a weapon, while only foreseeing, without intending, the death of the
crew. (I don’t know how far one can take this line without sophistry. Can one
drop a bomb on an infantry unit intending to smash up their rifles without
intending to kill the soldiers?) Similarly, one can bomb enemy weapons
factories.

Whether such a limited way of waging war could be successful probably depends
on the case. If one combined the non-lethal (or not intentionally lethal)
means with technological and numerical superiority, it wouldn’t be surprising
to me if one could win.

First, we have “officially” non-lethal weapons: tasers, gas, etc. Some of
these might violate current international law, but it seems that a pacifist
country could modify its commitment to some accords.

Imagine a moderate pacifist who rejects lethal self-defense, but allows non-
lethal self-defense when appropriate, say by use of tasers.

If you can tase one person to stop the murder of ten, then (1) should be
permissible if it’s the only option. But tasers occasionally kill people. We
don’t know how often. Apparently it’s [less than 1 in
400](https://www.usatoday.com/in-depth/news/investigations/2021/04/23/police-
use-tasers-ends-hundreds-deaths-like-daunte-wright/7221153002/) uses. Suppose
it’s 1 in 4000. Then option (1) results in 250 enemy deaths.

Now, imagine that one person is attacking you and nine other innocents, with
the intent of killing the ten of you, and you can stop them with a taser.
Surely you should, and surely the moderate pacifist will say that this is an
appropriate use case for the taser.

Alex  
  
Doesn't you post here entail that we should murder Putin?

One should never murder anyone. Killing, on the other hand, can sometimes be
justified.  
  
One question about killing the leader of an invading country is whether the
leader counts as a civilian, since the killing of civilians is forbidden by
the Geneva Convention. There is also some worry about the Geneva Convention's
killing by "perfidy", which is taken to rule out at least some assassinations.
So, as a matter of positive international law, it seems a difficult question.  
  
Were there no international law on the matter, I wouldn't see a significant
difference between killing a political leader and killing a general, if both
are giving orders to fight. Morally speaking, but not necessarily in
international law, both seem to me to be equally combatants.  
  
Besides the moral and legal questions, there is also a prudential question. In
my post I assumed that killing the general would stop the invasion. I got to
assume that because I was making up the case. Whether in actual fact killing a
leader would stop an invasion is less clear. Indeed such a thing might be seen
as such a serious attack on the country that the retaliation might be really
horrific.

These may seem to be consequentialist arguments. I don't think so. I don't
have the same intuitions if we replace the general by the general's innocent
child in (2) and (4), even if killing the child were to stop the war (e.g., by
making the general afraid that their other children would be murdered).

Alex  
  
Of course one should never murder anyone, but the question is: would killing
Putin be murder?

Note that a version of this argument goes through even if the moderate
pacifist backs up and says that tasers are too lethal. For suppose instead of
tasers we have drones that destroy the dominant hand of an enemy soldier while
guaranteeing survival (with science fictional medical technology). It’s
clearly right to release such a drone on a soldier who is about to kill ten
innocents. But now compare:

I think (4) is still morally preferable to causing the kind of disruption to
the lives of a million people that plan (3) would involve.

Very well. Now consider this on a national level. Suppose there are a million
enemy soldiers ordered to commit genocide against ten million, and you have
two ways to stop them:

So maybe our choice is between tasing a million, thereby non-intentionally
killing 250 soldiers, and intentionally killing one general. It seems to me
that (2) is morally preferable, even though our moderate pacifist has to allow
(1) and forbid (2).

Perhaps the real problem for a lot of people with a causal view of normative
powers is that it tends to lead to a violation of supervenience. For if it is
metaphysically possble to have the exercise of the normative power without the
exercise of the natural power, or vice versa, then it seems we don’t have
supervenience of the normative on the non-normative. But supervenience does
not seem to me to be inescapable.

But why not allow for a causal model? Why not suppose that a normative power
is a causal power to make an irreducible normative property come to be
instantiated in someone? Thus, my power to promise is the power to cause
myself to be obligated to do what I have promised.

Maybe the answer to both questions is that I could, but only metaphysically
and not causally. In other words, it could be that the laws of nature, or of
human nature, make it impossible for me to exercise one of the powers without
the other, just as I cannot wiggle my ring finger without wiggling my middle
finger as well. On this view, if there is a God, he could cause me to acquire
promissory-type obligations without my promising, and he could let me engage
in the natural act of promising while blocking the exercise of normative power
and leaving me normatively unbound. This doesn’t seem particularly
problematic.

There are two versions of the above model. On one version, there is an
underlying fundamental conditional normative fact _C_ , such as that if I have
promised something then I should do it, and my exercise of normative power
supplies the antecedent _A_ of that conditional, and then the normative
consequent of _C_ comes to be grounded in _C_ and _A_. On another version,
there there are some natural acts that are directly constitutive of a
normative state of affairs, not merely by supplying the antecedent of a
conditional normative fact. I think the first version of the model is the more
plausible in paradigmatic cases.

A normative power is a power to change a normative condition.
[Raz](https://core.ac.uk/download/pdf/230182259.pdf) says the change is not
produced “causally” but “normatively”.

I think the difficulty with a causal model is the fact that in paradigm cases
of normative power, there is a natural power that _is_ being exercised, and we
have the intuition that the exercise of the natural power is necessary and
sufficient for the normative effect. But on a causal model, why couldn’t I
cause a promissory-type obligation without promising, simply causing the
relevant property of being obligated to come to be instantiated in me? And why
couldn’t I engage in the speech act while yet remaining normatively unbound,
because my normative power wasn’t exercised in parallel with the natural
power?

Here is a picture on which this is correct. We exercise a normative power by
exercising a natural power in such a context that the successful exercise of
the natural power is partly constitutive of a normative fact. For instance, we
utter a promise, thereby exercising a natural power to engage in a certain
kind of speech act, and our exercise of that speech act is partly constitutive
of, rather than causal of, the state of affairs of our being obligated to
carry out the promised action.

**Case 1:** There is some device which does something useful when you trigger
it. It is triggered by electrical activity. You strap it on to your arm, and
raise your arm, so that the electrical activity in your muscles triggers the
device. Your raising your arm has the arm going up as an end, but that end is
not perceived as good, but merely neutral. All you care about is the
electrical activity in your muscles.

In both cases, it is still true that the agent acts for a good end—the useful
triggering of the device and the production of the cake. But in both cases it
seems they are also acting for a worthless end. Thus the cases seem to fit
with the weak but not the strong guise of the good thesis.

For the strong version to have any plausibility, “good” must include cases of
purely instrumental goodness.

But perhaps we can say this. We have a normative power to endow some neutral
things with value by making them our ends. And in fact the only way to act for
an end that does not have any independent value is by exercising that
normative power. And exercising that normative power involves your seeing the
thing you’re endowing with value as valuable. And maybe the only way to raise
your arm or for Bob to bake the cake in the examples is by exercising the
normative power, and doing so involves seeing the end as good. Maybe. This has
some phenomenological plausibility and it would be nice if it were true,
because the strong guise of the good thesis is pretty plausible to me.

**Weak** : Whenever you act, you act for an end that you perceive is good.

**Case 2:** Back when they were dating in high school, Bob promised to try his
best to bake a nine-layer chocolate cake for Alice’s 40th birthday. Since
then, Bob and Alice have had a falling out, and hate each other’s guts.
Moreover, Alice and all her guests hate chocolate. But Alice doesn’t release
Bob from his promise. Bob tries his best to bake the cake in order to fulfill
his promise, and happens to succeed. In trying to bake the cake, Bob acted for
the end of producing a cake. But producing the cake was worthless, since no
one would eat it. The only value was in the trying, since that was the
fulfillment of his promise.

I was going to leave it at this. But then I thought of a way to save the
strong guise of the good thesis. Success is valuable as such. When I try to do
something, succeeding at it has value. So the arm going up or the cake being
produced _are_ valuable as necessary parts of the success of one’s action. So
perhaps every end of your action _is_ trivially good, because it is good for
your action to succeed, and the end is a (constitutive, not causal) means to
success.

**Strong** : Whenever you act, you act for an end, and every end you act for
you perceive as good.

I think there is still reason to be sceptical of the strong version.

According to the guise of the good thesis, one always acts for the sake of an
apparent good. There is a weaker and a stronger version of this:

This isn’t quite enough for a defense of the strong thesis. For even if the
success is good, it does not follow that you perceive the success as good. You
might subscribe to an axiological theory on which success is not good in
general, but only success at something good.

If you lose, but you tried to win, she pays you double what you lost.

Is this possible? I think so. We just need to
[distinguish](http://alexanderpruss.blogspot.com/2019/08/two-ways-to-pursue-y-
for-sake-of-z.html) between pursuing victory for the sake of something else
that follows from victory and pursuing victory for the sake of something that
might follow from the pursuit of victory.

Clearly the prudent thing to do is to try to win. For if you don’t try to win,
then you are guaranteed not to get any money. But if you do try, you won’t
lose anything, and you might gain.

Here is the oddity: you are trying to win in order to get paid, but you only
get paid if you don’t win. Thus, you are trying to achieve something, the
achievement of which would undercut the end you are pursuing.

Suppose Alice can read your mind, and you are playing poker against a set of
people not including Alice. You don’t care about winning, just about money.
Alice has a deal for you that you can’t refuse.

If we think that it’s never rational for a rational agent to refuse free
information, then the above argument can be made rigorous to establish that
any discontinuous rise in the epistemic value of credence at the point at
which knowledge of a truth is reached is exactly mirrored by a discontinuous
fall in the epistemic value of a state of credence where seeming-knowledge of
a falsehood is reached. Moreover, the rise and the fall must be in the ratio 1
− _r_ : _r_ where _r_ is the knowledge threshold. Note that for knowledge, _r_
is plausibly pretty large, around 0.95 at least, and so the ratio between the
special value of knowledge of a truth and the special disvalue of evidence-
sufficient-for-knowledge for a falsehood will need to be at most 1:19. This
kind of a ratio seems intuitively implausible to me. It seems unlikely that
the special disvalue of evidence-sufficient-for-knowledge of a falsehood is an
order of magnitude greater than the special value of knowledge. This
contributes to my scepticism that there is a special value of knowledge.

Suppose that _a_ and _b_ are positive and _a_ / _b_ = (1− _r_ )/ _r_. Then if
my scribbled notes are correct, it is straightforward but annoying to check
that _s_ 1 is proper, and it has a discontinuous reward _a_ for meeting
threshold _r_ with respect to a truth and a discontinuous penalty  − _a_ for
meeting threshold _r_ with respect to a falsehood. To get a strictly proper
scoring rule, just add to it any strictly proper continous accuracy scoring
rule (e.g., Brier).

But that’s not quite right. For from Alice’s point of view, because the
threshold for knowledge is not 1, there is a real possibility that _p_ is
false. But it may be that just as there is a discontinuous gain in epistemic
value when your (rational) credence becomes sufficient for knowledge of
something that is in fact true, it may be that there is a discontinuous loss
of epistemic value when your credence becomes sufficient for knowledge of
something false. (Of course, you can’t know anything false, but you can have
evidence-sufficient-for-knowledge with respect to something false.) This is
not implausible, and given this, by looking at the information, by her lights
Alice also has a chance of a significant gain in value due to losing the
illusion of knowledge in something false.

Can we rigorously model this kind of an epistemic value assignment? I think
so. Consider the following discontinuous accuracy scoring rule _s_ 1( _x_ ,
_t_ ), where _x_ is a probability and _t_ is a 0 or 1 truth value:

This hypothesis initially seemed to me to have an unfortunate consequence.
Suppose Alice has just barely exceeded the threshold for knowledge of _p_ ,
and she is offered a cost-free piece of information that may turn out to
slightly increase or slightly decrease her overall evidence with respect to
_p_ , where the decrease would be sufficient to lose her knowledge of _p_
(since she has only “barely” exceeded the evidential threshold for knowledge).
It seems that Alice should refuse to look at the information, since the
benefit of a slight improvement in credence if the evidence is non-misleading
is outweighed by the danger of a significant and discontinuous loss of value
due to loss of knowledge.

_s_ 1( _x_ , _t_ ) = _a_ if _r_ < _x_ and _t_ = 1 or if _x_ < 1 − _r_ and _t_
= 0

Plausibly, then, if we imagine Alice has some evidence for a truth _p_ that is
insufficient for knowledge, and slowly and continuously her evidence for _p_
mounts up, when the evidence has crossed the threshold needed for knowledge,
the value of Alice’s state with respect to _p_ will have suddenly and
discontinuously increased.

_s_ 1( _x_ , _t_ ) = − _b_ if _r_ < _x_ and _t_ = 0 or if _x_ < 1 − _r_ and
_t_ = 1.

Suppose there is a distinctive and significant value to knowledge. What I mean
by that is that if two epistemic are very similar in terms of truth, the level
and type of justification, the subject matter and its relevant to life, the
degree of belief, etc., but one is knowledge and the other is not, then the
one that is knowledge has a significantly higher value because it is
knowledge.

_s_ 1( _x_ , _t_ ) = 0 if 1 − _r_ ≤ _x_ ≤ _r_

What does this tell us about _r_ and _α_? We can actually figure this out.
Consider a test for _p_ that have no false negatives, but has a false positive
rate of _β_. Let _E_ be a positive test result. Our best bet to generating a
counterexample to (a)–(c) will be if the priors for _p_ are as close to _r_ as
possible while yet definitely below, i.e., if the priors for _p_ are _r_ −
_α_. For making the priors be that makes (c) easier to definitely satisfy
while keeping (b) definitely satisfied. Since there are no false negatives,
the posterior for _p_ will be:

In [a recent post](http://alexanderpruss.blogspot.com/2023/01/knowing-you-
will-soon-have-enough.html), I noted that it is possible to cook up a Bayesian
setup where you don’t meet some threshold, say for belief or knowledge, with
respect to some proposition, but you do meet the same threshold with respect
to the claim that after you examine a piece of evidence, then you _will_ meet
the threshold. This is counterintuitive: it seems to imply that you can know
that you will have enough evidence to know something even though you don’t
yet. In a comment, Ian noted that one way out of this is to say that beliefs
do not correspond to sharp credences. It then occurred to me that one could
use the setup to probe the question of how sharp our credences are and what
the thresholds for things like belief and knowledge are, perhaps
complementarily to the considerations in [this
paper](https://philarchive.org/rec/PRUBSA).

We thus have our joint constraint on the squishiness of our credences: bad
things happen if our credences are so precise as to make (6) true with respect
to a threshold _r_ for which we don’t want (a)–(c) to definitely hold. The
easiest scenario for making (a)–(c) definitely hold will be a binary test with
no false negatives.

But what’s a reasonable threshold for belief? Maybe something like 0.9 or
0.95. At _r_ = 0.9, the squishiness needed for paradox is _α_ = 0.046. I
suspect our credences are more precise than that. If we agree that the
squishiness of our credences is less than 4.6%, then we have an argument that
the threshold for belief is _more_ than 0.9. On the other hand, at _r_ = 0.95,
the squishiness needed for paradox is 2.4%. At this point, it becomes more
plausible that our credences lack that kind of precision, but it’s not clear.
At _r_ = 0.98, the squishiness needed for paradox dips below 1%. Depending on
how precise we think our credences are, we get an argument that the threshold
for belief is something like 0.95 or 0.98.

Note that by appropriate choice of _β_ , we can make _z_ be anything between
_r_ − _α_ and 1, and the right-hand-side of (3) is at least _r_ − _α_ since
_r_ \+ _α_ ≤ 1. Thus we can make (c) definitely hold as long as the right-
hand-side of (3) is bigger than or equal to the right-hand-side of (4), i.e.,
if and only if:

(We definitely won’t meet _r_ on a negative test result.) Thus to get (c)
definitely true, we need (3) to hold as well as the probability of a positive
test result to be at least _r_ \+ _α_ :

For suppose we have a credence threshold _r_ and that our intuitions agree
that we can’t have a situation where:

Note that the squishiness of our credences likely varies with where the
credences lie on the line from 0 to 1, i.e., varies with respect to the
relevant threshold. For we _can_ tell the difference between 0.999 and 1.000,
but we probably can’t tell the difference between 0.700 and 0.701. So the
squishiness should probably be counted relative to the threshold. Or perhaps
it should be correlated to log-odds. But I need to get to looking at grad
admissions files now.

_P_ ( _p_ | _E_ ) = _P_ ( _p_ )/ _P_ ( _E_ ) = ( _r_ − _α_ )/( _r_ − _α_ + _β_
(1−( _r_ − _α_ ))).

We thus have our joint constraint on the squishiness of our credences: bad
things happen if our credences have a level of precision equal to the right-
hand-side of (6). What exactly that says about _α_ depends on where the
relevant threshold lies. If the threshold _r_ is 1/2, the squishiness _α_ is
0.15. That’s surely higher than the actual squishiness of our credences. So if
we are concerned merely with the threshold being more-likely-than-not, then we
can’t avoid the paradox, because there will be cases where our credence is
definitely below the threshold and it’s definitely above the threshold that
examination of the evidence will push us about the threshold.

we meet _r_ with respect to the proposition that we will meet the threshold
with respect to _p_ after we examine evidence _E_.

Let _z_ = _r_ − _α_ \+ _β_ (1−( _r_ − _α_ )) = (1− _β_ )( _r_ − _α_ ) + _β_.
This is the prior probability of a positive test result. We will definitely
meet _r_ on a positive test result just in case we have ( _r_ − _α_ )/ _z_ =
_P_ ( _p_ | _E_ ) ≥ _r_ \+ _α_ , i.e., just in case

It’s in fact not hard to see that (6) is necessary and sufficient for the
existence of a case where (a)–(c) definitely hold.

Let _α_ > 0 be the “squishiness” of our credences. Let’s say that for one
credence to be definitely bigger than another, their difference has to be at
least _α_ , and that to definitely meet (fail to meet) a threshold, we must be
at least _α_ above (below) it. We assume that our threshold _r_ is definitely
less than one: _r_ \+ _α_ ≤ 1.

Now, the state, just like individuals, should _ceteris paribus_ avoid causing
grave harm. Hence, the state should generally avoid getting people to do
things that violate their conscience in major matters.

In cases where doing wrong and suffering wrong are of roughly the same order
of magnitude, it is very intuitive that we should prevent the suffering of
wrong rather than the doing of wrong. Imagine that Alice is drowning while at
the same time Bob is getting ready to assassinate a politician, but we know
for sure that Bob’s bullets have all been replaced with blanks. If our choice
is whether to try to dissuade Bob from attempting murder or keep Alice from
drowning, we should keep Alice from drowning, evne if on the Socratic view the
harm to Bob from attempting murder will be greater than that to Alice from
drowning. (I am assuming that in this case the two harms are nonetheless of
something like the same order of magnitude.)

In some cases the person of mistaken conscience will still do the wrong deed
despite the law’s contrary incentive. In such a case, both the perpetrator and
the victim may be slightly better off for the law. The victim has a dignitary
benefit from the very fact that the state says that the harm was unlawful.
That dignitary benefit may be a cold comfort if the victim suffered a grave
harm, but it is still a benefit. And the perpetrator is slightly better off,
because following one’s conscience _against_ external pressure has an element
of admirability even when the conscience is mistaken.

The harm of violating one’s conscience only happens to one if one _willingly_
violates one’s conscience. If law enforcement physically prevents me from
doing something that conscience requires from me, then I haven’t suffered the
harm. Thus, interestingly, the consideration I sketched against violating
one’s conscience does not apply when one is literally forced (fear of
punishment, unless it is severe enough to suspend one’s freedom of will, does
not actually _force_ , but only incentives).

A reasonable optimism says that in most cases most people’s consciences are
correct. Thus typically we would expect that most violators of a legitimate
law will not be acting out of conscience—for a necessarily condition for the
legitimacy of a law is that it does not conflict with a correct conscience.
Thus, even if there is the rare murderer acting from mistaken conscience, most
murderers act against conscience, and by incentivizing abstention from murder,
in most cases the law _helps_ people follow their conscience, and the small
number of other cases can be tolerated as a side effect. Thus the
considerations of conscience favor intolerant laws in such cases. Nonetheless,
there are cases where most violators of a law would likely be acting from
conscience. Thus, if we had a law requiring eating meat, we would expect that
most of the violators would be conscientious. Similarly, a law against
something—say, the wearing of certain clothes or symbols—that is rarely done
except as a religious practice would likely be a law most violators of which
would be conscientious.

The difficult case, however, is when people’s consciences are mistaken to such
a degree that conscience requires them to do something that unjustly harms
others. (A less problematic mistake is when conscience is mistaken to such a
degree that conscience requires them to do something that’s permissible, but
not wrong. In those cases, tolerance is clearly called for. We shouldn’t
pressure vegetarians to eat animals even if their conscientious objection to
eating animals happens to be mistaken.)

One of the central insights of Western philosophy, beginning with Socrates,
has been that few if any things are as bad for an individual as culpably doing
wrong. It is better, we are told through much of the Western philosophical
tradition, that it is better to suffer than do injustice.

In a just defensive war, to refuse to fight to defend one’s fellow citizens
without special reason (perhaps priests and doctors should not kill) is wrong.
But a grave harm is done to a conscientious objector who is gotten to fight by
legal incentives. Let’s think through the five considerations above. The first
mainly applies to laws prohibiting a behavior rather than ones requiring a
behavior. Short of brainwashing, it is impossible to _make_ someone fight. (We
could superglue their hands to a gun, and then administer electric shocks
causing their fingers to spasm and fire a bullet, but that wouldn’t count as
_fighting_.) The second applies somewhat: we do need to weigh the harms to
innocent citizens from enemy invaders, harms that might be prevented if our
conscientious objector fought. But note that there is something rather
speculative about these harms. Someone who fights contrary to conscience is
unlikely to be a very effective fighter, and it is far from clear that their
military activity would actually prevent any actual harm to innocents. Now,
regarding the third consideration, one can design a conscription law with an
exemption that few who aren’t conscientious objectors would take advantage of.
One way to do this is to require evidence of one’s conscience’s objection to
fighting (e.g., prior membership in a pacifist organization). Another way is
to impose non-combat duties on conscientious objectors that are as onerous and
maybe as dangerous as combat would be. Regarding the fourth consideration, it
seems unlikely that a typical conscientious objector’s objections to war would
be changed by legal penalties. And the fifth seems a weak consideration in
general. Putting all these together, we do not outweigh the _prima facie_
considerations against pressuring conscientious objectors to act against their
(mistaken) conscience from the harms in going against conscience.

Nonetheless, there will be cases where these considerations do not suffice,
and the law should be tolerant of mistaken conscience.

Now, acting against one’s conscience _is_ always wrong, and is almost always
_culpably_ wrong. For the most common case when doing something wrong isn’t
culpable is that one is ignorant of the wrongness, but when one acts against
one’s conscience one surely isn’t ignorant that one is acting against
conscience, and that we ought follow our conscience is obvious.

That said, I think a qualification is plausible. Some wrongdoings are minor,
and in those cases the harm to the wrongdoer may be minor as well. But in any
case, to get someone to act against their conscience in a matter that
according to their conscience is major is to do them grave harm, a harm not
that different from death.

One might think that what I said earlier implies that in this difficult case
the state should always allow people to follow their conscience, because after
all it is worse to do wrong—and violating conscience is wrong—than to have
wrong done to one. But that would be absurd and horrible—think of a racist
murderer whose faulty conscience requires them to kill.

When someone’s conscience mistakenly requires something that violates an
objective moral rule, there is a two-fold benefit to that person from a law
incentivizing following the moral rule. The law is a teacher, and the state’s
disapproval may change one’s mind about the matter. And even if it a harm to
one to violate conscience, it is also a harm to one to do something wrong even
inculpably. Thus, the harm of violating conscience is somewhat offset by the
benefit from not doing something else that is wrong.

Ian:  
  
I don't think the worries about sharpness of credences apply. For we can
suppose that the cases we apply the argument to are precisely ones with
sufficiently sharp credences. They might be cases concerning gambling
scenarios, or they could be medical cases where the only relevant data one has
is statistics from the most recent publications on the subject.  
  
I think all we really need for the argument is that in certain kinds of cases
meeting a credence threshold is what makes the difference between not having a
belief (or a justified belief) and having a belief (or a justified belief).  
  
By the way, suppose we think that the argument doesn't work for sharpness
reasons. Let's say that u is a level of sharpness such that it only makes
sense to say that you've met the threshold if you are at r+u or higher and
that you haven't met it if you are at r-u or lower, and if you're between r-u
and r+u it's indeterminate. Then working through the Bayesian details should
provide one with an interesting joint constraint on r and u. And then we can
ask see if it's reasonable to think that r and u actually satisfy the joint
constraint.

As long as the test could be a true negative, you could always be surprised
and update to P of 0.0 if the test is negative. So waiting on the test before
you update is justified.

In other words, oddly enough, just prior to getting the test results I can
reasonably say:

I’d take this as an argument against the view that belief is (or is justified
by) credence above a threshold.  
  
But that view seems implausible in any case. It’s pretty much only in gambling
setups with well-defined chances that we have sharp credences. But we all have
vast numbers of beliefs (taken in an informal ‘folk’ sense). So it seems that
some other approach is needed.  
  
A thorough Bayesian would be untroubled by this example. She would have her
credences, and that would be enough. She would not care whether they exceeded
some arbitrary threshold for belief. She would be similarly untroubled by the
lottery paradox. But no one in practice is a thorough Bayesian, or even close.
So again, it seems that some other approach is needed.

Suppose I am just the slightest bit short of the evidence needed for belief
that I have some condition _C_. I consider taking a test for _C_ that has a
zero false negative rate and a middling false positive rate—neither close to
zero nor close to one. On reasonable numerical interpretations of the previous
two sentences:

I have enough evidence to believe that the test would come out positive.

To see that (1) is true, note that the test is certain to come out positive if
I have _C_ and has a significant probability of coming out positive even if I
don’t have _C_. Hence, the probability of a positive test result will be
significantly higher than the probability that I have _C_. But I am just the
slightest bit short of the evidence needed for belief that I have _C_ , so the
evidence that the test would be positive (let’s suppose a deterministic
setting, so we have no worries about the sense of the subjunctive conditional
here) is sufficient for belief.

I have enough evidence to _know_ that the test would come out positive,

To be clear, I don’t doubt that the example works. It does, and it illustrates
a genuine problem with the view that belief is (or is justified by) credence
above a threshold. Specifically, this violates an apparently natural
reflection principle, that if I rationally believe that I will rationally come
to believe, then I should believe now. Issues like this seem unavoidable with
a threshold (other than 1). The lottery paradox illustrates a different one:
violation of logical closure. Of course, you may be prepared to live with such
issues - maybe beliefs don’t have to be logically consistent.  
  
  
My problem with the threshold approach is more basic. If I don’t have a
credence, I can’t apply the threshold test. If I do have a credence, then
noting whether it is above or below a threshold adds nothing useful. One
response to this is straight Bayesianism, which rejects any role for belief
(beyond credence 1). Another is that credence and belief are separate and
related, but not straightforwardly. In the example, I would say that I
_believe_ the relevant medical information about the chance that I have C and
about the reliability of the test, and set my _credences_ accordingly.

If the test comes out positive, it will be another piece of evidence for the
hypothesis that I have _C_ , and it will push me over the edge to belief that
I have _C_.

If I am rational, my beliefs will follow the evidence. So if I am rational, in
a situation like the above, I will take myself to have a way of bringing it
about that I believe, and do so rationally, that I have _C_. Moreover, this
way of bringing it about that I believe that I have _C_ will itself be
perfectly rational if the test is free. For of course it’s rational to accept
free information. So I will be in a position where I am rationally able to
bring it about that I rationally believe _C_ , while not yet believing it.

If the test comes out positive, I will have enough evidence to know that I
have _C_.

**Appendix:** Suppose the threshold for belief or knowledge is _r_ , where _r_
< 1. Suppose that the false-positive rate for the test is 1/2 and the false-
negative rate is zero. If _E_ is a positive test result, then _P_ ( _C_ | _E_
) = _P_ ( _C_ ) _P_ ( _E_ | _C_ )/ _P_ ( _E_ ) = _P_ ( _C_ )/ _P_ ( _E_ ) = 2
_P_ ( _C_ )/(1+ _P_ ( _C_ )). It follows by a bit of algebra that if my prior
_P_ ( _C_ ) is more than _r_ /(2− _r_ ), then _P_ ( _C_ | _E_ ) is above the
threshold _r_. Since _r_ < 1, we have _r_ /(2− _r_ ) < _r_ , and so the story
(either in the belief or knowledge form) works for the non-empty range of
priors strictly between _r_ /(2− _r_ ) and _r_.

But (6) is absurd: if I know that I will know something, then I am in a
position to know that the matter is so, since that I will know _p_ entails
that _p_ is true (assuming that _p_ doesn’t concern an open future). However,
there is no similar absurdity in (5). I may know that I will have enough
evidence to know _C_ , but that’s not the same as knowing that I will _know_
_C_ or even be in a position to know _C_. For it is possible to have enough
evidence to know something without being in a position to know it (namely,
when the thing isn’t true or when one is Gettiered).

I don’t know that I have _C_ but I know that I will know.

I don’t yet have enough evidence to know that I have _C_ , but I know that in
a moment I will.

Still, there is something odd about (5). It’s a bit like the line:

Ian,  
  
I hadn't thought about this in the context of reflection. But technically
there is no counterexample here to van Fraassen's reflection principle, which
only says that P(H | credence at t will be p) = p. My examples satisfy that,
assuming full transparency about my own credences and full certainty that I
will conditionalize (cf.: https://jonathanweisberg.org/pdf/C_R_and_SKv2.SP.pdf
).  
  
The original reflection principle implies that my credence should be the
expected value of my future credences (at a fixed future time; we might also
generalize to any martingale stopping time). This is not a problem in my
cases, because although it is likely that my future credence will be bigger
than it is now, there is a small chance that my future credence will plummet
to zero due to a negative test result, as William notes.  
  
Upon further thought, I think it can be quite reasonable to say: "I know that
tomorrow I will have knowledge-level evidence for p, but I don't yet." And if
so, then my subsequent post on squishiness, even if technically correct, is
moot.

In fact, the same thing can be said about knowledge, assuming there is
knowledge in lottery situations. For suppose that I am just the slightest bit
short of the evidence needed for _knowledge_ that I have _C_. Then I can set
up the story such that:

To see that (2) is true, note that given that the false negative rate is zero,
and the false positive rate is not close to one, I will indeed have non-
negligible evidence for _C_ if the test is positive.

I am not sure I buy (1). But it sounds kind of right to me now. Additionally,
(3) kind of sounds correct on its own. If _A_ causes _B_ and _B_ causes _C_
but there is no explanation of _A_ , then it seems that _B_ and _C_ are really
unexplained. Aristotle notes that there was a presocratic philosopher who
explained why the earth doesn’t fall down by saying that it floats on water,
and he notes that the philosopher failed to ask the same question about the
water. I think one lesson of Aristotle’s critique is that if it is unexplained
why the water doesn’t fall down it is unexplained why the earth falls down.

Dr Pruss, do you think that Bradley type regresses must also stop in some sort
of self explanatory fact? For example instead of having an infinite regress of
relations being related to their relata maybe it is somehow self explanatory
that the relation is related to its relata?

I have a problem with (2). (2) only follows if a complete explanation is
possible, which is kind of what you want to prove. So, it seems to me (2) begs
the question.  
If the 'water' is a brute fact, 'the earth floats on water' is as far as an
explanation can get. In a way it is merely partial but it isn't actually part
of something complete because there isn't something complete.  
That's why (1) is 'kind of right' but it isn't completely right.  
  
  
  

So, if any event _E_ has an explanation, it has an explanation going all the
way back to something self-explanatory. (1,2)

Any explanation for an event _E_ that does not go all the way back to
something self-explanatory is merely partial.

An explanation going back to something self-explanatory involves the activity
of a necessary being.

A partial explanation is one that is a part of a complete explanation.

Another interpretation of the question of what we are is that it is asking for
*classification*. Generics can be useful here, because they (a) are apt
answers to classification questions and (b) still allow for exceptions. The
former feature makes them substantive. The latter makes them modest and immune
to certain styles of counterexample. For more on this (with respect to the
question of what *we* are, but the tools at play are widely applicable), see:
https://andrewmbailey.com/GenericAnimalism.pdf

What am I? A herring-eater, a husband, a badminton player, a philosopher, a
father, a Canadian, a six-footer and a human are all correct answers. There is
an ancient form of the “What is _x_?” question where we are looking for a
“central” answer, and of course “a human” is usually taken to be that one.
What makes for that answer being central?

_x_ is good and _x_ is _F_ if and only if _x_ is a good _F_

Maybe, but can you say: What it is for you to be good is for you to be a good
man?  
  
Maybe the issue is that there is an ambiguity. A man (in the relevant sense)
is an adult male human. But "good adult male human" is a bit ambiguous: is one
good at the adulting, the maleness or the humanness, or at all three?  
  
I would say that what makes you be good is that you are good at being human,
and given that you are a man, this may have certain implications that it
wouldn't have if you weren't a man. (E.g., that you are a man makes it bad for
you to say "I am not a man".)

But the same is not true for any other attribute besides “human” among those
of the first paragraph. I can be good and a badminton player while not being a
good badminton player, and I can be a good herring-eater without being good
and a herring-eater. And I have no idea what it is to be a good six-footer,
but perhaps the fact that I am a quarter of an inch short of six feet right
now makes me not be one. (In some cases one direction may hold. It may be that
if I am good and a human, then I am a good human.)

And yet “a virtuous human” would not be the answer to the ancient “What am I
centrally, really, deep-down, essentially?” question even if I were in fact a
virtuous human.

Necessarily, if I am human, what it is for me to be good is for me to be a
good human.

Necessarily, I am good and a human if and only if I am a good human.

In other words, that which I am centrally, really, deep-down and essentially
is that which sets the norms for me to be good _simpliciter_.

On the "male human" point: it does seem like I can say "I am good and a man
iff I am a good man." But then (if we take the above approach) it seems like I
am centrally (really, deep-down) a man, not merely a human. This seems to sit
less comfortably with the Athanasian point. Perhaps this depends on the notion
of gender-specific norms? I'm not enough of a philosopher of gender to know
the literature on things like that.  
  

**Objection 2:** “Good” is attributive, and hence there is no such thing as
being good _simpliciter_.

In other words, if I am a human, being a good human explains my being good. On
the other hand, even if I were a virtuous human, my being a good virtuous
human would not explain my being good. For redundancy is to be avoided in
explanation, and “good virtuous human” is redundant.

So the sense of the “What am I centrally, really, deep-down, essentially?”
question isn’t just modal. What is it?

Sometimes the word “essential” is thrown in: I am _essentially_ human. But
what does that mean? In contemporary analytic jargon, it just means that I
cannot exist without being human. But the “central” answer to “What am I?” is
not just an answer that states a property that I cannot lack. There are, after
all, many such properties essential in such a way that do not answer the
question. “Someone conceived in 1972” and “Someone in a world where 2+2=4”
attribute properties I cannot lack, but are not the central answers.

But perhaps we can do better. The necessary biconditional (2) holds in the
case “virtuous human”, but in a kind of trivial way: “a good virtuous human”
is repetition. I think that, as often, we need to pass from a modal to a
hyperintensional characterization. Consider that not only is (1) true, but
also:

is not _generally_ logically valid. But some instances of a schema can be
logically valid even if the schema is not logically valid in general.

So our initial account of what is being asked for is that we are asking for an
attribute _F_ such that:

_x_ is “centrally, really, deep-down, essentially” _F_ just in case what it is
for _x_ to be good is for _x_ to be a good _F_.

Necessarily, _x_ is a good _F_ if and only if _x_ is good.

Andrew:  
  
Nice paper!  
  
But I think classification is not what I am getting at. I am aptly classified
as a male human, a human, a mammal, a vertebrate, an animal, an organism, and
an enmattered thing. However, I think only "human" is the answer to the
question I am after, the "what am I really" question. I am looking for a
particular "deep" classification. This is neither the most general
classification nor the most specific one, but the one that captures what most
deeply I am.  
  
On Athanasian theology, what makes me be saved is that the second person of
the Trinity became human just as I am human. He also became a male human just
as I am a male human and he became a vertebrate just as I am a vertebrate. But
these things do not save me. What saves me is his co-humanity, not his co-
vertebricity or co-animality or co-maleness. There is something deeper about
humanity than about any of these other things.  
  
This may be ethically important, too. Should I care more for a squid or a
mouse, if their intelligence is equal? The mouse is a fellow vertebrate, but
so what? It makes no difference. Similarly, that someone is a fellow male
matters not a whit. However, that someone is a fellow human, that seems to
matter.

**Objection 1:** Some things are “centrally” electrons, but something’s being
good at electronicity is absurd.

Necessarily, I am good and a virtuous human if and only if I am a good
virtuous human.

**Response:** I deny that it’s absurd. It’s just that all the electrons we
meet _are_ good at electronicity.

Alex  
  
As Ian says, (1) applies to finite creatures, it doesn't apply to an
omniscient being. On open theism, God knows everything that can be known, but
libertarian free choices cannot be known in advance. If God were to "guess"
what I will do tomorrow, of course on open theism, he could turn out to be
wrong, but it would be a guess and not a belief.

Ian:  
  
I am somewhat inclined to say that a high credence just *is* what a belief is.  
  
THToD:  
  
Like you and Alston, I am inclined to agree that "belief" is at best an
awkward way to describe God's cognition. However, I think a major part of the
reason why God doesn't fundamentally have beliefs is that there is no room for
a gap in God between cognition and reality. Whether an open theist can have
the "no gap" view of God depends on the variety of open theism. If the open
theist is an open futurist, who thinks there are no facts in reality about
future contingents, then they can have the "no gap" view. But an open theist
like Swinburne or van Inwagen who thinks there are facts about future
contingents and God doesn't know them thinks there is a gap between God's
knowledge and reality, and now God is sounding very much like us -- there is
stuff that God is uncertain about.

A rational being believes anything that they take to have probability bigger
than 1 − (1/10100) given their evidence.

I’d reject (1) in any case. Perfectly rational beings would have no use for
belief. They would have consistent credences (which could be 0 or 1) for
everything. On this view, beliefs are a shortcut that we, with our limited
mental capacity, use to reduce processing load.

These three theses, together with some auxiliary assumptions, yield a serious
problem for open theism.

Consider worlds created by God that contain four hundred people, each of whom
has an independent 1/2 chance of freely choosing to eat an orange tomorrow
(they love their oranges). Let _p_ be the proposition that at least one of
these 400 people will freely choose to eat an orange tomorrow. The chance of
not- _p_ in any such world will be (1/2)400 < 1/10100. Assuming open theism,
so God doesn’t just directly know whether _p_ is true or not, God will take
the probability of _p_ in any such world to be bigger than 1 − (1/10100) and
by (1) God will believe _p_ in these worlds. But in some of these worlds, that
belief will turn out to be false—no one will freely eat the orange. And this
violates (3).

I suppose the best way out is for the open theist to deny (1).

Speaking of "God's beliefs" seems about as coherent as discussing what God ate
for lunch. He's not a finite creature.

Maybe we should say this. Even if all intentional action is end-directed,
there are two kinds of reasons for an action: the reasons that come from the
value of the end and the reasons that come from the value of the pursuit of
that end. In the case of follow-through, there may be a fairly trivial success
value in the follow-through—a success value that comes from one’s exercise of
normative power in adopting the follow-through as one’s end—but that success
value provides only fairly trivial reasons. However, there can be
significantly non-trivial reasons for one’s pursuing that end, reasons
independent of that end.

Clearly the follow-through is _intended_ —it’s consciously planned, aimed at,
etc. But it need not be a means to anything one cares about in the game
(though, of course, in some cases it can be a means to impressing the
spectators or intimidating an opponent). But is it an end? It seems pointless
as an end!

Yet it seems that whatever is intended is intended as a means or an end. One
might reject this principle, taking follow-through to be a counterexample.

But there is a problem here. For even if there is a “success value” in
accomplishing a self-set goal, the strength of the reasons for pursuing the
follow-through is also proportioned to facts independent of this exercise of
normative power. Rather, the reasons for pursuing the follow-through will
include the internal and external goods of victory (winning as such, prizes,
adulation, etc.), and these are independent of one’s setting follow-through as
one’s goal.

Another move is this. We actually have a normative power to make something
_be_ an end. And then it becomes genuinely worth pursuing, because we have
adopted it as an end. So the player first exercises the normative power to
make follow-through be an end, and then pursues that end as an end.

This is interesting action-theoretically. The follow-through appears
pointless, because the agent’s interest is in what happens _before_ the
follow-through, the impact’s having the right physical properties, and yet
there is surely no backwards causation here. But there not appear to be an
effective way to reliably secure these physical properties of the impact
except by trying for the follow-through. So the follow-through itself is
pointless, but one’s _aiming at_ or _trying for_ the follow-through very much
has a point. And here the order of causality is respected: one swings aiming
at the follow-through, which causes an impact with the right physical
properties, and the swing then continues on to the “pointless” follow-through.

As far as I know, in all racquet sports players are told to follow-through: to
continue the racquet swing after the ball or shuttle have left the racquet.
But of course the ball or shuttle doesn’t care what the racquet is doing at
that point. So what’s the point of follow-through? The usual story is this: by
aiming to follow-through, one hits the ball or shuttle better. If one weren’t
trying to follow-through, the swing’s direction would be wrong or the swing
might slow down.

Alex  
  
How does it follow from the idea that God is not simple that God’s beliefs
change as the reality they are about changes?  
  
If (2) is true, isn't that a problem for divine simplicity as well? For on
divine simplicity, God id identical to God's beliefs, so if (2) is true it
seems that God changes as the reality changes, which conradicts divine
simplicity as well as divine immutability.

I agree with Walter. It seems to me that 2 is more directly incompatible with
God's atemporality than with his lack of complexity as such. An atemporal God
who sees Creation across its history in toto as a block, whether He is simple
or complex, has only true beliefs about everything that has been or will be,
and about when each fact will be true for finite creatures who are temporal.
But his beliefs don't change.  
  
Although a God who changed with and experienced time such that his beliefs
also changed might be thus considered complex across time by having temporal
parts, that would seem to be assuming an A-theory of time for all (including
God), then importing a B perspective at the end, incoherently. So, simplicity
of essence (at any point in time) and atemporality could be distinguished I
suppose, such that a person could affirm both simplicity and temporality of
God. I'm not saying that any open theist holds this opinion, or should, but it
seems a logical possibility from within that framework.

I'm not sure that the creaturely changes are necessarily "destroying" (or
creating) parts of God (beliefs) on the open theists' assumption. I think they
are determining them. I will assume open theists who believe God is maximally
knowledgeable, that is, knows all that can be known, which does not include
perfect knowledge of the future according to them.  
  
On this set of assumptions God will still, however, have perfect foreknowledge
of all possible outcomes as he can understand all possible cause/effect
branches as First Cause. He just doesn't know which path along the branching
options will eventuate before the time that they do. This means his knowledge
grows only in the sense that he is able to pick out which of each fully
foreseen branch is actualised. In other words, the only way his knowledge
grows is by the steady lengthening of a "track" through a foreknown "space" of
possible world-states over time. In such a scenario, creatures with free will
would determine the direction of the track and thus the precise determination
of God’s knowledge of it, but they would not be adding to or subtracting from
God's beliefs in any quantitative sense. The addition that would occur comes
about automatically as time flows, no matter what finite agents decide.  
  
Please note, I do not hold this position, but it seems to me it is not guilty
of the exact absurdity alleged here, whatever its other flaws.  

God is not simple, and in particular God’s beliefs are proper parts of God.

And of course by standing up, I bring it about that a new divine belief
exists. So:

It depends on the details of how they think beliefs work. But suppose they
hold to this picture: for each proposition p that God knows, there is God's
act of believing p. Moreover, these opponents of simplicity think that God's
act of believing p is a different act for each different p that God believes.
These different acts are on that view something like accidents of God. Thus,
by raising one's arm one brings it about that God's belief that one's arm is
in a lowered state does not exist and that God's belief that one's arm is in a
raised state does exist, and so one destroys a part of God and creates a new
one. (It is a part of the package that I am criticizing that propositions are
tensed, and so it's not enough for God to eternally know that at t1 the arm is
lowered and at t2 the arm is raised.)

I think one can hold that God's beliefs are accidents of his which are not
identical to him (rejecting a strong doctrine of divine simplicity) without
holding that they are "parts" of him in any sense that makes (3) or (4)
absurd. Even for created beings like us, the accident-substance relationship
does not appear to be at all the same as the part-whole relationship as common
sense conceives it. (I would find it weird to say that my beliefs are a part
of me in the same sense that my arm is a part of my body, for instance.)

How? Easy. I am now sitting, and God knows that. So, a part of God is the
belief that I am sitting. But I can destroy that belief of God’s by standing
up! For as soon as I stand up, the belief that I am sitting will no longer
exist. But on the view in question, God’s beliefs are parts of him. So by
standing up, I would bring it about that a part of God doesn’t exist.

I agree that one could hold 1 without 2. But my argument is against people who
hold the package of 1 and 2. Think here of process theologians, and their more
orthodox cousins.

On the other hand, if we think of _horrendous_ evils, like the torture of
children, it is difficult to think of much greater goods. Maybe with
difficulty we can come up with one or two possibilities, but not enough to
make it _easy_ to suppose a justification for God’s permission of the evil in
terms of the goods.

There seems to be a legitimate reason to allow evil choices to exist and to
allow those evil choices to affect other people. Seemingly, actions wouldn't
mean much (such as freely loving God) if we were all in cubicles and couldn't
actually interact with people negatively. Free love and free hatred seems to
be two sides of the same coin. I would also say that there are legitimate
reasons for suffering to exist. Perhaps the world would have less good in it
if there is no suffering. Would God permit suffering to allow more good? I
don't see a reason as to why He wouldn't. Theodicies and defenses are very
interesting, though it seems to me that, intuitively at least, most theodicies
seem to deal with the problem of trivial evils.

The reason why few people seriously run an argument from trivial evils is
simply because the case is much more obvious in the case of horrendous evil.  
But a truly opnipotent being should be able to accomplish Evert good without
any evil, unless this is logically impossible  
So, unless there is a convincing argument for why a certain good cannot
posdibly be accomplished without permitting evil, the default position should
be that it is possible.  

Unknown  
  
So, God's actions do not mean much if He can't interact with people
negatively?

Nobody seriously runs an argument against the existence of God from _trivial_
evils, like hangnails or mosquito bites.

Perhaps not negatively, but certainly 'without love'. It seems to me that God
did not have to create you and thus God did not have to love you. If God were
forced to love you or loved you randomly, I wouldn't consider God to be a
moral being. I also wouldn't consider love to be a moral good if it were
forced. Therefore, it seems possible that evil actions are merely privations
of good actions, and thus one cannot choose to do "nothing" because "nothing"
is a lack of good, which seems to be some sort of evil. Perhaps one could make
a distinction between evils, but it seems that this might be a better
distinction that the one I previously drew.

Unknown  
  
I wouldn't say "forced" is the correct word here, but it is true that God
_necessarily_ loves you. So, in a sense, God _did/does_ have to love you.

There is, however, a difference between the cases. Many great ordinary goods
that dwarf trivial evils, like the courage of a Socrates, are known to us. Few
if any finite goods that dwarf horrendous evils are known to us. Nonetheless,
if theism is true, it is very likely that such goods are possible. And since
the argument from evil is addressed against the theist, it seems fair for the
theist to invoke that hierarchy.

So if God exists, we would expect there to be unknown possible finite goods
that are related to the known horrendous evils in something like the
proportion in which the known great finite goods are related to the known
trivial evils. Thus, if God exists, there very likely is  
an upward hierarchy of possible goods to which the horrendous evils of this
life stand like a mosquito bite to the courage of a Socrates. If we believe in
this hierarchy of goods, then it seems we should be no more impressed by the
atheological evidential force of horrendous evils than the ordinary person is
by the atheological evidential force of trivial evils.

Why not? Here is a hypothesis. There are so very many possible much greater
goods—goods qualitatively and not just quantitatively much greter—that it
would be easy to suppose that God’s permitting the trivial evil could promote
or enhance one of these goods to a degree sufficient to yield justification.

However, if God exists, we would _expect_ there to be an unbounded upward
qualitative hierarchy of possible finite goods. God is infinitely good, and
finite goods are participations in God, so we would expect a hierarchy of
qualitatively greater and greater types of good that reflect God’s infinite
goodness better and better.

Moreover, we might ask whether our ignorance of goods higher up in the
hierarchy of goods beyond the ordinary goods is not itself evidence against
the existence of such goods. Here, I think the answer is that it is very
little evidence. We would expect any particular finite being to be able to
recognize only a finite number of types of good, and thus the fact that there
are only a finite number of goods that we recognize is very little evidence
against the hypothesis of the upward hierarchy of goods.

Premise (1) is controversial. The ancient Greeks would have denied it. But I
think the reason they denied it is that they didn’t have the examples that the
Christian tradition does, highly attractive examples examples of accomplished
lives of great humility.

Walter:  
  
Good point.  
  
I propose to revise premise 2 to read:  
2*. There are some highly accomplished individuals such that for them humility
is an appropriate attitude only if God exists.  
  
Then we don't even need premise 3.  
  
To respond to your point, now, I suggest that we think of highly accomplished
individuals who did not have many of the kinds of advantages you list, and who
were hindered in various ways (e.g., by racism, sexism, poverty, war, etc.).
While no doubt everyone got some help from other humans, in some cases a
realistic appraisal of the degree of that help, especially when combined with
the degree to which fellow humans hindered the individual, will not suffice
for humility to be appropriate *if* God does not exist.

Alex  
  
I have already answered this in my first reply. For the individuals you
describe, humililty is not appropriate.

1) I think the participationism explanation is problematic, since by that same
logic one could maybe deny one truly possesses being or goodness or value; one
could even deny one causes anything at all, concluding occasionalism. But we
do have our own being & goodness, so it's not the divine being & goodness in
us, per Aquinas.  
  
  
2) In fact, there's a real sense in which our actions & thereby
accomplishments are uniquely our own in a way they aren't God's precisely
because our actions are rooted in our secondary causality, which is distinct
from God's causality. And secondary causality being distinct from primary
causality, the causal responsibility for secondary causal acts is in the
creature, since the primary causality of God in sustaining our actions in
existence only determins them insofar as they exist, but we determine them
insofar as whether we cause them to occur (the occurrence of them is distinct
from their basic existence) and what we cause to occur.  
  
3) Additionally, there may be a difference between bragging and general pride
- some moral theology manuals seem to suggest being proud of one's
accomplishments isn't sinful, because that's distinct from bragging morally.
And one can't be proud of things one doesn't truly possess or cause in some
way.  
  
Of course, other sources say any form of pride is sinful, so this isn't a
clear issue it seems - but if one did take the route that some forms of pride
ARE morally okay, then this makes the thesis we don't possess anything,
whether being or our actions, unlikely.

Here’s another way to think about it. Given (1) and (3), we need an
explanation of how it is that humility is an appropriate attitude for a highly
accomplished individual. Classical theism’s doctrine of participation provides
such an explanation: all the efforts and all the accomplishments are not truly
theirs but a participation in God’s perfection.

Here’s the thought behind premise (2). If there is no God, then highly
accomplished individuals have much to brag about. Many of their
accomplishments are primarily _theirs_.

Wesley:  
  
"by that same logic one could maybe deny one truly possesses being or goodness
or value"  
  
There is precedent for saying something like that. "No one is good but God
alone" (Mark 10:8).  
  
There has got to be a sense, and a very important one, in which what Jesus
says is true. But there is also a sense in which we are good--but only good-
by-participation.

@Alex I think an important nuance here is that in this specific view of
participation, goodness-by-participation isn't actually **true or intrinsic**
goodness. But this is problematic because we do have other verses affirming
the true goodness of creatures, such as Genesis 1 explicitly affirms this, as
does 1 Timothy 4 (first four verses on marriage & food) and Matthew 10
(sparrows).  
  
The main problem would be that this view of participationism _wouldn't just
make it impossible_ to take pride or "brag" about something we did (which I
can concede is wrong), but that it also makes it impossible to actually say we
are good or have value as well. In the **same manner & for the same reasons**
we can't take pride in or brag about our actions, we also can't affirm we are
**good,** or have being, and this seems to be a big problem with that specific
account.  
  
And it's also important to point out that Scripture often uses **hyperbolic
negation** or hyperbolic merism - God for example in Jeremiah 7:22 apparently
denies He ever commanded the Hebrews to do sacrifice, yet that's not a literal
negation but a hyperbolic one to point out that loving God is more important -
rhetorically DENYING one thing to point out the greater importance of another,
without intending to truly deny the importance of the secondary. Same thing
with loving Christ & hating one's parents - intentionally hyperbolic contrasts
that actually convey a hierarchy of love, but not pure exclusivity.  
  
So basing a very specific view of participationism (because not all models of
participationism would agree that we aren't actually truly good) on a phrase
that is likely using intentional rhetorical hyperbole is more speculative than
solid.

This seems question-begging. It's not because highly accomplished individuals
have a lot to brag about that they can brag about everything.  
  
Suppose you think this is a great argument, you certainly have reason to brag
about it, but even if there is no God, you also have reason to be humble,
because you may have been the first to come up with this particular argument,
but you could only have built a successful argument because of your education,
because of what you have studied, read, even the way your parents raised you.  
Nothing we do is exclusively our own accomplishment.  
Now if you don't agree with this, you don't have grounds for premise (1),
because if something really is your own accomplishment, humililty is not
appropriate because there is nothing wrong with being proud of what you have
accomplished.  
  
  
  

Suppose your end is irrationality. Is it really true that you _should_ adopt
the means to that, such as reasoning badly? Surely not! Instead, you should
reject the end.

But what is wrong with being such that you adopt means inappropriate to your
ends is not necessarily the means—it could be the ends.

I am inclined to think (1) is false if by “end” is meant the end the agent
actually adopts, as opposed to a natural end of the agent. If your ends are
sufficiently irrational, adopting means appropriate to them may be less
rational than adopting means inappropriate to them.

Unjust laws have no normative force, and stupid ends have no normative force,
either.

The amount of things one person can do with enough time is insane. I wouldn't
believe you if you told me that one person can get two PhDs, teach classes in
metaphysics, run a blog with constantly new and exciting arguments and
positions, is called one of the foremost Christian philosophers currently
alive, and STILL has time to measure bicycle energy output. I am convinced
that Dr. Pruss is some sort of superhuman.

[Adapting](https://www.instructables.com/Playing-NES-Power-Pad-Games-in-
Emulation/) Dance Dance Revolution and other mat controllers to work as NES
Power Pad controllers for emulation.

There is a way to connect the right and wrong with the good and bad:

I think (1) is something that everyone should accept. Even consequentialists
can and should accept (1) (though utilitarian consequentialists have too
shallow an axiology to make (1) true). But natural law theorists might add a
further claim to (1): the left-hand-side is true because the right-hand-side
is true.

An action is right (respectively, wrong) if and only if it is
noninstrumentally good (respectively, bad) to do it.

The title of this post contradicts the title of [another recent
post](http://alexanderpruss.blogspot.com/2022/12/the-right-cannot-be-derived-
from-good.html), but the contents do not.

This is compatible with there being cases where it is bad for one to do the
right thing. Thus, refraining from stealing the money that one would need to
sign up for a class on virtue is right and noninstrumentally good, but if the
class is really effective then stealing the money might be instrumentally good
for one, though noninstrumentally ba.

At the same time, the harmony need not be perfect. Just as there may be times
when the good of the community and the good of the individual conflict in
respect of non-epistemic flourishig, there [may be such
conflict](http://alexanderpruss.blogspot.com/2011/01/epistemic-self-sacrifice-
and-prisoner.html) in epistemic flourishing.

Of course, that it would be _good_ for the community if some norm of
individual rationality obtained does not prove that the norm obtains.

Moreover, note that it is very plausible that what range of variation of
priors is good for the community depends on the species of rational animal we
are talking about. Rational apes like us are likely more epistemically
cooperative than rational sharks would be, and so rational sharks would
benefit less from variation of priors, since for them the good of the
community would be closer to just the sum of the individual goods.

This could well be true because differences in priors lead to a variety of
lines of investigation, a greater need for effort in convincing others, and
less danger of the community as a whole getting stuck in a local epistemic
optimum. If this hypothesis is true, then we would have an interesting story
about why it would be good for our community if a range of priors were
rationally permissible.

I am grateful to Anna Judd for pointing me to a possible connection between
permissivism and natural law epistemology.

It is epistemically better for the human community if human beings do not all
have the same (ur-) priors.

I think it does. I have been trying to defend a natural law account of
rationality on which just as our moral norms are given by what is natural for
the will, our epistemic norms are given by what is natural for our intellect.
And just as our will is the will of a particular kind of deliberative animal,
so too our intellect is the intellect of a particular kind of investigative
animal. And we expect a correlation between what a social animal’s nature
impels it to do and what is good for the social animal’s community. Thus, we
expect a degree of harmony between the norms of epistemic rationality—which on
my view are imposed by the nature of the animal—and the good of the community.

Panteleology is also entailed by a panpsychism that follows Leibniz in
including the ubiquity of “appetitions” and not just perceptions. And it seems
to me that if we think through the kinds of reasons people have for
panpsychism, these reasons extend to appetitions—just as a discontinuity in
perception is mysterious, a discontinuity in action-driving is mysterious.

I think many theists would admit that God has created every substance for a
purpose, but that'd be an extrinsic teleology, and would be less
controversial. You've said elsewhere that you think that every substance has a
teleology. Are you meaning to argue that every substance has intrinsic
teleology? That'd be more controversial.  
  
  
  

Panteleology holds that teleology is ubiquitous. Every substance aims at  
some end.

Panteleology seems to be exactly what we would expect in a world created by
God. Everything _should_ glorify God.

That said, I think the quantum realm provides room for saying that things
don’t “just do what they do”. If an electron is in a mixed spin up/down state,
it seems right to think about it as having a directedness at a pure spin-up
state and a directedness at a pure spin-down state, and only one of these
directednesses will succeed.

Couldn't one relate teleology to causal powers and the possible effects they
could accomplish? Final causality exists as long as anything has a causal
power TOWARDS anything, and this directedness of power - without needing to be
active even - is itself a real example of teleology.  
  
For any agent that has the power to cause any effect in any way, it must be
directed towards that end at least insofar as any POWER only makes coherent
sense insofar as it has an EFFECT which it includes within itself and thereby
points.

The main objection to panteleology is the same as that to panpsychism: the
incredulous stare. I think a part of the puzzlement comes from the thought
that things that are neither biological nor artifactual “just do what they
do”, and there is no such thing as failure. But this seems to me to be a
mistake. Imagine a miracle where a rock fails to fall down, despite being
unsupported and in a gravitational field. It seems very natural to say that in
that case the rock failed to do what rocks should do! So it may be that away
from the biological realm (namely organisms and stuff made by organisms)
failure takes a miracle, but the logical possibility of such a miracle makes
it not implausible to think that there really is a directedness.

One might suppose that among the small number of fundamental abstract moral
principles one will have some principles about respect for bodily integrity. I
doubt it, though. Respect for bodily integrity is an immensely complex area of
ethics, and it is very unlikely that it can be encapsulated in a small number
of abstract moral principles. Respect for bodily integrity differs in very
complex ways depending on the body part and the nature of the relationship
between the agent and the patient.

All facts about rightness and wrongness can be derived from descriptive facts,
facts about non-rightness value, and a small number of fundamental abstract
moral principles.

I think Utilitarians might try to say something like, "If governments took
kidneys, this would have a different psychological effect on society than if
they took 20% of our income. Because of this accidental feature of our
psychologies, it actually would be more harmful for them to take our kidneys,
and look here I can show how this extra harm is unjustifiable via my abstract
moral principles."

I don’t think we can derive (2) in accordance with the strictures in (1). If a
kidney were a lot more valuable than 20% of lifetime income, we would have
some hope of deriving (2) from descriptive facts, non-rightness value facts,
and abstract moral principles, for we might have some abstract moral principle
prohibiting the government from forcibly and non-punitively taking something
above some value. But a kidney is not a lot more valuable than 20% of lifetime
income. Indeed, if it would cost you 20% of your lifetime income to prevent
the destruction of one of your kidneys, it need not be unreasonable for you to
refuse to pay. Indeed, it seems that either 20% of lifetime income is
incommensurable with a kidney, or in some cases it is more valuable than a
kidney.

If loss of a kidney were to impact one’s autonomy significantly more than loss
of 20% of your lifetime income, then again there would be some hope for a
derivation of (2). But whether loss of a kidney is more of an autonomy impact
than loss of 20% of income will differ from person to person.

I don’t have a good definition of “abstract moral principle”, but I want them
to be highly general principles about moral agency such as “Choose the greater
over the lesser good”, “Do not will the evil”, etc.

I should note that the above argument fails against divine command theories.
Divine command theorists will say that about rightness and wrongness are
identified with descriptive facts about what God commands, and these facts can
be very rich and hence include enough data to determine (2). For the argument
against (1) to work, the “descriptive facts” have to be more like the facts of
natural science than like facts about divine commands.

The restriction to non-rightness good and bad is to avoid triviality. By
“rightness value” here, I mean only the value that an action or character has
in virtue of its being right or wrong to the extent that it is.

It is not wrong for the government to forcibly and non-punitively take 20% of
your lifetime income, but it is wrong for the government to forcibly and non-
punitively take one of your kidneys.

Consider the following thesis that both Kantians, utilitarians and New Natural
Law thinkers will agree on:

When making my [Guinness application record
video](http://alexanderpruss.blogspot.com/2022/12/a-new-but-uncertified-world-
record.html), I wanted to include a time display in the video and Guinness
also required a running count display. I ended up writing a [Python
script](https://gist.github.com/arpruss/3a705c859d4ea4a482c4e47e96a08f79)
using OpenCV2 to generate a video of the time and lap count, and overlaid it
with the main video in Adobe Premiere Rush.

The code uses [webpxmux.js](https://github.com/sumimakito/webpxmux.js), though
it was a little bit tricky because in-browser Javascript may not have enough
memory to store all the uncompressed images that webpxmux.js needs to generate
an animation. So instead I encode each frame to WebP using webpxmux.js,
extract the compressed ALPH and VP8 chunks from the WebP file, and store only
the compressed chunks, writing them all at the end. (It would be even better
from the memory point of view to write the chunks one by one rather than
storing them in memory, but a WebP file has a filesize in its header, and
that’s not known until all the compressed chunks have been generated. One
could get around this limitation by generating the video twice, but that would
be twice as slow.)

Since then, I wrote a [web-based tool](https://arpruss.github.io//webpanim)
for generating a WebP animation of a timer and text synchronized to a set of
times. The timer can be in seconds or tenths of a second, and you can specify
a list of text messages and the times to display them (or to hide them). You
can then overlay it on a video in Premiere Rush or Pro. There is alpha
support, so you can have a transparent or translucent background if you like,
and a bunch of fonts to choose from (including the geeky-looking Hershey font
that I used in my Python script.)

where “ _N_ ” is some norm like that of prudence or etiquette. In this case,
the moral requirement of _ϕ_ ing overrides the _N_ -prohibition on _ϕ_ ing.
Thus, you might be rude to make a point of justice or sacrifice your life for
the sake of justice.

Still, consider this. The judgment whether moral considerations override the
non-moral ones seems to be an eminently _moral_ judgment. It is the person
with _moral_ virtue who is best suited to figuring out whether such overriding
happens. But what happens if morality says that the moral considerations do
not override the _N_ -prohibition? Is that not a case of morality giving its
endorsement to the _N_ -prohibition, so that the _N_ -prohibition would rise
to the level of a moral prohibition as well? But if so, then that pushes us
back to the previous story where it is reasonable to take _N_ -considerations
to be subsumed into moral considerations.

People often talk of moral norms as overriding. The paradigm kind of case
seems to be like this:

Cases of supererogation look like that: you are morally permitted to do
something contrary to prudential norms, but not required to do so.

However, there is another story possible. Perhaps in the case where the moral
considerations are at too low a level to override the _N_ -prohibition, we can
still have _moral_ permission to _ϕ_ , but that permission no longer overrides
the _N_ -prohibition. On this story, there are two kinds of cases, in both of
which we have moral permission, but in one case the moral permission comes
along with sufficiently strong moral considerations to override the _N_
-prohibition, while in the other it does not. On this story, moral requirement
always overrides non-moral reasons; but whether moral considerations override
non-moral considerations depends on the relative strengths of the two sets of
considerations.

But if there are cases like (1), there will surely also be cases where the
moral considerations in favor of _ϕ_ ing do not rise to the level of a
requirement, but are sufficient to override the _N_ -prohibition. In those
cases, presumably:

So far so good. Moral norms can override non-moral norms in two ways: by
creating a moral requirement contrary to the non-moral norms or by creating a
moral permission contrary to the non-moral norms.

But this would be quite interesting. It would imply that in the absence of
sufficient moral considerations in favor of _ϕ_ ing, an _N_ -prohibition would
automatically generate a _moral_ prohibition. But this means that the real
normative upshot in all three cases is given by morality, and the _N_ -norms
aren’t actually doing any independent normative work. This suggests strongly
that on such a picture, we should take the _N_ -norms to be simply a species
of moral norms.

But now consider this. What happens if the moral considerations are at an even
lower level, a level insufficient to override the _N_ -prohibition? (E.g.,
what if to save someone’s finger you would need to sacrifice your arm?) Then,
it seems:

I don’t want to say that all norms are moral norms. But it may well be that
all norms governing the functioning of the will are moral norms.

The promise is simply irrelevant here. It is true that in normal
circumstances, it makes sense for egoists to keep promises in order to fool
people into thinking that they have morality. But I’ve assumed full shared
knowledge of each other’s tendencies here, and so no such considerations apply
here.

This means that in cases like this, with full transparency of behavioral
tendencies, utilitarians and amoral egoists will do well to brainwash or
hypnotize themselves into promise-keeping.

It is true that if Alice expects Bob to expect her to keep her promise, then
Alice will expect Bob to raise his right hand, and hence she should raise her
right hand. But since she’s known to be an amoral egoist, there is no reason
for Bob to expect Alice to keep her promise. And the same vice versa.

In ordinary life, this problem doesn’t arise as much, because as long as at
least one person is more typical, and hence takes promises to have reason-
giving force, or if public opinion is around to enforce promise-keeping, then
the issue doesn’t come up. But I think there is a lesson here and in the
[previous post](http://alexanderpruss.blogspot.com/2022/12/utilitarianism-and-
communication.html): for many ordinary practice, the utilitarian is free-
riding on the non-utilitarians.

My feeling is it's not hard to solve -- as long as you don't place artificial
restrictions on what counts as a reason, in the way the utilitarian or egoist
does.

They confer before the game and promise to one another to raise the right
hand. They go into their separate rooms. And what happens next?

What if they are utilitarians? It makes no difference. Since in this case both
always get the same outcome, there is no difference between utilitarians and
amoral egoists.

Take first the case where they are both perfect amoral egoists. Amoral egoists
don’t care about promises. So the fact that an amoral egoist promised to raise
the right hand is no evidence at all that they will raise the right hand,
unless there is something in it for them. But is there anything in it for
them? Well, if Bob raises his right hand, then there is something in it for
Alice to raise her right hand. But note that this conditional is true
_regardless_ of whether they’ve made any promises to each other, and it is
equally true that if Bob raises his left hand, then there is something in it
for Alice to raise her left hand.

Here's a further line of thought. Suppose that Alice and Bob are not fully
utilitarian, but they incorporate into their ethics (which they follow
perfectly) an anti-conventionalist element (and that's also a part of their
shared knowledge). Thus, they think that the fact that one has promised p is a
fairly weak reason, of degree epsilon, against performing p. Then, if
epsilon>0, then it seems that by their lights Alice and Bob should lift their
left hands rather than their right, if they promised to lift their right,
since their behavioral bias is anti-promissory, and each knows the other to
have such a bias. Very well. Now take the limit as epsilon (the strength of
the reason they think favors breaking promises) to zero. For every epsilon>0,
they should lift their left hands rather than their right. It seems to be a
reasonable continuity conclusion that for epsilon=0, they should either be
neutral between lifting their left hands rather than their right or should
still prefer the left, but definitely should not prefer their right. And yet
epsilon=0 is just the full utilitarian case.

This is a fun puzzle. It seems like David Lewis’ convention work, or Thomas
Schelling’s coordination and focal points stuff must be relevant.

Suppose Alice and Bob are perfect utilitarians or perfect amoral egoists in
any combination. They are about to play a game where they raise a left hand or
a right hand in a separate booth, and if they both raise the same hand, they
both get something good. Otherwise, nobody gets that good. Nobody sees what
they’re doing in the game: the game is fully automated. And they both have
full shared knowledge of the above.

But I’ve been trying really hard to figure out how is it that such a
conventional behavior would indicate to Bob that the lion is on the left path.

If the above argument is correct—and I am far from confident of that, since it
makes my head spin—then we have an argument that in order for communication to
be possible, at least one of the agents must be convention-bound. One way to
be convention-bound is to think, in a way utilitarians don’t, that convention
provides non-consequentialist reasons. Another way is to be an akratic
utilitarian, addicted to following convention. Now, the possibility of
communication is essential for the utility of the kinds of social animals that
we are. Thus we have an argument that at least some subjective utilitarians
will have to become convention-bound, either by getting themselves to believe
that convention has normative force or by being akratic.

Similarly, if Bob were a typical human being, he would have a habit of forming
his beliefs on the basis of testimony interpreted via established social
conventions absent reason to think one is being misinformed, and so Alice’s
engaging in conventional left-path lion-indicating behavior would lead Bob to
think there is a lion on the left, and hence to go on the right. And while it
woudl still be true that social convention has no normative force for Alice,
Alice would think have reason to think that Bob follows convention, and for
the sake of maximizing utility would suit her behavior to his. But Bob is a
perfect Bayesian. He doesn’t form beliefs out of habit. He updates on
evidence. And given that Alice is not a typical human being, but a
subjectively perfect utilitarian, it is unclear to me why her engaging in the
conventional left-path lion-indicating behavior is more evidence for the lion
being on the left than for the lion being on the right. For Bob knows that
convention carries no normative force for Alice.

I put this as an issue about communication. But maybe it’s really an issue
about communication but coordination. Maybe the literature on repeated games
might help in some way.

Here is a brief way to put it. For Alice and Bob, convention carries no weight
except as a predictor of the behavior of convention-bound people, i.e., people
who are not subjectively perfect utilitarians. It is shared knowledge between
Alice and Bob that neither is convention-bound. So convention is irrelevant to
the problem at hand, the problem of getting Bob to avoid the lion. But there
is no solution to the problem absent convention or some other tool unavailable
to the utilitarian (a natural law theorist might claim that mimicry and
pointing are _natural_ indicators).

If Alice were a typical human being, she would have a habit of using
established social conventions to tell the truth about things, except perhaps
in exceptional cases (such as the murderer at the door), and so her use of the
conventional lion-indicating behavior would correlate with the presence of
lions, and would provide Bob with evidence of the presence of lions. But Alice
is not a typical human being. She is a subjectively perfect utilitarian.
Social convention has no normative force for Alice (or Bob, for that matter).
Only utility does.

This is not a refutation of utilitarianism. Utilitarians, following Parfit,
are willing to admit that there could be utility maximization reasons to cease
to be utilitarian. But it is, nonetheless, really interesting if something as
fundamental as communication provides such a reason.

Suppose the lion is on the left path. What should Alice do? Well, if she can,
she should bring it about that Bob takes the right path, because doing so
would clearly maximize utility. How can she do that? An obvious suggestion:
Engage in a conventional behavior indicating a where the lion is, such as
pointing left and roaring, or saying “Hail well-met traveler, lest you be
eaten, I advise you to avoid the leftward leonine path.”

Alice and Bob are both perfect Bayesian epistemic agents and subjectively
perfect utilitarians (i.e., they always do what by their lights maximizes
expected utility). Bob is going to Megara. He comes to a crossroads, from
which two different paths lead to Megara. On exactly one of these paths there
is a man-eating lion and on the other there is nothing special. Alice knows
which path has the lion. The above is all shared knowledge for Alice and Bob.

I've wondered about such incompossible goods myself. Weightlifting can reduce
agility, for example, and while being tall is great for basketball, it is not
good for weightlifting. Various niches seem to exist. It does raise the
question of what a perfect human being consists of, however. Are some of these
"perfections" merely accidental in the sense that they exploit what are
normatively speaking (in relation to human nature) flaws? We know some sports
are actually bad for the human body, or neglect the overall health of the
body. Certain perfections seem to be perfections only in an analogical sense,
perhaps something along the lines of being a "good thief" or an "effective
deceiver".  
  
Where intelligence is concerned, I am tempted to argue against incompossible
goods either because intelligence isn't like that or for similar natural law
reasons, but even more strongly, especially on account of the centrality of
intelligence to humanity.  
  
Consider your favorite déformation professionnelle, which, I submit, is more
of a result of imprudence, habit, ignorance, lack of practice using other
methods, and even effeminacy and arrogance. Someone with a rigorous
philosophical education is less likely to try to pigeonhole reality into the
reductive and simplified straitjacket of our physical models in the manner of
at least some physicists who generally lack serious exposure to philosophy and
may even hold it in contempt out of ignorance. A physicist may also be tempted
to pigeonhole simply because of pride; if he isn't any good at metaphysics,
then his thoughts on a metaphysical subject matter aren't likely to be very
valuable or interesting, and that stings the prideful man accustomed to
feeling like a hotshot. There is also the threat of seeing one's own field put
in its methodological place, so to speak, deflating any pretensions to the
kind of ultimacy that metaphysics lays claim to. A competent physics may also
derive greater pleasure from exercising his specialized competence and choose
his methods simply on the basis of what feels good and now what is called for
by a problem. So here the question resurfaces: is there an incompossibility
between being a good physicist and a good metaphysician? I suspect there isn't
intrinsically, even if that is often the case which I suspect is rather a
result of how one's time is spent. But even if it is the case, because general
knowledge is superior and more worthy of human attention than specialized
knowledge, we could argue that competence in physics that occurs at the
expense of philosophical depth is, in fact, a kind of failure to attain human
excellence by failing to devote proportional attention and effort to the kinds
of knowledge that are most essential, and in doing so, risking intellectual
deformation in important and even necessary matters.

I have no idea if anything like this transfer works for other people.

I do have a piece of anecdotal data, though. I’ve been doing some endurance-
ish sports. Nothing nearly like a marathon, but things like swimming 2-3 km,
or climbing for an hour, typically (but [not
always](http://alexanderpruss.blogspot.com/2022/12/a-new-but-uncertified-
world-record.html)) competing against myself.

BTW, it may depend on how much one keeps to proper form in the different
racquet sports. Someone like me who plays lots of different racquet sports
(regularly: badminton; semi-regularly: tennis, racquetball, table tennis,
pickleball; used to do occasional squash until our university's one court
closed as nobody but my son and I played; used to do crossminton during
Covid), maybe at an upper beginner level, perhaps does not have enough good
form in any one of the sports for it to matter. I hadn't played much tennis
this fall, but I played a fair amount of badminton, and seemed to find my
tennis improved when I got back to it, maybe due to transfer of thinking about
things like "how do I hit the shuttle away from where my opponents are", or
maybe just due to general fitness improvement.

I think that's correct. The more one concentrates on good form, the more
detrimental negative transfer becomes. This also applies in martial arts like
Tae Kwon Do, on the self-defense side, whereby training to 'pull' kicks and
punches (i.e. to safeguard your training partner), negatively transfers to
real-life situations.

There are empirical indications that various skills and maybe even virtues are
pretty domain specific. It seems that being good at reasoning about one thing
need not make one good at reasoning about another, even if the reasoning is
formally equivalent.

This is akin to the 'positive transfer' of skills, as discussed with the sport
science. So-called 'negative transfer', its opposite, is to be avoided at all
costs. For example, if I try to play squash to improve my racket skills, I
will ruin my tennis game. The racket skills are subtly different, due to the
'wrist flick' in the squash technique.

And I _have_ noticed some transfer of skills and maybe even of the virtue of
patience both between the various sports and between the sports and other
repetitive activities, such as grading. There is a distinctive feeling I have
when I am half-way through something, and where I am fairly confident I can
finish it, and a kind of relaxation past the half-way point where I become
more patient, and time seems to flow “better”. For instance, I can compare how
tired I feel half-way through a long set of climbs and how tired I feel half-
way through a 2 km swim, and the comparison can give me some strength. Similar
positive thinking can happen while grading, things like “I can do it” or
“There isn’t all that much left.” Though there are also differences between
the sports and the grading, because in grading the quality of the work matters
a lot more, and since I am not racing against myself so there is no point of a
burst of speed at the end if I find myself with an excess of energy. Pacing is
also much less important for grading.

Now, one might have technical worries about saturated nonmeasurable sets
figuring in decisions. I do. (E.g., see the Axiom of Choice chapter in my
infinity book.) But now instead of supposing saturated nonmeasurable sets,
suppose a case where an agent subjectively has literally no idea whether some
event _E_ will happen—has no probability assignment for _E_ whatsoever, not
even a ranged one (except for the full range from 0 to 1). The spinner landing
on a set believed to be saturated nonmeasurable might be an example of such a
case, but the case could be more humdrum—it’s just a case of extreme
agnosticism. And now suppose that the agent is told that if they so opt, then
they will get something nice on _E_ and a deserving stranger will get
something nice otherwise.

**Response:** That may be right in the simple case. But now imagine that the
“red” set is a saturated nonmeasurable subset of the spinner edge, and the
“green” set is also such. A saturated nonmeasurable subset has no reasonable
probability assignment, not even a non-trivial range of probabilities like
from 1/3 to 1/2 (at best we can assign it the full range from 0 to 1). Now the
reason-giving strength of a chancy outcome is proportionate to the
probability. But in the saturated nonmeasurable case, there is no probability,
and hence no meaningful strength for the red-based reason or for the green-
based reason. But there is a meaningful strength for the red-or-green moral-
cum-prudential reason. The red-or-green-based reason hence does not reduce to
two separate reasons, one moral and one prudential.

So what should we say? One possibility is to say that there are _only_ reasons
of one type, say the moral. I find that attractive. Then benefits to yourself
also give you _moral_ reason to act, and so you simply have a moral reason to
spin the spinner. Another possibility is to say that in addition to moral and
prudential reasons there is some third class of “mixed” or “combination”
reasons.

**Objection:** The chance _p_ of the spinner landing on red is a prudential
reason and the chance 1 − _p_ of its landing on green is a moral reason. So
you have _two_ reasons, one moral and one prudential.

One might think that reasons for action are exhaustively and exclusively
divided into the moral and the prudential. Here is a problem with this.
Suppose that you have a spinner divided into red and green areas. If you spin
it and it lands into red, something nice happens to you; if it lands on green,
something nice happens to a deserving stranger. You clearly have reason to
spin the spinner. But, assuming the division of reasons, your reason for
spinning it is neither moral nor prudential.

**Final remark:** The argument applies to any exclusive and exhaustive
division of reasons into “simple” (i.e., non-combination) types.

It is correct to say that the Greeks discovered an incommensurability fact.
But it is, I think, worth noting that this incommensurability fact is not
really geometric fact: it is a geometric-cum-arithmetical fact. Here is why.
The claim that two line segments are commensurable says that there are
positive integers _m_ and _n_ such that _m_ copies of the first segment have
the same length as _n_ copies of the second. This claim is essentially
arithmetical in that it quantifies over positive integers.

Don't you rather mean _"Th(something)"_ to be our "intended" model of
"something", such that "something" could be the naturals N, such that Th(N) is
our "intended" model of naturals N?!?  
  
Why isn't Th(N) recursively axiomattizaböe though?!?

I was thinking of the decidability of Th(N), where N is our "intended" model
of the naturals, not of the decidability of any particular recursive
axiomatization. (Th(N) is not recursively axiomatizable, of course.)

And because pure (Tarskian) geometry is decidable, while the theory of the
positive integers is not decidable, the positive integers are not definable in
terms of pure geometry, so we cannot eliminate the quantification over
positive integers. In fact, it is known that the rational numbers are not
definable in terms of pure geometry either, so neither the incommensurability
formulation nor theory irrationality formulation is a purely geometric claim.

I think it is sometimes said that it is anachronistic to attribute to the
ancient Greeks the discovery that the square root of two is irrational,
because what they discovered was a properly _geometrical_ fact, that the side
and diagonal of a square are incommensurable, rather than a fact about real
numbers.

I just noticed that you mentioned capturing video footage for Guinness, so I
suppose that answers my prior question. I apologize for the oversight.
Congrats again!

As we say in Australia, you are 'a gun'! Have you ever thought about writing
philosophically about climbing? Perhaps even just in the vaguely existential
vein that Murakami mines in his 'What I Talk About When I Talk About Running'.

A Kindle Fire running a pre-release version of my [Giant
Stopwatch](https://play.google.com/store/apps/details?id=omegacentauri.mobi.simplestopwatch&hl=en_US&gl=US)
app provided unofficial timing for audience to see and for my pacing. I had to
modify the app to have a periodic beep to meet Guinness's requirements of an
audible stop signal.

I am guessing that if one is doing one of the longer records, say an 8 hour
one, they aren't going to want to watch all of the video, except maybe at high
speed, and so looking at notable moments might make sense.  
  
I expect they also choose some small fraction of the records to feature on
social media, and I could see them using the notable moments list to extract
video for that.  
  
I pity those who don't have programming skills, though. Inserting a running
count of laps into a video by manually inserting a title into the video at
each point with a video editor program would be as much an endurance sport as
actually doing the laps. It took me a while to type up the times of all the
climb tops, but after that I just had the computer automatically generate a
video track for the inset window with time and counts.

I spent over a whole day documenting things for Guinness. They want photos,
attempt video (with running count--there was more python scripting to generate
that, plus looking at the video to figure out the times of all the ascents),
statements from two witnesses and two timekeepers for the attempt, a
measurement statement from a "surveyor or other qualified person" (a geology
grad student in our case), a measurement video, two witnesses for the
measurement, an index to the photos, documentation of timekeeper and witness
qualifications, and a notable moments list for the video.  
  
On the bright side, all this means that their records are probably pretty
reliable.  
  
It's now submitted, and they should respond in three months. Fingers crossed.
For GBP 500, they can do a rush review in five days, but why bother?

On the ground there was a sheet of paper with the start and end times of each
break printed in large letters (calculated by [this
script](https://gist.github.com/arpruss/10e364904dfcc19b043a594232e0acde)), as
well as the mid-point time for each set of 10 to keep me better on pace.

The route was a standard 5.7 grade for most of my training (including when I
unofficially beat the records), with Rock management kindly agreeing to keep
the route up for several months for me. For the final attempt, we added holds
to make the finish at the top of the wall, and changed three other holds to
easier ones. (Guinness has no route grade requirements.)

I trained for about three months, not very heavily. In training did two
unofficial full-length practice runs, and in each I beat the previous record:
in the first one I got 947.1 meters and in the second I got 1004.5, so I was
pretty confident I could beat the 928.5 meters on the official attempt (though
I was still pretty nervous). I also trained by doing a small number of
approximately 1/2 or 1/3 sized practices (maybe three or so), and more regular
shorter runs (1-10 climbs) at fast pace.

I wore moderately worn (one small hole) and comfortable 5.10 Anasazi shoes, a
Camp USA Energy harness, shorts and a T-shirt. (I have not received any
sponsorship.) My belayer used a tube-style device and wore belay gloves.

Most of my practice was with an auto-belay, and at a shorter distance per
climb (and hence greater number of climbs needed) since the auto-belay makes
it impossible to get to the top of the wall. The auto-belay is also spring
loaded so it effectively decreases body weight (by 7 lbs at the bottom
according to my measurement). Then a couple of weeks ago the auto-belay was
closed by management due to a maintenance issue, and I had a break in training
until the Wednesday before the official attempt when I trained with a manual
belay.

I climbed in sets of 10. The planned pace was 8:18 per set and a 44-45 second
rest between sets (clock runs during rests,), averaging at 49.8 seconds per
climb including descent. I was always ahead of pace, and I occasionally took a
mini break at the mid-point time if I was too far ahead.

And now for something not very philosophical. Today, in front of two witnesses
and two timekeepers and with the help of Levi Durham doing an amazing feat of
belaying me for an hour, I beat the [Guinness World
Record](https://www.guinnessworldrecords.com/world-records/438677-greatest-
vertical-distance-climbed-on-an-artificial-climbing-wall-in-1-hour-indi) in
greatest vertical distance climbed in one hour on an indoor climbing wall. The
previous record was 928.5m and I did 1013.7m (with about half a minute to
spare). On Baylor's climbing wall, this involved 67 climbs divided into sets
of 10 (the last was 7), with about a minute of rest between sets (the clock
kept on running during the rest).

The top of the wall is 15.13 meters vertically from the ground (as measured by
a geology grad student), at 3.5 degree slab.

Wow, I had no idea that their confirmation process was so extensive! I wonder
what purpose the "notable moments" list is meant to be playing, especially
since they already have the video itself.

In the morning I stress-baked pumpkin muffins for myself and the volunteers. I
had the muffins, water and loose chalk on a table for use during breaks.

I was actually really stressed yesterday. I had already beaten the record
unofficially in practice, but with lots of people coming to watch (local
climbers, grad students and colleagues), I didn't want to disappoint.
Contributing to the stress was that the conditions changed from my practice
runs (the auto-belay was down from maintenance and I had to use a manual
belay; this involved lengthening the route slightly, increasing the length of
each set and decreasing the total count, as well as an effective increase in
my weight since the auto-belay subtracts a few pounds due to its spring-
loading), and that I hadn't been able to train on the particular route for
several weeks prior to Wednesday due to the auto-belay being down for
maintenance while on Wednesday's approximately half-distance-at-half-time
practice I was more tired at the end than I should have been due to poor
pacing.

You should make a living out of that, Alex.  
Would give you a lot less stress.

Since Guinness requires video proof in addition to human witnesses, in the
interests of redundancy, I had three cameras pointed at the attempt. The best
footage (above) is from a Sony A7R2 with a zoom lens at 16mm, producing 1080P
at 59.94 fps. Video was processed with Adobe Premiere Rush. The processing
consisted of trimming the start and end, and adding a timing video track I
generated with a [Python OpenCV2
script](https://gist.github.com/arpruss/3a705c859d4ea4a482c4e47e96a08f79),
synchronized with single-frame precision at the 1:00:00 point with the footage
of Giant Stopwatch (barely visible under the table towards the end of the
video; early in the video, glare hides it). For the unofficial version I link
above, I accelerated the middle climbs 10X in Premiere Rush.

About half-way through, I ducked into the storage area inside the rock and
changed to a dry shirt.

Whether such a limited way of waging war could be successful probably depends
on the case. If one combined the non-lethal (or not intentionally lethal)
means with technological and numerical superiority, it wouldn’t be surprising
to me if one could win.

I’ve been wondering whether it is possible for a country to count as pacifist
and yet wage a defensive war. I think the answer is positive, as long as one
has a moderate pacifism that is opposed to lethal violence but not to all
violence. I think that a prohibition of all violence is untenable. It seems
obvious that if you see someone about to shoot an innocent person, and you can
give the shooter a shove to make them miss, you presumptively should.

Third, we might subdivide moderate pacifists based on whether they prohibit
all violence that foreseeably leads to death or just violence that
intentionally leads to death. If it is only intentionally lethal violence that
is forbidden, then quite a bit of modern warfare can stand. If the enemy is
attacking with tanks or planes, one can intentionally destroy the tank or
plane as a weapon, while only foreseeing, without intending, the death of the
crew. (I don’t know how far one can take this line without sophistry. Can one
drop a bomb on an infantry unit intending to smash up their rifles without
intending to kill the soldiers?) Similarly, one can bomb enemy weapons
factories.

Second, “lethal” weapons can be used less than lethally. For instance, with
modern medicine, abdominal gunshot wounds are only 10% fatal, yet they are no
doubt very effective at stopping an attacker. While it may seem weird to
imagine a pacifist shooting someone in the stomach, when the chance of
survival is 90%, it does not seem unreasonable to say that the pacifist could
be aiming to stop the attacker non-lethally. After all, tasers sometimes kill,
too. They do so less than 0.25% of the time, but that’s a difference of degree
rather than of principle.

First, we have “officially” non-lethal weapons: tasers, gas, etc. Some of
these might violate current international law, but it seems that a pacifist
country could modify its commitment to some accords.

Very well. Now consider this on a national level. Suppose there are a million
enemy soldiers ordered to commit genocide against ten million, and you have
two ways to stop them:

These may seem to be consequentialist arguments. I don't think so. I don't
have the same intuitions if we replace the general by the general's innocent
child in (2) and (4), even if killing the child were to stop the war (e.g., by
making the general afraid that their other children would be murdered).

Alex  
  
Of course one should never murder anyone, but the question is: would killing
Putin be murder?

Imagine a moderate pacifist who rejects lethal self-defense, but allows non-
lethal self-defense when appropriate, say by use of tasers.

Alex  
  
Doesn't you post here entail that we should murder Putin?

Now, imagine that one person is attacking you and nine other innocents, with
the intent of killing the ten of you, and you can stop them with a taser.
Surely you should, and surely the moderate pacifist will say that this is an
appropriate use case for the taser.

I think (4) is still morally preferable to causing the kind of disruption to
the lives of a million people that plan (3) would involve.

Note that a version of this argument goes through even if the moderate
pacifist backs up and says that tasers are too lethal. For suppose instead of
tasers we have drones that destroy the dominant hand of an enemy soldier while
guaranteeing survival (with science fictional medical technology). It’s
clearly right to release such a drone on a soldier who is about to kill ten
innocents. But now compare:

So maybe our choice is between tasing a million, thereby non-intentionally
killing 250 soldiers, and intentionally killing one general. It seems to me
that (2) is morally preferable, even though our moderate pacifist has to allow
(1) and forbid (2).

One should never murder anyone. Killing, on the other hand, can sometimes be
justified.  
  
One question about killing the leader of an invading country is whether the
leader counts as a civilian, since the killing of civilians is forbidden by
the Geneva Convention. There is also some worry about the Geneva Convention's
killing by "perfidy", which is taken to rule out at least some assassinations.
So, as a matter of positive international law, it seems a difficult question.  
  
Were there no international law on the matter, I wouldn't see a significant
difference between killing a political leader and killing a general, if both
are giving orders to fight. Morally speaking, but not necessarily in
international law, both seem to me to be equally combatants.  
  
Besides the moral and legal questions, there is also a prudential question. In
my post I assumed that killing the general would stop the invasion. I got to
assume that because I was making up the case. Whether in actual fact killing a
leader would stop an invasion is less clear. Indeed such a thing might be seen
as such a serious attack on the country that the retaliation might be really
horrific.

If you can tase one person to stop the murder of ten, then (1) should be
permissible if it’s the only option. But tasers occasionally kill people. We
don’t know how often. Apparently it’s [less than 1 in
400](https://www.usatoday.com/in-depth/news/investigations/2021/04/23/police-
use-tasers-ends-hundreds-deaths-like-daunte-wright/7221153002/) uses. Suppose
it’s 1 in 4000. Then option (1) results in 250 enemy deaths.

Maybe the answer to both questions is that I could, but only metaphysically
and not causally. In other words, it could be that the laws of nature, or of
human nature, make it impossible for me to exercise one of the powers without
the other, just as I cannot wiggle my ring finger without wiggling my middle
finger as well. On this view, if there is a God, he could cause me to acquire
promissory-type obligations without my promising, and he could let me engage
in the natural act of promising while blocking the exercise of normative power
and leaving me normatively unbound. This doesn’t seem particularly
problematic.

A normative power is a power to change a normative condition.
[Raz](https://core.ac.uk/download/pdf/230182259.pdf) says the change is not
produced “causally” but “normatively”.

I think the difficulty with a causal model is the fact that in paradigm cases
of normative power, there is a natural power that _is_ being exercised, and we
have the intuition that the exercise of the natural power is necessary and
sufficient for the normative effect. But on a causal model, why couldn’t I
cause a promissory-type obligation without promising, simply causing the
relevant property of being obligated to come to be instantiated in me? And why
couldn’t I engage in the speech act while yet remaining normatively unbound,
because my normative power wasn’t exercised in parallel with the natural
power?

Here is a picture on which this is correct. We exercise a normative power by
exercising a natural power in such a context that the successful exercise of
the natural power is partly constitutive of a normative fact. For instance, we
utter a promise, thereby exercising a natural power to engage in a certain
kind of speech act, and our exercise of that speech act is partly constitutive
of, rather than causal of, the state of affairs of our being obligated to
carry out the promised action.

But why not allow for a causal model? Why not suppose that a normative power
is a causal power to make an irreducible normative property come to be
instantiated in someone? Thus, my power to promise is the power to cause
myself to be obligated to do what I have promised.

Perhaps the real problem for a lot of people with a causal view of normative
powers is that it tends to lead to a violation of supervenience. For if it is
metaphysically possble to have the exercise of the normative power without the
exercise of the natural power, or vice versa, then it seems we don’t have
supervenience of the normative on the non-normative. But supervenience does
not seem to me to be inescapable.

There are two versions of the above model. On one version, there is an
underlying fundamental conditional normative fact _C_ , such as that if I have
promised something then I should do it, and my exercise of normative power
supplies the antecedent _A_ of that conditional, and then the normative
consequent of _C_ comes to be grounded in _C_ and _A_. On another version,
there there are some natural acts that are directly constitutive of a
normative state of affairs, not merely by supplying the antecedent of a
conditional normative fact. I think the first version of the model is the more
plausible in paradigmatic cases.

For the strong version to have any plausibility, “good” must include cases of
purely instrumental goodness.

**Strong** : Whenever you act, you act for an end, and every end you act for
you perceive as good.

I think there is still reason to be sceptical of the strong version.

**Case 2:** Back when they were dating in high school, Bob promised to try his
best to bake a nine-layer chocolate cake for Alice’s 40th birthday. Since
then, Bob and Alice have had a falling out, and hate each other’s guts.
Moreover, Alice and all her guests hate chocolate. But Alice doesn’t release
Bob from his promise. Bob tries his best to bake the cake in order to fulfill
his promise, and happens to succeed. In trying to bake the cake, Bob acted for
the end of producing a cake. But producing the cake was worthless, since no
one would eat it. The only value was in the trying, since that was the
fulfillment of his promise.

**Case 1:** There is some device which does something useful when you trigger
it. It is triggered by electrical activity. You strap it on to your arm, and
raise your arm, so that the electrical activity in your muscles triggers the
device. Your raising your arm has the arm going up as an end, but that end is
not perceived as good, but merely neutral. All you care about is the
electrical activity in your muscles.

But perhaps we can say this. We have a normative power to endow some neutral
things with value by making them our ends. And in fact the only way to act for
an end that does not have any independent value is by exercising that
normative power. And exercising that normative power involves your seeing the
thing you’re endowing with value as valuable. And maybe the only way to raise
your arm or for Bob to bake the cake in the examples is by exercising the
normative power, and doing so involves seeing the end as good. Maybe. This has
some phenomenological plausibility and it would be nice if it were true,
because the strong guise of the good thesis is pretty plausible to me.

I was going to leave it at this. But then I thought of a way to save the
strong guise of the good thesis. Success is valuable as such. When I try to do
something, succeeding at it has value. So the arm going up or the cake being
produced _are_ valuable as necessary parts of the success of one’s action. So
perhaps every end of your action _is_ trivially good, because it is good for
your action to succeed, and the end is a (constitutive, not causal) means to
success.

This isn’t quite enough for a defense of the strong thesis. For even if the
success is good, it does not follow that you perceive the success as good. You
might subscribe to an axiological theory on which success is not good in
general, but only success at something good.

In both cases, it is still true that the agent acts for a good end—the useful
triggering of the device and the production of the cake. But in both cases it
seems they are also acting for a worthless end. Thus the cases seem to fit
with the weak but not the strong guise of the good thesis.

According to the guise of the good thesis, one always acts for the sake of an
apparent good. There is a weaker and a stronger version of this:

**Weak** : Whenever you act, you act for an end that you perceive is good.

Is this possible? I think so. We just need to
[distinguish](http://alexanderpruss.blogspot.com/2019/08/two-ways-to-pursue-y-
for-sake-of-z.html) between pursuing victory for the sake of something else
that follows from victory and pursuing victory for the sake of something that
might follow from the pursuit of victory.

Clearly the prudent thing to do is to try to win. For if you don’t try to win,
then you are guaranteed not to get any money. But if you do try, you won’t
lose anything, and you might gain.

Suppose Alice can read your mind, and you are playing poker against a set of
people not including Alice. You don’t care about winning, just about money.
Alice has a deal for you that you can’t refuse.

Here is the oddity: you are trying to win in order to get paid, but you only
get paid if you don’t win. Thus, you are trying to achieve something, the
achievement of which would undercut the end you are pursuing.

If you lose, but you tried to win, she pays you double what you lost.

_s_ 1( _x_ , _t_ ) = _a_ if _r_ < _x_ and _t_ = 1 or if _x_ < 1 − _r_ and _t_
= 0

_s_ 1( _x_ , _t_ ) = − _b_ if _r_ < _x_ and _t_ = 0 or if _x_ < 1 − _r_ and
_t_ = 1.

_s_ 1( _x_ , _t_ ) = 0 if 1 − _r_ ≤ _x_ ≤ _r_

Plausibly, then, if we imagine Alice has some evidence for a truth _p_ that is
insufficient for knowledge, and slowly and continuously her evidence for _p_
mounts up, when the evidence has crossed the threshold needed for knowledge,
the value of Alice’s state with respect to _p_ will have suddenly and
discontinuously increased.

Suppose there is a distinctive and significant value to knowledge. What I mean
by that is that if two epistemic are very similar in terms of truth, the level
and type of justification, the subject matter and its relevant to life, the
degree of belief, etc., but one is knowledge and the other is not, then the
one that is knowledge has a significantly higher value because it is
knowledge.

If we think that it’s never rational for a rational agent to refuse free
information, then the above argument can be made rigorous to establish that
any discontinuous rise in the epistemic value of credence at the point at
which knowledge of a truth is reached is exactly mirrored by a discontinuous
fall in the epistemic value of a state of credence where seeming-knowledge of
a falsehood is reached. Moreover, the rise and the fall must be in the ratio 1
− _r_ : _r_ where _r_ is the knowledge threshold. Note that for knowledge, _r_
is plausibly pretty large, around 0.95 at least, and so the ratio between the
special value of knowledge of a truth and the special disvalue of evidence-
sufficient-for-knowledge for a falsehood will need to be at most 1:19. This
kind of a ratio seems intuitively implausible to me. It seems unlikely that
the special disvalue of evidence-sufficient-for-knowledge of a falsehood is an
order of magnitude greater than the special value of knowledge. This
contributes to my scepticism that there is a special value of knowledge.

Suppose that _a_ and _b_ are positive and _a_ / _b_ = (1− _r_ )/ _r_. Then if
my scribbled notes are correct, it is straightforward but annoying to check
that _s_ 1 is proper, and it has a discontinuous reward _a_ for meeting
threshold _r_ with respect to a truth and a discontinuous penalty  − _a_ for
meeting threshold _r_ with respect to a falsehood. To get a strictly proper
scoring rule, just add to it any strictly proper continous accuracy scoring
rule (e.g., Brier).

This hypothesis initially seemed to me to have an unfortunate consequence.
Suppose Alice has just barely exceeded the threshold for knowledge of _p_ ,
and she is offered a cost-free piece of information that may turn out to
slightly increase or slightly decrease her overall evidence with respect to
_p_ , where the decrease would be sufficient to lose her knowledge of _p_
(since she has only “barely” exceeded the evidential threshold for knowledge).
It seems that Alice should refuse to look at the information, since the
benefit of a slight improvement in credence if the evidence is non-misleading
is outweighed by the danger of a significant and discontinuous loss of value
due to loss of knowledge.

But that’s not quite right. For from Alice’s point of view, because the
threshold for knowledge is not 1, there is a real possibility that _p_ is
false. But it may be that just as there is a discontinuous gain in epistemic
value when your (rational) credence becomes sufficient for knowledge of
something that is in fact true, it may be that there is a discontinuous loss
of epistemic value when your credence becomes sufficient for knowledge of
something false. (Of course, you can’t know anything false, but you can have
evidence-sufficient-for-knowledge with respect to something false.) This is
not implausible, and given this, by looking at the information, by her lights
Alice also has a chance of a significant gain in value due to losing the
illusion of knowledge in something false.

Can we rigorously model this kind of an epistemic value assignment? I think
so. Consider the following discontinuous accuracy scoring rule _s_ 1( _x_ ,
_t_ ), where _x_ is a probability and _t_ is a 0 or 1 truth value:

Let _α_ > 0 be the “squishiness” of our credences. Let’s say that for one
credence to be definitely bigger than another, their difference has to be at
least _α_ , and that to definitely meet (fail to meet) a threshold, we must be
at least _α_ above (below) it. We assume that our threshold _r_ is definitely
less than one: _r_ \+ _α_ ≤ 1.

Let _z_ = _r_ − _α_ \+ _β_ (1−( _r_ − _α_ )) = (1− _β_ )( _r_ − _α_ ) + _β_.
This is the prior probability of a positive test result. We will definitely
meet _r_ on a positive test result just in case we have ( _r_ − _α_ )/ _z_ =
_P_ ( _p_ | _E_ ) ≥ _r_ \+ _α_ , i.e., just in case

(We definitely won’t meet _r_ on a negative test result.) Thus to get (c)
definitely true, we need (3) to hold as well as the probability of a positive
test result to be at least _r_ \+ _α_ :

Note that by appropriate choice of _β_ , we can make _z_ be anything between
_r_ − _α_ and 1, and the right-hand-side of (3) is at least _r_ − _α_ since
_r_ \+ _α_ ≤ 1. Thus we can make (c) definitely hold as long as the right-
hand-side of (3) is bigger than or equal to the right-hand-side of (4), i.e.,
if and only if:

What does this tell us about _r_ and _α_? We can actually figure this out.
Consider a test for _p_ that have no false negatives, but has a false positive
rate of _β_. Let _E_ be a positive test result. Our best bet to generating a
counterexample to (a)–(c) will be if the priors for _p_ are as close to _r_ as
possible while yet definitely below, i.e., if the priors for _p_ are _r_ −
_α_. For making the priors be that makes (c) easier to definitely satisfy
while keeping (b) definitely satisfied. Since there are no false negatives,
the posterior for _p_ will be:

In [a recent post](http://alexanderpruss.blogspot.com/2023/01/knowing-you-
will-soon-have-enough.html), I noted that it is possible to cook up a Bayesian
setup where you don’t meet some threshold, say for belief or knowledge, with
respect to some proposition, but you do meet the same threshold with respect
to the claim that after you examine a piece of evidence, then you _will_ meet
the threshold. This is counterintuitive: it seems to imply that you can know
that you will have enough evidence to know something even though you don’t
yet. In a comment, Ian noted that one way out of this is to say that beliefs
do not correspond to sharp credences. It then occurred to me that one could
use the setup to probe the question of how sharp our credences are and what
the thresholds for things like belief and knowledge are, perhaps
complementarily to the considerations in [this
paper](https://philarchive.org/rec/PRUBSA).

It’s in fact not hard to see that (6) is necessary and sufficient for the
existence of a case where (a)–(c) definitely hold.

_P_ ( _p_ | _E_ ) = _P_ ( _p_ )/ _P_ ( _E_ ) = ( _r_ − _α_ )/( _r_ − _α_ + _β_
(1−( _r_ − _α_ ))).

We thus have our joint constraint on the squishiness of our credences: bad
things happen if our credences have a level of precision equal to the right-
hand-side of (6). What exactly that says about _α_ depends on where the
relevant threshold lies. If the threshold _r_ is 1/2, the squishiness _α_ is
0.15. That’s surely higher than the actual squishiness of our credences. So if
we are concerned merely with the threshold being more-likely-than-not, then we
can’t avoid the paradox, because there will be cases where our credence is
definitely below the threshold and it’s definitely above the threshold that
examination of the evidence will push us about the threshold.

we meet _r_ with respect to the proposition that we will meet the threshold
with respect to _p_ after we examine evidence _E_.

We thus have our joint constraint on the squishiness of our credences: bad
things happen if our credences are so precise as to make (6) true with respect
to a threshold _r_ for which we don’t want (a)–(c) to definitely hold. The
easiest scenario for making (a)–(c) definitely hold will be a binary test with
no false negatives.

Note that the squishiness of our credences likely varies with where the
credences lie on the line from 0 to 1, i.e., varies with respect to the
relevant threshold. For we _can_ tell the difference between 0.999 and 1.000,
but we probably can’t tell the difference between 0.700 and 0.701. So the
squishiness should probably be counted relative to the threshold. Or perhaps
it should be correlated to log-odds. But I need to get to looking at grad
admissions files now.

For suppose we have a credence threshold _r_ and that our intuitions agree
that we can’t have a situation where:

But what’s a reasonable threshold for belief? Maybe something like 0.9 or
0.95. At _r_ = 0.9, the squishiness needed for paradox is _α_ = 0.046. I
suspect our credences are more precise than that. If we agree that the
squishiness of our credences is less than 4.6%, then we have an argument that
the threshold for belief is _more_ than 0.9. On the other hand, at _r_ = 0.95,
the squishiness needed for paradox is 2.4%. At this point, it becomes more
plausible that our credences lack that kind of precision, but it’s not clear.
At _r_ = 0.98, the squishiness needed for paradox dips below 1%. Depending on
how precise we think our credences are, we get an argument that the threshold
for belief is something like 0.95 or 0.98.

When someone’s conscience mistakenly requires something that violates an
objective moral rule, there is a two-fold benefit to that person from a law
incentivizing following the moral rule. The law is a teacher, and the state’s
disapproval may change one’s mind about the matter. And even if it a harm to
one to violate conscience, it is also a harm to one to do something wrong even
inculpably. Thus, the harm of violating conscience is somewhat offset by the
benefit from not doing something else that is wrong.

A reasonable optimism says that in most cases most people’s consciences are
correct. Thus typically we would expect that most violators of a legitimate
law will not be acting out of conscience—for a necessarily condition for the
legitimacy of a law is that it does not conflict with a correct conscience.
Thus, even if there is the rare murderer acting from mistaken conscience, most
murderers act against conscience, and by incentivizing abstention from murder,
in most cases the law _helps_ people follow their conscience, and the small
number of other cases can be tolerated as a side effect. Thus the
considerations of conscience favor intolerant laws in such cases. Nonetheless,
there are cases where most violators of a law would likely be acting from
conscience. Thus, if we had a law requiring eating meat, we would expect that
most of the violators would be conscientious. Similarly, a law against
something—say, the wearing of certain clothes or symbols—that is rarely done
except as a religious practice would likely be a law most violators of which
would be conscientious.

The difficult case, however, is when people’s consciences are mistaken to such
a degree that conscience requires them to do something that unjustly harms
others. (A less problematic mistake is when conscience is mistaken to such a
degree that conscience requires them to do something that’s permissible, but
not wrong. In those cases, tolerance is clearly called for. We shouldn’t
pressure vegetarians to eat animals even if their conscientious objection to
eating animals happens to be mistaken.)

In a just defensive war, to refuse to fight to defend one’s fellow citizens
without special reason (perhaps priests and doctors should not kill) is wrong.
But a grave harm is done to a conscientious objector who is gotten to fight by
legal incentives. Let’s think through the five considerations above. The first
mainly applies to laws prohibiting a behavior rather than ones requiring a
behavior. Short of brainwashing, it is impossible to _make_ someone fight. (We
could superglue their hands to a gun, and then administer electric shocks
causing their fingers to spasm and fire a bullet, but that wouldn’t count as
_fighting_.) The second applies somewhat: we do need to weigh the harms to
innocent citizens from enemy invaders, harms that might be prevented if our
conscientious objector fought. But note that there is something rather
speculative about these harms. Someone who fights contrary to conscience is
unlikely to be a very effective fighter, and it is far from clear that their
military activity would actually prevent any actual harm to innocents. Now,
regarding the third consideration, one can design a conscription law with an
exemption that few who aren’t conscientious objectors would take advantage of.
One way to do this is to require evidence of one’s conscience’s objection to
fighting (e.g., prior membership in a pacifist organization). Another way is
to impose non-combat duties on conscientious objectors that are as onerous and
maybe as dangerous as combat would be. Regarding the fourth consideration, it
seems unlikely that a typical conscientious objector’s objections to war would
be changed by legal penalties. And the fifth seems a weak consideration in
general. Putting all these together, we do not outweigh the _prima facie_
considerations against pressuring conscientious objectors to act against their
(mistaken) conscience from the harms in going against conscience.

One of the central insights of Western philosophy, beginning with Socrates,
has been that few if any things are as bad for an individual as culpably doing
wrong. It is better, we are told through much of the Western philosophical
tradition, that it is better to suffer than do injustice.

That said, I think a qualification is plausible. Some wrongdoings are minor,
and in those cases the harm to the wrongdoer may be minor as well. But in any
case, to get someone to act against their conscience in a matter that
according to their conscience is major is to do them grave harm, a harm not
that different from death.

In cases where doing wrong and suffering wrong are of roughly the same order
of magnitude, it is very intuitive that we should prevent the suffering of
wrong rather than the doing of wrong. Imagine that Alice is drowning while at
the same time Bob is getting ready to assassinate a politician, but we know
for sure that Bob’s bullets have all been replaced with blanks. If our choice
is whether to try to dissuade Bob from attempting murder or keep Alice from
drowning, we should keep Alice from drowning, evne if on the Socratic view the
harm to Bob from attempting murder will be greater than that to Alice from
drowning. (I am assuming that in this case the two harms are nonetheless of
something like the same order of magnitude.)

The harm of violating one’s conscience only happens to one if one _willingly_
violates one’s conscience. If law enforcement physically prevents me from
doing something that conscience requires from me, then I haven’t suffered the
harm. Thus, interestingly, the consideration I sketched against violating
one’s conscience does not apply when one is literally forced (fear of
punishment, unless it is severe enough to suspend one’s freedom of will, does
not actually _force_ , but only incentives).

Now, the state, just like individuals, should _ceteris paribus_ avoid causing
grave harm. Hence, the state should generally avoid getting people to do
things that violate their conscience in major matters.

In some cases the person of mistaken conscience will still do the wrong deed
despite the law’s contrary incentive. In such a case, both the perpetrator and
the victim may be slightly better off for the law. The victim has a dignitary
benefit from the very fact that the state says that the harm was unlawful.
That dignitary benefit may be a cold comfort if the victim suffered a grave
harm, but it is still a benefit. And the perpetrator is slightly better off,
because following one’s conscience _against_ external pressure has an element
of admirability even when the conscience is mistaken.

Now, acting against one’s conscience _is_ always wrong, and is almost always
_culpably_ wrong. For the most common case when doing something wrong isn’t
culpable is that one is ignorant of the wrongness, but when one acts against
one’s conscience one surely isn’t ignorant that one is acting against
conscience, and that we ought follow our conscience is obvious.

One might think that what I said earlier implies that in this difficult case
the state should always allow people to follow their conscience, because after
all it is worse to do wrong—and violating conscience is wrong—than to have
wrong done to one. But that would be absurd and horrible—think of a racist
murderer whose faulty conscience requires them to kill.

Nonetheless, there will be cases where these considerations do not suffice,
and the law should be tolerant of mistaken conscience.

Suppose I am just the slightest bit short of the evidence needed for belief
that I have some condition _C_. I consider taking a test for _C_ that has a
zero false negative rate and a middling false positive rate—neither close to
zero nor close to one. On reasonable numerical interpretations of the previous
two sentences:

If the test comes out positive, it will be another piece of evidence for the
hypothesis that I have _C_ , and it will push me over the edge to belief that
I have _C_.

I don’t yet have enough evidence to know that I have _C_ , but I know that in
a moment I will.

Ian:  
  
I don't think the worries about sharpness of credences apply. For we can
suppose that the cases we apply the argument to are precisely ones with
sufficiently sharp credences. They might be cases concerning gambling
scenarios, or they could be medical cases where the only relevant data one has
is statistics from the most recent publications on the subject.  
  
I think all we really need for the argument is that in certain kinds of cases
meeting a credence threshold is what makes the difference between not having a
belief (or a justified belief) and having a belief (or a justified belief).  
  
By the way, suppose we think that the argument doesn't work for sharpness
reasons. Let's say that u is a level of sharpness such that it only makes
sense to say that you've met the threshold if you are at r+u or higher and
that you haven't met it if you are at r-u or lower, and if you're between r-u
and r+u it's indeterminate. Then working through the Bayesian details should
provide one with an interesting joint constraint on r and u. And then we can
ask see if it's reasonable to think that r and u actually satisfy the joint
constraint.

I’d take this as an argument against the view that belief is (or is justified
by) credence above a threshold.  
  
But that view seems implausible in any case. It’s pretty much only in gambling
setups with well-defined chances that we have sharp credences. But we all have
vast numbers of beliefs (taken in an informal ‘folk’ sense). So it seems that
some other approach is needed.  
  
A thorough Bayesian would be untroubled by this example. She would have her
credences, and that would be enough. She would not care whether they exceeded
some arbitrary threshold for belief. She would be similarly untroubled by the
lottery paradox. But no one in practice is a thorough Bayesian, or even close.
So again, it seems that some other approach is needed.

**Appendix:** Suppose the threshold for belief or knowledge is _r_ , where _r_
< 1. Suppose that the false-positive rate for the test is 1/2 and the false-
negative rate is zero. If _E_ is a positive test result, then _P_ ( _C_ | _E_
) = _P_ ( _C_ ) _P_ ( _E_ | _C_ )/ _P_ ( _E_ ) = _P_ ( _C_ )/ _P_ ( _E_ ) = 2
_P_ ( _C_ )/(1+ _P_ ( _C_ )). It follows by a bit of algebra that if my prior
_P_ ( _C_ ) is more than _r_ /(2− _r_ ), then _P_ ( _C_ | _E_ ) is above the
threshold _r_. Since _r_ < 1, we have _r_ /(2− _r_ ) < _r_ , and so the story
(either in the belief or knowledge form) works for the non-empty range of
priors strictly between _r_ /(2− _r_ ) and _r_.

But (6) is absurd: if I know that I will know something, then I am in a
position to know that the matter is so, since that I will know _p_ entails
that _p_ is true (assuming that _p_ doesn’t concern an open future). However,
there is no similar absurdity in (5). I may know that I will have enough
evidence to know _C_ , but that’s not the same as knowing that I will _know_
_C_ or even be in a position to know _C_. For it is possible to have enough
evidence to know something without being in a position to know it (namely,
when the thing isn’t true or when one is Gettiered).

In other words, oddly enough, just prior to getting the test results I can
reasonably say:

If the test comes out positive, I will have enough evidence to know that I
have _C_.

I have enough evidence to _know_ that the test would come out positive,

To see that (2) is true, note that given that the false negative rate is zero,
and the false positive rate is not close to one, I will indeed have non-
negligible evidence for _C_ if the test is positive.

In fact, the same thing can be said about knowledge, assuming there is
knowledge in lottery situations. For suppose that I am just the slightest bit
short of the evidence needed for _knowledge_ that I have _C_. Then I can set
up the story such that:

I have enough evidence to believe that the test would come out positive.

I don’t know that I have _C_ but I know that I will know.

To be clear, I don’t doubt that the example works. It does, and it illustrates
a genuine problem with the view that belief is (or is justified by) credence
above a threshold. Specifically, this violates an apparently natural
reflection principle, that if I rationally believe that I will rationally come
to believe, then I should believe now. Issues like this seem unavoidable with
a threshold (other than 1). The lottery paradox illustrates a different one:
violation of logical closure. Of course, you may be prepared to live with such
issues - maybe beliefs don’t have to be logically consistent.  
  
  
My problem with the threshold approach is more basic. If I don’t have a
credence, I can’t apply the threshold test. If I do have a credence, then
noting whether it is above or below a threshold adds nothing useful. One
response to this is straight Bayesianism, which rejects any role for belief
(beyond credence 1). Another is that credence and belief are separate and
related, but not straightforwardly. In the example, I would say that I
_believe_ the relevant medical information about the chance that I have C and
about the reliability of the test, and set my _credences_ accordingly.

Ian,  
  
I hadn't thought about this in the context of reflection. But technically
there is no counterexample here to van Fraassen's reflection principle, which
only says that P(H | credence at t will be p) = p. My examples satisfy that,
assuming full transparency about my own credences and full certainty that I
will conditionalize (cf.: https://jonathanweisberg.org/pdf/C_R_and_SKv2.SP.pdf
).  
  
The original reflection principle implies that my credence should be the
expected value of my future credences (at a fixed future time; we might also
generalize to any martingale stopping time). This is not a problem in my
cases, because although it is likely that my future credence will be bigger
than it is now, there is a small chance that my future credence will plummet
to zero due to a negative test result, as William notes.  
  
Upon further thought, I think it can be quite reasonable to say: "I know that
tomorrow I will have knowledge-level evidence for p, but I don't yet." And if
so, then my subsequent post on squishiness, even if technically correct, is
moot.

If I am rational, my beliefs will follow the evidence. So if I am rational, in
a situation like the above, I will take myself to have a way of bringing it
about that I believe, and do so rationally, that I have _C_. Moreover, this
way of bringing it about that I believe that I have _C_ will itself be
perfectly rational if the test is free. For of course it’s rational to accept
free information. So I will be in a position where I am rationally able to
bring it about that I rationally believe _C_ , while not yet believing it.

To see that (1) is true, note that the test is certain to come out positive if
I have _C_ and has a significant probability of coming out positive even if I
don’t have _C_. Hence, the probability of a positive test result will be
significantly higher than the probability that I have _C_. But I am just the
slightest bit short of the evidence needed for belief that I have _C_ , so the
evidence that the test would be positive (let’s suppose a deterministic
setting, so we have no worries about the sense of the subjunctive conditional
here) is sufficient for belief.

Still, there is something odd about (5). It’s a bit like the line:

As long as the test could be a true negative, you could always be surprised
and update to P of 0.0 if the test is negative. So waiting on the test before
you update is justified.

An explanation going back to something self-explanatory involves the activity
of a necessary being.

I am not sure I buy (1). But it sounds kind of right to me now. Additionally,
(3) kind of sounds correct on its own. If _A_ causes _B_ and _B_ causes _C_
but there is no explanation of _A_ , then it seems that _B_ and _C_ are really
unexplained. Aristotle notes that there was a presocratic philosopher who
explained why the earth doesn’t fall down by saying that it floats on water,
and he notes that the philosopher failed to ask the same question about the
water. I think one lesson of Aristotle’s critique is that if it is unexplained
why the water doesn’t fall down it is unexplained why the earth falls down.

I have a problem with (2). (2) only follows if a complete explanation is
possible, which is kind of what you want to prove. So, it seems to me (2) begs
the question.  
If the 'water' is a brute fact, 'the earth floats on water' is as far as an
explanation can get. In a way it is merely partial but it isn't actually part
of something complete because there isn't something complete.  
That's why (1) is 'kind of right' but it isn't completely right.  
  
  
  

A partial explanation is one that is a part of a complete explanation.

Dr Pruss, do you think that Bradley type regresses must also stop in some sort
of self explanatory fact? For example instead of having an infinite regress of
relations being related to their relata maybe it is somehow self explanatory
that the relation is related to its relata?

Any explanation for an event _E_ that does not go all the way back to
something self-explanatory is merely partial.

So, if any event _E_ has an explanation, it has an explanation going all the
way back to something self-explanatory. (1,2)

But perhaps we can do better. The necessary biconditional (2) holds in the
case “virtuous human”, but in a kind of trivial way: “a good virtuous human”
is repetition. I think that, as often, we need to pass from a modal to a
hyperintensional characterization. Consider that not only is (1) true, but
also:

So our initial account of what is being asked for is that we are asking for an
attribute _F_ such that:

In other words, that which I am centrally, really, deep-down and essentially
is that which sets the norms for me to be good _simpliciter_.

What am I? A herring-eater, a husband, a badminton player, a philosopher, a
father, a Canadian, a six-footer and a human are all correct answers. There is
an ancient form of the “What is _x_?” question where we are looking for a
“central” answer, and of course “a human” is usually taken to be that one.
What makes for that answer being central?

On the "male human" point: it does seem like I can say "I am good and a man
iff I am a good man." But then (if we take the above approach) it seems like I
am centrally (really, deep-down) a man, not merely a human. This seems to sit
less comfortably with the Athanasian point. Perhaps this depends on the notion
of gender-specific norms? I'm not enough of a philosopher of gender to know
the literature on things like that.  
  

**Objection 1:** Some things are “centrally” electrons, but something’s being
good at electronicity is absurd.

Necessarily, if I am human, what it is for me to be good is for me to be a
good human.

_x_ is “centrally, really, deep-down, essentially” _F_ just in case what it is
for _x_ to be good is for _x_ to be a good _F_.

Necessarily, I am good and a human if and only if I am a good human.

But the same is not true for any other attribute besides “human” among those
of the first paragraph. I can be good and a badminton player while not being a
good badminton player, and I can be a good herring-eater without being good
and a herring-eater. And I have no idea what it is to be a good six-footer,
but perhaps the fact that I am a quarter of an inch short of six feet right
now makes me not be one. (In some cases one direction may hold. It may be that
if I am good and a human, then I am a good human.)

Maybe, but can you say: What it is for you to be good is for you to be a good
man?  
  
Maybe the issue is that there is an ambiguity. A man (in the relevant sense)
is an adult male human. But "good adult male human" is a bit ambiguous: is one
good at the adulting, the maleness or the humanness, or at all three?  
  
I would say that what makes you be good is that you are good at being human,
and given that you are a man, this may have certain implications that it
wouldn't have if you weren't a man. (E.g., that you are a man makes it bad for
you to say "I am not a man".)

Necessarily, I am good and a virtuous human if and only if I am a good
virtuous human.

Sometimes the word “essential” is thrown in: I am _essentially_ human. But
what does that mean? In contemporary analytic jargon, it just means that I
cannot exist without being human. But the “central” answer to “What am I?” is
not just an answer that states a property that I cannot lack. There are, after
all, many such properties essential in such a way that do not answer the
question. “Someone conceived in 1972” and “Someone in a world where 2+2=4”
attribute properties I cannot lack, but are not the central answers.

Andrew:  
  
Nice paper!  
  
But I think classification is not what I am getting at. I am aptly classified
as a male human, a human, a mammal, a vertebrate, an animal, an organism, and
an enmattered thing. However, I think only "human" is the answer to the
question I am after, the "what am I really" question. I am looking for a
particular "deep" classification. This is neither the most general
classification nor the most specific one, but the one that captures what most
deeply I am.  
  
On Athanasian theology, what makes me be saved is that the second person of
the Trinity became human just as I am human. He also became a male human just
as I am a male human and he became a vertebrate just as I am a vertebrate. But
these things do not save me. What saves me is his co-humanity, not his co-
vertebricity or co-animality or co-maleness. There is something deeper about
humanity than about any of these other things.  
  
This may be ethically important, too. Should I care more for a squid or a
mouse, if their intelligence is equal? The mouse is a fellow vertebrate, but
so what? It makes no difference. Similarly, that someone is a fellow male
matters not a whit. However, that someone is a fellow human, that seems to
matter.

**Response:** I deny that it’s absurd. It’s just that all the electrons we
meet _are_ good at electronicity.

In other words, if I am a human, being a good human explains my being good. On
the other hand, even if I were a virtuous human, my being a good virtuous
human would not explain my being good. For redundancy is to be avoided in
explanation, and “good virtuous human” is redundant.

Another interpretation of the question of what we are is that it is asking for
*classification*. Generics can be useful here, because they (a) are apt
answers to classification questions and (b) still allow for exceptions. The
former feature makes them substantive. The latter makes them modest and immune
to certain styles of counterexample. For more on this (with respect to the
question of what *we* are, but the tools at play are widely applicable), see:
https://andrewmbailey.com/GenericAnimalism.pdf

**Objection 2:** “Good” is attributive, and hence there is no such thing as
being good _simpliciter_.

is not _generally_ logically valid. But some instances of a schema can be
logically valid even if the schema is not logically valid in general.

Necessarily, _x_ is a good _F_ if and only if _x_ is good.

And yet “a virtuous human” would not be the answer to the ancient “What am I
centrally, really, deep-down, essentially?” question even if I were in fact a
virtuous human.

So the sense of the “What am I centrally, really, deep-down, essentially?”
question isn’t just modal. What is it?

_x_ is good and _x_ is _F_ if and only if _x_ is a good _F_

Speaking of "God's beliefs" seems about as coherent as discussing what God ate
for lunch. He's not a finite creature.

These three theses, together with some auxiliary assumptions, yield a serious
problem for open theism.

I suppose the best way out is for the open theist to deny (1).

I’d reject (1) in any case. Perfectly rational beings would have no use for
belief. They would have consistent credences (which could be 0 or 1) for
everything. On this view, beliefs are a shortcut that we, with our limited
mental capacity, use to reduce processing load.

Ian:  
  
I am somewhat inclined to say that a high credence just *is* what a belief is.  
  
THToD:  
  
Like you and Alston, I am inclined to agree that "belief" is at best an
awkward way to describe God's cognition. However, I think a major part of the
reason why God doesn't fundamentally have beliefs is that there is no room for
a gap in God between cognition and reality. Whether an open theist can have
the "no gap" view of God depends on the variety of open theism. If the open
theist is an open futurist, who thinks there are no facts in reality about
future contingents, then they can have the "no gap" view. But an open theist
like Swinburne or van Inwagen who thinks there are facts about future
contingents and God doesn't know them thinks there is a gap between God's
knowledge and reality, and now God is sounding very much like us -- there is
stuff that God is uncertain about.

Alex  
  
As Ian says, (1) applies to finite creatures, it doesn't apply to an
omniscient being. On open theism, God knows everything that can be known, but
libertarian free choices cannot be known in advance. If God were to "guess"
what I will do tomorrow, of course on open theism, he could turn out to be
wrong, but it would be a guess and not a belief.

A rational being believes anything that they take to have probability bigger
than 1 − (1/10100) given their evidence.

Consider worlds created by God that contain four hundred people, each of whom
has an independent 1/2 chance of freely choosing to eat an orange tomorrow
(they love their oranges). Let _p_ be the proposition that at least one of
these 400 people will freely choose to eat an orange tomorrow. The chance of
not- _p_ in any such world will be (1/2)400 < 1/10100. Assuming open theism,
so God doesn’t just directly know whether _p_ is true or not, God will take
the probability of _p_ in any such world to be bigger than 1 − (1/10100) and
by (1) God will believe _p_ in these worlds. But in some of these worlds, that
belief will turn out to be false—no one will freely eat the orange. And this
violates (3).

Clearly the follow-through is _intended_ —it’s consciously planned, aimed at,
etc. But it need not be a means to anything one cares about in the game
(though, of course, in some cases it can be a means to impressing the
spectators or intimidating an opponent). But is it an end? It seems pointless
as an end!

Yet it seems that whatever is intended is intended as a means or an end. One
might reject this principle, taking follow-through to be a counterexample.

This is interesting action-theoretically. The follow-through appears
pointless, because the agent’s interest is in what happens _before_ the
follow-through, the impact’s having the right physical properties, and yet
there is surely no backwards causation here. But there not appear to be an
effective way to reliably secure these physical properties of the impact
except by trying for the follow-through. So the follow-through itself is
pointless, but one’s _aiming at_ or _trying for_ the follow-through very much
has a point. And here the order of causality is respected: one swings aiming
at the follow-through, which causes an impact with the right physical
properties, and the swing then continues on to the “pointless” follow-through.

But there is a problem here. For even if there is a “success value” in
accomplishing a self-set goal, the strength of the reasons for pursuing the
follow-through is also proportioned to facts independent of this exercise of
normative power. Rather, the reasons for pursuing the follow-through will
include the internal and external goods of victory (winning as such, prizes,
adulation, etc.), and these are independent of one’s setting follow-through as
one’s goal.

Another move is this. We actually have a normative power to make something
_be_ an end. And then it becomes genuinely worth pursuing, because we have
adopted it as an end. So the player first exercises the normative power to
make follow-through be an end, and then pursues that end as an end.

As far as I know, in all racquet sports players are told to follow-through: to
continue the racquet swing after the ball or shuttle have left the racquet.
But of course the ball or shuttle doesn’t care what the racquet is doing at
that point. So what’s the point of follow-through? The usual story is this: by
aiming to follow-through, one hits the ball or shuttle better. If one weren’t
trying to follow-through, the swing’s direction would be wrong or the swing
might slow down.

Maybe we should say this. Even if all intentional action is end-directed,
there are two kinds of reasons for an action: the reasons that come from the
value of the end and the reasons that come from the value of the pursuit of
that end. In the case of follow-through, there may be a fairly trivial success
value in the follow-through—a success value that comes from one’s exercise of
normative power in adopting the follow-through as one’s end—but that success
value provides only fairly trivial reasons. However, there can be
significantly non-trivial reasons for one’s pursuing that end, reasons
independent of that end.

I think one can hold that God's beliefs are accidents of his which are not
identical to him (rejecting a strong doctrine of divine simplicity) without
holding that they are "parts" of him in any sense that makes (3) or (4)
absurd. Even for created beings like us, the accident-substance relationship
does not appear to be at all the same as the part-whole relationship as common
sense conceives it. (I would find it weird to say that my beliefs are a part
of me in the same sense that my arm is a part of my body, for instance.)

God is not simple, and in particular God’s beliefs are proper parts of God.

I agree that one could hold 1 without 2. But my argument is against people who
hold the package of 1 and 2. Think here of process theologians, and their more
orthodox cousins.

It depends on the details of how they think beliefs work. But suppose they
hold to this picture: for each proposition p that God knows, there is God's
act of believing p. Moreover, these opponents of simplicity think that God's
act of believing p is a different act for each different p that God believes.
These different acts are on that view something like accidents of God. Thus,
by raising one's arm one brings it about that God's belief that one's arm is
in a lowered state does not exist and that God's belief that one's arm is in a
raised state does exist, and so one destroys a part of God and creates a new
one. (It is a part of the package that I am criticizing that propositions are
tensed, and so it's not enough for God to eternally know that at t1 the arm is
lowered and at t2 the arm is raised.)

I agree with Walter. It seems to me that 2 is more directly incompatible with
God's atemporality than with his lack of complexity as such. An atemporal God
who sees Creation across its history in toto as a block, whether He is simple
or complex, has only true beliefs about everything that has been or will be,
and about when each fact will be true for finite creatures who are temporal.
But his beliefs don't change.  
  
Although a God who changed with and experienced time such that his beliefs
also changed might be thus considered complex across time by having temporal
parts, that would seem to be assuming an A-theory of time for all (including
God), then importing a B perspective at the end, incoherently. So, simplicity
of essence (at any point in time) and atemporality could be distinguished I
suppose, such that a person could affirm both simplicity and temporality of
God. I'm not saying that any open theist holds this opinion, or should, but it
seems a logical possibility from within that framework.

Alex  
  
How does it follow from the idea that God is not simple that God’s beliefs
change as the reality they are about changes?  
  
If (2) is true, isn't that a problem for divine simplicity as well? For on
divine simplicity, God id identical to God's beliefs, so if (2) is true it
seems that God changes as the reality changes, which conradicts divine
simplicity as well as divine immutability.

And of course by standing up, I bring it about that a new divine belief
exists. So:

I'm not sure that the creaturely changes are necessarily "destroying" (or
creating) parts of God (beliefs) on the open theists' assumption. I think they
are determining them. I will assume open theists who believe God is maximally
knowledgeable, that is, knows all that can be known, which does not include
perfect knowledge of the future according to them.  
  
On this set of assumptions God will still, however, have perfect foreknowledge
of all possible outcomes as he can understand all possible cause/effect
branches as First Cause. He just doesn't know which path along the branching
options will eventuate before the time that they do. This means his knowledge
grows only in the sense that he is able to pick out which of each fully
foreseen branch is actualised. In other words, the only way his knowledge
grows is by the steady lengthening of a "track" through a foreknown "space" of
possible world-states over time. In such a scenario, creatures with free will
would determine the direction of the track and thus the precise determination
of God’s knowledge of it, but they would not be adding to or subtracting from
God's beliefs in any quantitative sense. The addition that would occur comes
about automatically as time flows, no matter what finite agents decide.  
  
Please note, I do not hold this position, but it seems to me it is not guilty
of the exact absurdity alleged here, whatever its other flaws.  

How? Easy. I am now sitting, and God knows that. So, a part of God is the
belief that I am sitting. But I can destroy that belief of God’s by standing
up! For as soon as I stand up, the belief that I am sitting will no longer
exist. But on the view in question, God’s beliefs are parts of him. So by
standing up, I would bring it about that a part of God doesn’t exist.

However, if God exists, we would _expect_ there to be an unbounded upward
qualitative hierarchy of possible finite goods. God is infinitely good, and
finite goods are participations in God, so we would expect a hierarchy of
qualitatively greater and greater types of good that reflect God’s infinite
goodness better and better.

Why not? Here is a hypothesis. There are so very many possible much greater
goods—goods qualitatively and not just quantitatively much greter—that it
would be easy to suppose that God’s permitting the trivial evil could promote
or enhance one of these goods to a degree sufficient to yield justification.

Unknown  
  
I wouldn't say "forced" is the correct word here, but it is true that God
_necessarily_ loves you. So, in a sense, God _did/does_ have to love you.

There seems to be a legitimate reason to allow evil choices to exist and to
allow those evil choices to affect other people. Seemingly, actions wouldn't
mean much (such as freely loving God) if we were all in cubicles and couldn't
actually interact with people negatively. Free love and free hatred seems to
be two sides of the same coin. I would also say that there are legitimate
reasons for suffering to exist. Perhaps the world would have less good in it
if there is no suffering. Would God permit suffering to allow more good? I
don't see a reason as to why He wouldn't. Theodicies and defenses are very
interesting, though it seems to me that, intuitively at least, most theodicies
seem to deal with the problem of trivial evils.

The reason why few people seriously run an argument from trivial evils is
simply because the case is much more obvious in the case of horrendous evil.  
But a truly opnipotent being should be able to accomplish Evert good without
any evil, unless this is logically impossible  
So, unless there is a convincing argument for why a certain good cannot
posdibly be accomplished without permitting evil, the default position should
be that it is possible.  

On the other hand, if we think of _horrendous_ evils, like the torture of
children, it is difficult to think of much greater goods. Maybe with
difficulty we can come up with one or two possibilities, but not enough to
make it _easy_ to suppose a justification for God’s permission of the evil in
terms of the goods.

Perhaps not negatively, but certainly 'without love'. It seems to me that God
did not have to create you and thus God did not have to love you. If God were
forced to love you or loved you randomly, I wouldn't consider God to be a
moral being. I also wouldn't consider love to be a moral good if it were
forced. Therefore, it seems possible that evil actions are merely privations
of good actions, and thus one cannot choose to do "nothing" because "nothing"
is a lack of good, which seems to be some sort of evil. Perhaps one could make
a distinction between evils, but it seems that this might be a better
distinction that the one I previously drew.

So if God exists, we would expect there to be unknown possible finite goods
that are related to the known horrendous evils in something like the
proportion in which the known great finite goods are related to the known
trivial evils. Thus, if God exists, there very likely is  
an upward hierarchy of possible goods to which the horrendous evils of this
life stand like a mosquito bite to the courage of a Socrates. If we believe in
this hierarchy of goods, then it seems we should be no more impressed by the
atheological evidential force of horrendous evils than the ordinary person is
by the atheological evidential force of trivial evils.

Nobody seriously runs an argument against the existence of God from _trivial_
evils, like hangnails or mosquito bites.

Moreover, we might ask whether our ignorance of goods higher up in the
hierarchy of goods beyond the ordinary goods is not itself evidence against
the existence of such goods. Here, I think the answer is that it is very
little evidence. We would expect any particular finite being to be able to
recognize only a finite number of types of good, and thus the fact that there
are only a finite number of goods that we recognize is very little evidence
against the hypothesis of the upward hierarchy of goods.

Unknown  
  
So, God's actions do not mean much if He can't interact with people
negatively?

There is, however, a difference between the cases. Many great ordinary goods
that dwarf trivial evils, like the courage of a Socrates, are known to us. Few
if any finite goods that dwarf horrendous evils are known to us. Nonetheless,
if theism is true, it is very likely that such goods are possible. And since
the argument from evil is addressed against the theist, it seems fair for the
theist to invoke that hierarchy.

Premise (1) is controversial. The ancient Greeks would have denied it. But I
think the reason they denied it is that they didn’t have the examples that the
Christian tradition does, highly attractive examples examples of accomplished
lives of great humility.

Wesley:  
  
"by that same logic one could maybe deny one truly possesses being or goodness
or value"  
  
There is precedent for saying something like that. "No one is good but God
alone" (Mark 10:8).  
  
There has got to be a sense, and a very important one, in which what Jesus
says is true. But there is also a sense in which we are good--but only good-
by-participation.

1) I think the participationism explanation is problematic, since by that same
logic one could maybe deny one truly possesses being or goodness or value; one
could even deny one causes anything at all, concluding occasionalism. But we
do have our own being & goodness, so it's not the divine being & goodness in
us, per Aquinas.  
  
  
2) In fact, there's a real sense in which our actions & thereby
accomplishments are uniquely our own in a way they aren't God's precisely
because our actions are rooted in our secondary causality, which is distinct
from God's causality. And secondary causality being distinct from primary
causality, the causal responsibility for secondary causal acts is in the
creature, since the primary causality of God in sustaining our actions in
existence only determins them insofar as they exist, but we determine them
insofar as whether we cause them to occur (the occurrence of them is distinct
from their basic existence) and what we cause to occur.  
  
3) Additionally, there may be a difference between bragging and general pride
- some moral theology manuals seem to suggest being proud of one's
accomplishments isn't sinful, because that's distinct from bragging morally.
And one can't be proud of things one doesn't truly possess or cause in some
way.  
  
Of course, other sources say any form of pride is sinful, so this isn't a
clear issue it seems - but if one did take the route that some forms of pride
ARE morally okay, then this makes the thesis we don't possess anything,
whether being or our actions, unlikely.

@Alex I think an important nuance here is that in this specific view of
participation, goodness-by-participation isn't actually **true or intrinsic**
goodness. But this is problematic because we do have other verses affirming
the true goodness of creatures, such as Genesis 1 explicitly affirms this, as
does 1 Timothy 4 (first four verses on marriage & food) and Matthew 10
(sparrows).  
  
The main problem would be that this view of participationism _wouldn't just
make it impossible_ to take pride or "brag" about something we did (which I
can concede is wrong), but that it also makes it impossible to actually say we
are good or have value as well. In the **same manner & for the same reasons**
we can't take pride in or brag about our actions, we also can't affirm we are
**good,** or have being, and this seems to be a big problem with that specific
account.  
  
And it's also important to point out that Scripture often uses **hyperbolic
negation** or hyperbolic merism - God for example in Jeremiah 7:22 apparently
denies He ever commanded the Hebrews to do sacrifice, yet that's not a literal
negation but a hyperbolic one to point out that loving God is more important -
rhetorically DENYING one thing to point out the greater importance of another,
without intending to truly deny the importance of the secondary. Same thing
with loving Christ & hating one's parents - intentionally hyperbolic contrasts
that actually convey a hierarchy of love, but not pure exclusivity.  
  
So basing a very specific view of participationism (because not all models of
participationism would agree that we aren't actually truly good) on a phrase
that is likely using intentional rhetorical hyperbole is more speculative than
solid.

This seems question-begging. It's not because highly accomplished individuals
have a lot to brag about that they can brag about everything.  
  
Suppose you think this is a great argument, you certainly have reason to brag
about it, but even if there is no God, you also have reason to be humble,
because you may have been the first to come up with this particular argument,
but you could only have built a successful argument because of your education,
because of what you have studied, read, even the way your parents raised you.  
Nothing we do is exclusively our own accomplishment.  
Now if you don't agree with this, you don't have grounds for premise (1),
because if something really is your own accomplishment, humililty is not
appropriate because there is nothing wrong with being proud of what you have
accomplished.  
  
  
  

Walter:  
  
Good point.  
  
I propose to revise premise 2 to read:  
2*. There are some highly accomplished individuals such that for them humility
is an appropriate attitude only if God exists.  
  
Then we don't even need premise 3.  
  
To respond to your point, now, I suggest that we think of highly accomplished
individuals who did not have many of the kinds of advantages you list, and who
were hindered in various ways (e.g., by racism, sexism, poverty, war, etc.).
While no doubt everyone got some help from other humans, in some cases a
realistic appraisal of the degree of that help, especially when combined with
the degree to which fellow humans hindered the individual, will not suffice
for humility to be appropriate *if* God does not exist.

Here’s the thought behind premise (2). If there is no God, then highly
accomplished individuals have much to brag about. Many of their
accomplishments are primarily _theirs_.

Here’s another way to think about it. Given (1) and (3), we need an
explanation of how it is that humility is an appropriate attitude for a highly
accomplished individual. Classical theism’s doctrine of participation provides
such an explanation: all the efforts and all the accomplishments are not truly
theirs but a participation in God’s perfection.

Alex  
  
I have already answered this in my first reply. For the individuals you
describe, humililty is not appropriate.

Unjust laws have no normative force, and stupid ends have no normative force,
either.

I am inclined to think (1) is false if by “end” is meant the end the agent
actually adopts, as opposed to a natural end of the agent. If your ends are
sufficiently irrational, adopting means appropriate to them may be less
rational than adopting means inappropriate to them.

But what is wrong with being such that you adopt means inappropriate to your
ends is not necessarily the means—it could be the ends.

Suppose your end is irrationality. Is it really true that you _should_ adopt
the means to that, such as reasoning badly? Surely not! Instead, you should
reject the end.

[Adapting](https://www.instructables.com/Playing-NES-Power-Pad-Games-in-
Emulation/) Dance Dance Revolution and other mat controllers to work as NES
Power Pad controllers for emulation.

The amount of things one person can do with enough time is insane. I wouldn't
believe you if you told me that one person can get two PhDs, teach classes in
metaphysics, run a blog with constantly new and exciting arguments and
positions, is called one of the foremost Christian philosophers currently
alive, and STILL has time to measure bicycle energy output. I am convinced
that Dr. Pruss is some sort of superhuman.

An action is right (respectively, wrong) if and only if it is
noninstrumentally good (respectively, bad) to do it.

This is compatible with there being cases where it is bad for one to do the
right thing. Thus, refraining from stealing the money that one would need to
sign up for a class on virtue is right and noninstrumentally good, but if the
class is really effective then stealing the money might be instrumentally good
for one, though noninstrumentally ba.

I think (1) is something that everyone should accept. Even consequentialists
can and should accept (1) (though utilitarian consequentialists have too
shallow an axiology to make (1) true). But natural law theorists might add a
further claim to (1): the left-hand-side is true because the right-hand-side
is true.

The title of this post contradicts the title of [another recent
post](http://alexanderpruss.blogspot.com/2022/12/the-right-cannot-be-derived-
from-good.html), but the contents do not.

There is a way to connect the right and wrong with the good and bad:

This could well be true because differences in priors lead to a variety of
lines of investigation, a greater need for effort in convincing others, and
less danger of the community as a whole getting stuck in a local epistemic
optimum. If this hypothesis is true, then we would have an interesting story
about why it would be good for our community if a range of priors were
rationally permissible.

I am grateful to Anna Judd for pointing me to a possible connection between
permissivism and natural law epistemology.

I think it does. I have been trying to defend a natural law account of
rationality on which just as our moral norms are given by what is natural for
the will, our epistemic norms are given by what is natural for our intellect.
And just as our will is the will of a particular kind of deliberative animal,
so too our intellect is the intellect of a particular kind of investigative
animal. And we expect a correlation between what a social animal’s nature
impels it to do and what is good for the social animal’s community. Thus, we
expect a degree of harmony between the norms of epistemic rationality—which on
my view are imposed by the nature of the animal—and the good of the community.

Of course, that it would be _good_ for the community if some norm of
individual rationality obtained does not prove that the norm obtains.

At the same time, the harmony need not be perfect. Just as there may be times
when the good of the community and the good of the individual conflict in
respect of non-epistemic flourishig, there [may be such
conflict](http://alexanderpruss.blogspot.com/2011/01/epistemic-self-sacrifice-
and-prisoner.html) in epistemic flourishing.

It is epistemically better for the human community if human beings do not all
have the same (ur-) priors.

Moreover, note that it is very plausible that what range of variation of
priors is good for the community depends on the species of rational animal we
are talking about. Rational apes like us are likely more epistemically
cooperative than rational sharks would be, and so rational sharks would
benefit less from variation of priors, since for them the good of the
community would be closer to just the sum of the individual goods.

In the _Meno_ , when Socrates talks about knowledge as something that is tied
down (like the statues of Daedalus), the thing that does the tying down
doesn’t seem to be 20th century ‘justification’ or anything about reaching a
certain credence but “an account of the reason why”:  
  
“True opinions . . . are not willing to remain long, and they escape from a
man’s mind, so that they are not worth much until one ties them down by
(giving) an account of the reason why [τις αὐτὰς δήσῃ αἰτίας λογισμῷ]. . . .
After they are tied down, in the first place they become knowledge, and then
they remain in place” (97e–98a).  
  
I wonder if we get more stability when we do this route than the
justification/credence route.

Or maybe we should just get rid of the concept of knowledge and theorize in
terms of credence, justification and truth.

This method only works if my credence is slightly above what’s needed for
knowledge. If what’s needed for knowledge is 0.990, then as soon as my
credence rises to 0.995, there is no rational method with reliability better
than 1/2 for making me lose the credence needed for knowledge (this follows
from Proposition 1 [here](https://philpapers.org/archive/PRUBSA.pdf)). So if
you find yourself coming to know something that you don’t want to know, you
should act fast, or you’ll have so much credence you will be beyond rational
help. :-)

More seriously, we think of knowledge as something stable. But since evidence
comes in degrees, there have got to be cases of knowledge that are quite
unstable—cases where one “just barely knows”. It makes sense to think that if
knowledge has some special value, these cases have rather less of it. Maybe
it’s because knowledge comes in degrees, and these cases have less knowledge.

Inform me whether the following conjunction is true: all coins landed heads
and _p_ is true.

Then at least 7/8 of the time, they will inform me that the conjunction is
false. That’s a little bit of evidence against _p_. I do a Bayesian update on
this evidence, and my posterior credence will be 0.9897, which is not enough
for knowledge. Thus, with at least 7/8 reliability, I can lose my knowledge.

Sometimes we know things we wish we didn’t. In some cases, without any
brainwashing, forgetting or other irrational processes, there is a fairly
reliable way to make that wish come true.

Suppose that a necessary condition for knowing is that my evidence yields a
credence of 0.9900, and that I know _p_ with evidence yielding a credence of
0.9910. Then here is how I can rid myself of the knowledge fairly reliably. I
find someone completely trustworthy who would know for sure whether _p_ is
true, and I pay them to do the following:

Suppose there is a distinctive and significant value to knowledge. What I mean
by that is that if two epistemic are very similar in terms of truth, the level
and type of justification, the subject matter and its relevant to life, the
degree of belief, etc., but one is knowledge and the other is not, then the
one that is knowledge has a significantly higher value because it is
knowledge.

Can we rigorously model this kind of an epistemic value assignment? I think
so. Consider the following discontinuous accuracy scoring rule _s_ 1( _x_ ,
_t_ ), where _x_ is a probability and _t_ is a 0 or 1 truth value:

But that’s not quite right. For from Alice’s point of view, because the
threshold for knowledge is not 1, there is a real possibility that _p_ is
false. But it may be that just as there is a discontinuous gain in epistemic
value when your (rational) credence becomes sufficient for knowledge of
something that is in fact true, it may be that there is a discontinuous loss
of epistemic value when your credence becomes sufficient for knowledge of
something false. (Of course, you can’t know anything false, but you can have
evidence-sufficient-for-knowledge with respect to something false.) This is
not implausible, and given this, by looking at the information, by her lights
Alice also has a chance of a significant gain in value due to losing the
illusion of knowledge in something false.

_s_ 1( _x_ , _t_ ) = _a_ if _r_ < _x_ and _t_ = 1 or if _x_ < 1 − _r_ and _t_
= 0

_s_ 1( _x_ , _t_ ) = − _b_ if _r_ < _x_ and _t_ = 0 or if _x_ < 1 − _r_ and
_t_ = 1.

_s_ 1( _x_ , _t_ ) = 0 if 1 − _r_ ≤ _x_ ≤ _r_

Suppose that _a_ and _b_ are positive and _a_ / _b_ = (1− _r_ )/ _r_. Then if
my scribbled notes are correct, it is straightforward but annoying to check
that _s_ 1 is proper, and it has a discontinuous reward _a_ for meeting
threshold _r_ with respect to a truth and a discontinuous penalty  − _a_ for
meeting threshold _r_ with respect to a falsehood. To get a strictly proper
scoring rule, just add to it any strictly proper continous accuracy scoring
rule (e.g., Brier).

Plausibly, then, if we imagine Alice has some evidence for a truth _p_ that is
insufficient for knowledge, and slowly and continuously her evidence for _p_
mounts up, when the evidence has crossed the threshold needed for knowledge,
the value of Alice’s state with respect to _p_ will have suddenly and
discontinuously increased.

If we think that it’s never rational for a rational agent to refuse free
information, then the above argument can be made rigorous to establish that
any discontinuous rise in the epistemic value of credence at the point at
which knowledge of a truth is reached is exactly mirrored by a discontinuous
fall in the epistemic value of a state of credence where seeming-knowledge of
a falsehood is reached. Moreover, the rise and the fall must be in the ratio 1
− _r_ : _r_ where _r_ is the knowledge threshold. Note that for knowledge, _r_
is plausibly pretty large, around 0.95 at least, and so the ratio between the
special value of knowledge of a truth and the special disvalue of evidence-
sufficient-for-knowledge for a falsehood will need to be at most 1:19. This
kind of a ratio seems intuitively implausible to me. It seems unlikely that
the special disvalue of evidence-sufficient-for-knowledge of a falsehood is an
order of magnitude greater than the special value of knowledge. This
contributes to my scepticism that there is a special value of knowledge.

This hypothesis initially seemed to me to have an unfortunate consequence.
Suppose Alice has just barely exceeded the threshold for knowledge of _p_ ,
and she is offered a cost-free piece of information that may turn out to
slightly increase or slightly decrease her overall evidence with respect to
_p_ , where the decrease would be sufficient to lose her knowledge of _p_
(since she has only “barely” exceeded the evidential threshold for knowledge).
It seems that Alice should refuse to look at the information, since the
benefit of a slight improvement in credence if the evidence is non-misleading
is outweighed by the danger of a significant and discontinuous loss of value
due to loss of knowledge.

In [a recent post](http://alexanderpruss.blogspot.com/2023/01/knowing-you-
will-soon-have-enough.html), I noted that it is possible to cook up a Bayesian
setup where you don’t meet some threshold, say for belief or knowledge, with
respect to some proposition, but you do meet the same threshold with respect
to the claim that after you examine a piece of evidence, then you _will_ meet
the threshold. This is counterintuitive: it seems to imply that you can know
that you will have enough evidence to know something even though you don’t
yet. In a comment, Ian noted that one way out of this is to say that beliefs
do not correspond to sharp credences. It then occurred to me that one could
use the setup to probe the question of how sharp our credences are and what
the thresholds for things like belief and knowledge are, perhaps
complementarily to the considerations in [this
paper](https://philarchive.org/rec/PRUBSA).

we meet _r_ with respect to the proposition that we will meet the threshold
with respect to _p_ after we examine evidence _E_.

For suppose we have a credence threshold _r_ and that our intuitions agree
that we can’t have a situation where:

We thus have our joint constraint on the squishiness of our credences: bad
things happen if our credences are so precise as to make (6) true with respect
to a threshold _r_ for which we don’t want (a)–(c) to definitely hold. The
easiest scenario for making (a)–(c) definitely hold will be a binary test with
no false negatives.

(We definitely won’t meet _r_ on a negative test result.) Thus to get (c)
definitely true, we need (3) to hold as well as the probability of a positive
test result to be at least _r_ \+ _α_ :

_P_ ( _p_ | _E_ ) = _P_ ( _p_ )/ _P_ ( _E_ ) = ( _r_ − _α_ )/( _r_ − _α_ + _β_
(1−( _r_ − _α_ ))).

Note that by appropriate choice of _β_ , we can make _z_ be anything between
_r_ − _α_ and 1, and the right-hand-side of (3) is at least _r_ − _α_ since
_r_ \+ _α_ ≤ 1. Thus we can make (c) definitely hold as long as the right-
hand-side of (3) is bigger than or equal to the right-hand-side of (4), i.e.,
if and only if:

We thus have our joint constraint on the squishiness of our credences: bad
things happen if our credences have a level of precision equal to the right-
hand-side of (6). What exactly that says about _α_ depends on where the
relevant threshold lies. If the threshold _r_ is 1/2, the squishiness _α_ is
0.15. That’s surely higher than the actual squishiness of our credences. So if
we are concerned merely with the threshold being more-likely-than-not, then we
can’t avoid the paradox, because there will be cases where our credence is
definitely below the threshold and it’s definitely above the threshold that
examination of the evidence will push us about the threshold.

But what’s a reasonable threshold for belief? Maybe something like 0.9 or
0.95. At _r_ = 0.9, the squishiness needed for paradox is _α_ = 0.046. I
suspect our credences are more precise than that. If we agree that the
squishiness of our credences is less than 4.6%, then we have an argument that
the threshold for belief is _more_ than 0.9. On the other hand, at _r_ = 0.95,
the squishiness needed for paradox is 2.4%. At this point, it becomes more
plausible that our credences lack that kind of precision, but it’s not clear.
At _r_ = 0.98, the squishiness needed for paradox dips below 1%. Depending on
how precise we think our credences are, we get an argument that the threshold
for belief is something like 0.95 or 0.98.

It’s in fact not hard to see that (6) is necessary and sufficient for the
existence of a case where (a)–(c) definitely hold.

What does this tell us about _r_ and _α_? We can actually figure this out.
Consider a test for _p_ that have no false negatives, but has a false positive
rate of _β_. Let _E_ be a positive test result. Our best bet to generating a
counterexample to (a)–(c) will be if the priors for _p_ are as close to _r_ as
possible while yet definitely below, i.e., if the priors for _p_ are _r_ −
_α_. For making the priors be that makes (c) easier to definitely satisfy
while keeping (b) definitely satisfied. Since there are no false negatives,
the posterior for _p_ will be:

Let _α_ > 0 be the “squishiness” of our credences. Let’s say that for one
credence to be definitely bigger than another, their difference has to be at
least _α_ , and that to definitely meet (fail to meet) a threshold, we must be
at least _α_ above (below) it. We assume that our threshold _r_ is definitely
less than one: _r_ \+ _α_ ≤ 1.

Note that the squishiness of our credences likely varies with where the
credences lie on the line from 0 to 1, i.e., varies with respect to the
relevant threshold. For we _can_ tell the difference between 0.999 and 1.000,
but we probably can’t tell the difference between 0.700 and 0.701. So the
squishiness should probably be counted relative to the threshold. Or perhaps
it should be correlated to log-odds. But I need to get to looking at grad
admissions files now.

Let _z_ = _r_ − _α_ \+ _β_ (1−( _r_ − _α_ )) = (1− _β_ )( _r_ − _α_ ) + _β_.
This is the prior probability of a positive test result. We will definitely
meet _r_ on a positive test result just in case we have ( _r_ − _α_ )/ _z_ =
_P_ ( _p_ | _E_ ) ≥ _r_ \+ _α_ , i.e., just in case

One might think that what I said earlier implies that in this difficult case
the state should always allow people to follow their conscience, because after
all it is worse to do wrong—and violating conscience is wrong—than to have
wrong done to one. But that would be absurd and horrible—think of a racist
murderer whose faulty conscience requires them to kill.

In some cases the person of mistaken conscience will still do the wrong deed
despite the law’s contrary incentive. In such a case, both the perpetrator and
the victim may be slightly better off for the law. The victim has a dignitary
benefit from the very fact that the state says that the harm was unlawful.
That dignitary benefit may be a cold comfort if the victim suffered a grave
harm, but it is still a benefit. And the perpetrator is slightly better off,
because following one’s conscience _against_ external pressure has an element
of admirability even when the conscience is mistaken.

In cases where doing wrong and suffering wrong are of roughly the same order
of magnitude, it is very intuitive that we should prevent the suffering of
wrong rather than the doing of wrong. Imagine that Alice is drowning while at
the same time Bob is getting ready to assassinate a politician, but we know
for sure that Bob’s bullets have all been replaced with blanks. If our choice
is whether to try to dissuade Bob from attempting murder or keep Alice from
drowning, we should keep Alice from drowning, evne if on the Socratic view the
harm to Bob from attempting murder will be greater than that to Alice from
drowning. (I am assuming that in this case the two harms are nonetheless of
something like the same order of magnitude.)

In a just defensive war, to refuse to fight to defend one’s fellow citizens
without special reason (perhaps priests and doctors should not kill) is wrong.
But a grave harm is done to a conscientious objector who is gotten to fight by
legal incentives. Let’s think through the five considerations above. The first
mainly applies to laws prohibiting a behavior rather than ones requiring a
behavior. Short of brainwashing, it is impossible to _make_ someone fight. (We
could superglue their hands to a gun, and then administer electric shocks
causing their fingers to spasm and fire a bullet, but that wouldn’t count as
_fighting_.) The second applies somewhat: we do need to weigh the harms to
innocent citizens from enemy invaders, harms that might be prevented if our
conscientious objector fought. But note that there is something rather
speculative about these harms. Someone who fights contrary to conscience is
unlikely to be a very effective fighter, and it is far from clear that their
military activity would actually prevent any actual harm to innocents. Now,
regarding the third consideration, one can design a conscription law with an
exemption that few who aren’t conscientious objectors would take advantage of.
One way to do this is to require evidence of one’s conscience’s objection to
fighting (e.g., prior membership in a pacifist organization). Another way is
to impose non-combat duties on conscientious objectors that are as onerous and
maybe as dangerous as combat would be. Regarding the fourth consideration, it
seems unlikely that a typical conscientious objector’s objections to war would
be changed by legal penalties. And the fifth seems a weak consideration in
general. Putting all these together, we do not outweigh the _prima facie_
considerations against pressuring conscientious objectors to act against their
(mistaken) conscience from the harms in going against conscience.

Nonetheless, there will be cases where these considerations do not suffice,
and the law should be tolerant of mistaken conscience.

One of the central insights of Western philosophy, beginning with Socrates,
has been that few if any things are as bad for an individual as culpably doing
wrong. It is better, we are told through much of the Western philosophical
tradition, that it is better to suffer than do injustice.

A reasonable optimism says that in most cases most people’s consciences are
correct. Thus typically we would expect that most violators of a legitimate
law will not be acting out of conscience—for a necessarily condition for the
legitimacy of a law is that it does not conflict with a correct conscience.
Thus, even if there is the rare murderer acting from mistaken conscience, most
murderers act against conscience, and by incentivizing abstention from murder,
in most cases the law _helps_ people follow their conscience, and the small
number of other cases can be tolerated as a side effect. Thus the
considerations of conscience favor intolerant laws in such cases. Nonetheless,
there are cases where most violators of a law would likely be acting from
conscience. Thus, if we had a law requiring eating meat, we would expect that
most of the violators would be conscientious. Similarly, a law against
something—say, the wearing of certain clothes or symbols—that is rarely done
except as a religious practice would likely be a law most violators of which
would be conscientious.

Now, the state, just like individuals, should _ceteris paribus_ avoid causing
grave harm. Hence, the state should generally avoid getting people to do
things that violate their conscience in major matters.

The difficult case, however, is when people’s consciences are mistaken to such
a degree that conscience requires them to do something that unjustly harms
others. (A less problematic mistake is when conscience is mistaken to such a
degree that conscience requires them to do something that’s permissible, but
not wrong. In those cases, tolerance is clearly called for. We shouldn’t
pressure vegetarians to eat animals even if their conscientious objection to
eating animals happens to be mistaken.)

When someone’s conscience mistakenly requires something that violates an
objective moral rule, there is a two-fold benefit to that person from a law
incentivizing following the moral rule. The law is a teacher, and the state’s
disapproval may change one’s mind about the matter. And even if it a harm to
one to violate conscience, it is also a harm to one to do something wrong even
inculpably. Thus, the harm of violating conscience is somewhat offset by the
benefit from not doing something else that is wrong.

That said, I think a qualification is plausible. Some wrongdoings are minor,
and in those cases the harm to the wrongdoer may be minor as well. But in any
case, to get someone to act against their conscience in a matter that
according to their conscience is major is to do them grave harm, a harm not
that different from death.

The harm of violating one’s conscience only happens to one if one _willingly_
violates one’s conscience. If law enforcement physically prevents me from
doing something that conscience requires from me, then I haven’t suffered the
harm. Thus, interestingly, the consideration I sketched against violating
one’s conscience does not apply when one is literally forced (fear of
punishment, unless it is severe enough to suspend one’s freedom of will, does
not actually _force_ , but only incentives).

Now, acting against one’s conscience _is_ always wrong, and is almost always
_culpably_ wrong. For the most common case when doing something wrong isn’t
culpable is that one is ignorant of the wrongness, but when one acts against
one’s conscience one surely isn’t ignorant that one is acting against
conscience, and that we ought follow our conscience is obvious.

I don’t know that I have _C_ but I know that I will know.

I have enough evidence to believe that the test would come out positive.

If the test comes out positive, I will have enough evidence to know that I
have _C_.

If the test comes out positive, it will be another piece of evidence for the
hypothesis that I have _C_ , and it will push me over the edge to belief that
I have _C_.

Suppose I am just the slightest bit short of the evidence needed for belief
that I have some condition _C_. I consider taking a test for _C_ that has a
zero false negative rate and a middling false positive rate—neither close to
zero nor close to one. On reasonable numerical interpretations of the previous
two sentences:

Still, there is something odd about (5). It’s a bit like the line:

If I am rational, my beliefs will follow the evidence. So if I am rational, in
a situation like the above, I will take myself to have a way of bringing it
about that I believe, and do so rationally, that I have _C_. Moreover, this
way of bringing it about that I believe that I have _C_ will itself be
perfectly rational if the test is free. For of course it’s rational to accept
free information. So I will be in a position where I am rationally able to
bring it about that I rationally believe _C_ , while not yet believing it.

I don’t yet have enough evidence to know that I have _C_ , but I know that in
a moment I will.

But (6) is absurd: if I know that I will know something, then I am in a
position to know that the matter is so, since that I will know _p_ entails
that _p_ is true (assuming that _p_ doesn’t concern an open future). However,
there is no similar absurdity in (5). I may know that I will have enough
evidence to know _C_ , but that’s not the same as knowing that I will _know_
_C_ or even be in a position to know _C_. For it is possible to have enough
evidence to know something without being in a position to know it (namely,
when the thing isn’t true or when one is Gettiered).

To see that (2) is true, note that given that the false negative rate is zero,
and the false positive rate is not close to one, I will indeed have non-
negligible evidence for _C_ if the test is positive.

To see that (1) is true, note that the test is certain to come out positive if
I have _C_ and has a significant probability of coming out positive even if I
don’t have _C_. Hence, the probability of a positive test result will be
significantly higher than the probability that I have _C_. But I am just the
slightest bit short of the evidence needed for belief that I have _C_ , so the
evidence that the test would be positive (let’s suppose a deterministic
setting, so we have no worries about the sense of the subjunctive conditional
here) is sufficient for belief.

As long as the test could be a true negative, you could always be surprised
and update to P of 0.0 if the test is negative. So waiting on the test before
you update is justified.

In other words, oddly enough, just prior to getting the test results I can
reasonably say:

In fact, the same thing can be said about knowledge, assuming there is
knowledge in lottery situations. For suppose that I am just the slightest bit
short of the evidence needed for _knowledge_ that I have _C_. Then I can set
up the story such that:

**Appendix:** Suppose the threshold for belief or knowledge is _r_ , where _r_
< 1. Suppose that the false-positive rate for the test is 1/2 and the false-
negative rate is zero. If _E_ is a positive test result, then _P_ ( _C_ | _E_
) = _P_ ( _C_ ) _P_ ( _E_ | _C_ )/ _P_ ( _E_ ) = _P_ ( _C_ )/ _P_ ( _E_ ) = 2
_P_ ( _C_ )/(1+ _P_ ( _C_ )). It follows by a bit of algebra that if my prior
_P_ ( _C_ ) is more than _r_ /(2− _r_ ), then _P_ ( _C_ | _E_ ) is above the
threshold _r_. Since _r_ < 1, we have _r_ /(2− _r_ ) < _r_ , and so the story
(either in the belief or knowledge form) works for the non-empty range of
priors strictly between _r_ /(2− _r_ ) and _r_.

Ian:  
  
I don't think the worries about sharpness of credences apply. For we can
suppose that the cases we apply the argument to are precisely ones with
sufficiently sharp credences. They might be cases concerning gambling
scenarios, or they could be medical cases where the only relevant data one has
is statistics from the most recent publications on the subject.  
  
I think all we really need for the argument is that in certain kinds of cases
meeting a credence threshold is what makes the difference between not having a
belief (or a justified belief) and having a belief (or a justified belief).  
  
By the way, suppose we think that the argument doesn't work for sharpness
reasons. Let's say that u is a level of sharpness such that it only makes
sense to say that you've met the threshold if you are at r+u or higher and
that you haven't met it if you are at r-u or lower, and if you're between r-u
and r+u it's indeterminate. Then working through the Bayesian details should
provide one with an interesting joint constraint on r and u. And then we can
ask see if it's reasonable to think that r and u actually satisfy the joint
constraint.

I’d take this as an argument against the view that belief is (or is justified
by) credence above a threshold.  
  
But that view seems implausible in any case. It’s pretty much only in gambling
setups with well-defined chances that we have sharp credences. But we all have
vast numbers of beliefs (taken in an informal ‘folk’ sense). So it seems that
some other approach is needed.  
  
A thorough Bayesian would be untroubled by this example. She would have her
credences, and that would be enough. She would not care whether they exceeded
some arbitrary threshold for belief. She would be similarly untroubled by the
lottery paradox. But no one in practice is a thorough Bayesian, or even close.
So again, it seems that some other approach is needed.

To be clear, I don’t doubt that the example works. It does, and it illustrates
a genuine problem with the view that belief is (or is justified by) credence
above a threshold. Specifically, this violates an apparently natural
reflection principle, that if I rationally believe that I will rationally come
to believe, then I should believe now. Issues like this seem unavoidable with
a threshold (other than 1). The lottery paradox illustrates a different one:
violation of logical closure. Of course, you may be prepared to live with such
issues - maybe beliefs don’t have to be logically consistent.  
  
  
My problem with the threshold approach is more basic. If I don’t have a
credence, I can’t apply the threshold test. If I do have a credence, then
noting whether it is above or below a threshold adds nothing useful. One
response to this is straight Bayesianism, which rejects any role for belief
(beyond credence 1). Another is that credence and belief are separate and
related, but not straightforwardly. In the example, I would say that I
_believe_ the relevant medical information about the chance that I have C and
about the reliability of the test, and set my _credences_ accordingly.

Ian,  
  
I hadn't thought about this in the context of reflection. But technically
there is no counterexample here to van Fraassen's reflection principle, which
only says that P(H | credence at t will be p) = p. My examples satisfy that,
assuming full transparency about my own credences and full certainty that I
will conditionalize (cf.: https://jonathanweisberg.org/pdf/C_R_and_SKv2.SP.pdf
).  
  
The original reflection principle implies that my credence should be the
expected value of my future credences (at a fixed future time; we might also
generalize to any martingale stopping time). This is not a problem in my
cases, because although it is likely that my future credence will be bigger
than it is now, there is a small chance that my future credence will plummet
to zero due to a negative test result, as William notes.  
  
Upon further thought, I think it can be quite reasonable to say: "I know that
tomorrow I will have knowledge-level evidence for p, but I don't yet." And if
so, then my subsequent post on squishiness, even if technically correct, is
moot.

I have enough evidence to _know_ that the test would come out positive,

I have a problem with (2). (2) only follows if a complete explanation is
possible, which is kind of what you want to prove. So, it seems to me (2) begs
the question.  
If the 'water' is a brute fact, 'the earth floats on water' is as far as an
explanation can get. In a way it is merely partial but it isn't actually part
of something complete because there isn't something complete.  
That's why (1) is 'kind of right' but it isn't completely right.  
  
  
  

I was thinking of an argument I had seen by Della Rocca where he attempts to
show that relations cannot exist and therefore we should accept monism. If I
understand it correctly the idea is that relations depend on their relata for
their existence and therefore are grounded in one or more of their relata.
Then by asking what this grounding relation depends on it would start an
infinite regress of relations.

Any explanation for an event _E_ that does not go all the way back to
something self-explanatory is merely partial.

Trevor:  
  
I don't know why we need to say that the relation is related to the relata.
The relation relates the relata.

A partial explanation is one that is a part of a complete explanation.

So, if any event _E_ has an explanation, it has an explanation going all the
way back to something self-explanatory. (1,2)

I am not sure I buy (1). But it sounds kind of right to me now. Additionally,
(3) kind of sounds correct on its own. If _A_ causes _B_ and _B_ causes _C_
but there is no explanation of _A_ , then it seems that _B_ and _C_ are really
unexplained. Aristotle notes that there was a presocratic philosopher who
explained why the earth doesn’t fall down by saying that it floats on water,
and he notes that the philosopher failed to ask the same question about the
water. I think one lesson of Aristotle’s critique is that if it is unexplained
why the water doesn’t fall down it is unexplained why the earth falls down.

Dr Pruss, do you think that Bradley type regresses must also stop in some sort
of self explanatory fact? For example instead of having an infinite regress of
relations being related to their relata maybe it is somehow self explanatory
that the relation is related to its relata?

I see, so the relation between relation and relata would be merely conceptual?
I am curious, would you say that accidents are grounded in substances or do
you think that there is a real relation / real distinction between substances
and accidents?

An explanation going back to something self-explanatory involves the activity
of a necessary being.

is not _generally_ logically valid. But some instances of a schema can be
logically valid even if the schema is not logically valid in general.

Sometimes the word “essential” is thrown in: I am _essentially_ human. But
what does that mean? In contemporary analytic jargon, it just means that I
cannot exist without being human. But the “central” answer to “What am I?” is
not just an answer that states a property that I cannot lack. There are, after
all, many such properties essential in such a way that do not answer the
question. “Someone conceived in 1972” and “Someone in a world where 2+2=4”
attribute properties I cannot lack, but are not the central answers.

But perhaps we can do better. The necessary biconditional (2) holds in the
case “virtuous human”, but in a kind of trivial way: “a good virtuous human”
is repetition. I think that, as often, we need to pass from a modal to a
hyperintensional characterization. Consider that not only is (1) true, but
also:

Another interpretation of the question of what we are is that it is asking for
*classification*. Generics can be useful here, because they (a) are apt
answers to classification questions and (b) still allow for exceptions. The
former feature makes them substantive. The latter makes them modest and immune
to certain styles of counterexample. For more on this (with respect to the
question of what *we* are, but the tools at play are widely applicable), see:
https://andrewmbailey.com/GenericAnimalism.pdf

And yet “a virtuous human” would not be the answer to the ancient “What am I
centrally, really, deep-down, essentially?” question even if I were in fact a
virtuous human.

Andrew:  
  
Nice paper!  
  
But I think classification is not what I am getting at. I am aptly classified
as a male human, a human, a mammal, a vertebrate, an animal, an organism, and
an enmattered thing. However, I think only "human" is the answer to the
question I am after, the "what am I really" question. I am looking for a
particular "deep" classification. This is neither the most general
classification nor the most specific one, but the one that captures what most
deeply I am.  
  
On Athanasian theology, what makes me be saved is that the second person of
the Trinity became human just as I am human. He also became a male human just
as I am a male human and he became a vertebrate just as I am a vertebrate. But
these things do not save me. What saves me is his co-humanity, not his co-
vertebricity or co-animality or co-maleness. There is something deeper about
humanity than about any of these other things.  
  
This may be ethically important, too. Should I care more for a squid or a
mouse, if their intelligence is equal? The mouse is a fellow vertebrate, but
so what? It makes no difference. Similarly, that someone is a fellow male
matters not a whit. However, that someone is a fellow human, that seems to
matter.

In other words, if I am a human, being a good human explains my being good. On
the other hand, even if I were a virtuous human, my being a good virtuous
human would not explain my being good. For redundancy is to be avoided in
explanation, and “good virtuous human” is redundant.

So our initial account of what is being asked for is that we are asking for an
attribute _F_ such that:

On the "male human" point: it does seem like I can say "I am good and a man
iff I am a good man." But then (if we take the above approach) it seems like I
am centrally (really, deep-down) a man, not merely a human. This seems to sit
less comfortably with the Athanasian point. Perhaps this depends on the notion
of gender-specific norms? I'm not enough of a philosopher of gender to know
the literature on things like that.  
  

What am I? A herring-eater, a husband, a badminton player, a philosopher, a
father, a Canadian, a six-footer and a human are all correct answers. There is
an ancient form of the “What is _x_?” question where we are looking for a
“central” answer, and of course “a human” is usually taken to be that one.
What makes for that answer being central?

_x_ is “centrally, really, deep-down, essentially” _F_ just in case what it is
for _x_ to be good is for _x_ to be a good _F_.

Necessarily, if I am human, what it is for me to be good is for me to be a
good human.

**Response:** I deny that it’s absurd. It’s just that all the electrons we
meet _are_ good at electronicity.

_x_ is good and _x_ is _F_ if and only if _x_ is a good _F_

So the sense of the “What am I centrally, really, deep-down, essentially?”
question isn’t just modal. What is it?

**Objection 2:** “Good” is attributive, and hence there is no such thing as
being good _simpliciter_.

**Objection 1:** Some things are “centrally” electrons, but something’s being
good at electronicity is absurd.

But the same is not true for any other attribute besides “human” among those
of the first paragraph. I can be good and a badminton player while not being a
good badminton player, and I can be a good herring-eater without being good
and a herring-eater. And I have no idea what it is to be a good six-footer,
but perhaps the fact that I am a quarter of an inch short of six feet right
now makes me not be one. (In some cases one direction may hold. It may be that
if I am good and a human, then I am a good human.)

Necessarily, I am good and a human if and only if I am a good human.

In other words, that which I am centrally, really, deep-down and essentially
is that which sets the norms for me to be good _simpliciter_.

Necessarily, _x_ is a good _F_ if and only if _x_ is good.

Necessarily, I am good and a virtuous human if and only if I am a good
virtuous human.

Maybe, but can you say: What it is for you to be good is for you to be a good
man?  
  
Maybe the issue is that there is an ambiguity. A man (in the relevant sense)
is an adult male human. But "good adult male human" is a bit ambiguous: is one
good at the adulting, the maleness or the humanness, or at all three?  
  
I would say that what makes you be good is that you are good at being human,
and given that you are a man, this may have certain implications that it
wouldn't have if you weren't a man. (E.g., that you are a man makes it bad for
you to say "I am not a man".)

I suppose the best way out is for the open theist to deny (1).

Speaking of "God's beliefs" seems about as coherent as discussing what God ate
for lunch. He's not a finite creature.

Ian:  
  
I am somewhat inclined to say that a high credence just *is* what a belief is.  
  
THToD:  
  
Like you and Alston, I am inclined to agree that "belief" is at best an
awkward way to describe God's cognition. However, I think a major part of the
reason why God doesn't fundamentally have beliefs is that there is no room for
a gap in God between cognition and reality. Whether an open theist can have
the "no gap" view of God depends on the variety of open theism. If the open
theist is an open futurist, who thinks there are no facts in reality about
future contingents, then they can have the "no gap" view. But an open theist
like Swinburne or van Inwagen who thinks there are facts about future
contingents and God doesn't know them thinks there is a gap between God's
knowledge and reality, and now God is sounding very much like us -- there is
stuff that God is uncertain about.

These three theses, together with some auxiliary assumptions, yield a serious
problem for open theism.

Consider worlds created by God that contain four hundred people, each of whom
has an independent 1/2 chance of freely choosing to eat an orange tomorrow
(they love their oranges). Let _p_ be the proposition that at least one of
these 400 people will freely choose to eat an orange tomorrow. The chance of
not- _p_ in any such world will be (1/2)400 < 1/10100. Assuming open theism,
so God doesn’t just directly know whether _p_ is true or not, God will take
the probability of _p_ in any such world to be bigger than 1 − (1/10100) and
by (1) God will believe _p_ in these worlds. But in some of these worlds, that
belief will turn out to be false—no one will freely eat the orange. And this
violates (3).

I’d reject (1) in any case. Perfectly rational beings would have no use for
belief. They would have consistent credences (which could be 0 or 1) for
everything. On this view, beliefs are a shortcut that we, with our limited
mental capacity, use to reduce processing load.

Alex  
  
As Ian says, (1) applies to finite creatures, it doesn't apply to an
omniscient being. On open theism, God knows everything that can be known, but
libertarian free choices cannot be known in advance. If God were to "guess"
what I will do tomorrow, of course on open theism, he could turn out to be
wrong, but it would be a guess and not a belief.

A rational being believes anything that they take to have probability bigger
than 1 − (1/10100) given their evidence.

Another move is this. We actually have a normative power to make something
_be_ an end. And then it becomes genuinely worth pursuing, because we have
adopted it as an end. So the player first exercises the normative power to
make follow-through be an end, and then pursues that end as an end.

But there is a problem here. For even if there is a “success value” in
accomplishing a self-set goal, the strength of the reasons for pursuing the
follow-through is also proportioned to facts independent of this exercise of
normative power. Rather, the reasons for pursuing the follow-through will
include the internal and external goods of victory (winning as such, prizes,
adulation, etc.), and these are independent of one’s setting follow-through as
one’s goal.

Yet it seems that whatever is intended is intended as a means or an end. One
might reject this principle, taking follow-through to be a counterexample.

As far as I know, in all racquet sports players are told to follow-through: to
continue the racquet swing after the ball or shuttle have left the racquet.
But of course the ball or shuttle doesn’t care what the racquet is doing at
that point. So what’s the point of follow-through? The usual story is this: by
aiming to follow-through, one hits the ball or shuttle better. If one weren’t
trying to follow-through, the swing’s direction would be wrong or the swing
might slow down.

Clearly the follow-through is _intended_ —it’s consciously planned, aimed at,
etc. But it need not be a means to anything one cares about in the game
(though, of course, in some cases it can be a means to impressing the
spectators or intimidating an opponent). But is it an end? It seems pointless
as an end!

Maybe we should say this. Even if all intentional action is end-directed,
there are two kinds of reasons for an action: the reasons that come from the
value of the end and the reasons that come from the value of the pursuit of
that end. In the case of follow-through, there may be a fairly trivial success
value in the follow-through—a success value that comes from one’s exercise of
normative power in adopting the follow-through as one’s end—but that success
value provides only fairly trivial reasons. However, there can be
significantly non-trivial reasons for one’s pursuing that end, reasons
independent of that end.

This is interesting action-theoretically. The follow-through appears
pointless, because the agent’s interest is in what happens _before_ the
follow-through, the impact’s having the right physical properties, and yet
there is surely no backwards causation here. But there not appear to be an
effective way to reliably secure these physical properties of the impact
except by trying for the follow-through. So the follow-through itself is
pointless, but one’s _aiming at_ or _trying for_ the follow-through very much
has a point. And here the order of causality is respected: one swings aiming
at the follow-through, which causes an impact with the right physical
properties, and the swing then continues on to the “pointless” follow-through.

And of course by standing up, I bring it about that a new divine belief
exists. So:

I agree with Walter. It seems to me that 2 is more directly incompatible with
God's atemporality than with his lack of complexity as such. An atemporal God
who sees Creation across its history in toto as a block, whether He is simple
or complex, has only true beliefs about everything that has been or will be,
and about when each fact will be true for finite creatures who are temporal.
But his beliefs don't change.  
  
Although a God who changed with and experienced time such that his beliefs
also changed might be thus considered complex across time by having temporal
parts, that would seem to be assuming an A-theory of time for all (including
God), then importing a B perspective at the end, incoherently. So, simplicity
of essence (at any point in time) and atemporality could be distinguished I
suppose, such that a person could affirm both simplicity and temporality of
God. I'm not saying that any open theist holds this opinion, or should, but it
seems a logical possibility from within that framework.

Alex  
  
How does it follow from the idea that God is not simple that God’s beliefs
change as the reality they are about changes?  
  
If (2) is true, isn't that a problem for divine simplicity as well? For on
divine simplicity, God id identical to God's beliefs, so if (2) is true it
seems that God changes as the reality changes, which conradicts divine
simplicity as well as divine immutability.

I think one can hold that God's beliefs are accidents of his which are not
identical to him (rejecting a strong doctrine of divine simplicity) without
holding that they are "parts" of him in any sense that makes (3) or (4)
absurd. Even for created beings like us, the accident-substance relationship
does not appear to be at all the same as the part-whole relationship as common
sense conceives it. (I would find it weird to say that my beliefs are a part
of me in the same sense that my arm is a part of my body, for instance.)

God is not simple, and in particular God’s beliefs are proper parts of God.

How? Easy. I am now sitting, and God knows that. So, a part of God is the
belief that I am sitting. But I can destroy that belief of God’s by standing
up! For as soon as I stand up, the belief that I am sitting will no longer
exist. But on the view in question, God’s beliefs are parts of him. So by
standing up, I would bring it about that a part of God doesn’t exist.

I agree that one could hold 1 without 2. But my argument is against people who
hold the package of 1 and 2. Think here of process theologians, and their more
orthodox cousins.

It depends on the details of how they think beliefs work. But suppose they
hold to this picture: for each proposition p that God knows, there is God's
act of believing p. Moreover, these opponents of simplicity think that God's
act of believing p is a different act for each different p that God believes.
These different acts are on that view something like accidents of God. Thus,
by raising one's arm one brings it about that God's belief that one's arm is
in a lowered state does not exist and that God's belief that one's arm is in a
raised state does exist, and so one destroys a part of God and creates a new
one. (It is a part of the package that I am criticizing that propositions are
tensed, and so it's not enough for God to eternally know that at t1 the arm is
lowered and at t2 the arm is raised.)

I'm not sure that the creaturely changes are necessarily "destroying" (or
creating) parts of God (beliefs) on the open theists' assumption. I think they
are determining them. I will assume open theists who believe God is maximally
knowledgeable, that is, knows all that can be known, which does not include
perfect knowledge of the future according to them.  
  
On this set of assumptions God will still, however, have perfect foreknowledge
of all possible outcomes as he can understand all possible cause/effect
branches as First Cause. He just doesn't know which path along the branching
options will eventuate before the time that they do. This means his knowledge
grows only in the sense that he is able to pick out which of each fully
foreseen branch is actualised. In other words, the only way his knowledge
grows is by the steady lengthening of a "track" through a foreknown "space" of
possible world-states over time. In such a scenario, creatures with free will
would determine the direction of the track and thus the precise determination
of God’s knowledge of it, but they would not be adding to or subtracting from
God's beliefs in any quantitative sense. The addition that would occur comes
about automatically as time flows, no matter what finite agents decide.  
  
Please note, I do not hold this position, but it seems to me it is not guilty
of the exact absurdity alleged here, whatever its other flaws.  

The reason why few people seriously run an argument from trivial evils is
simply because the case is much more obvious in the case of horrendous evil.  
But a truly opnipotent being should be able to accomplish Evert good without
any evil, unless this is logically impossible  
So, unless there is a convincing argument for why a certain good cannot
posdibly be accomplished without permitting evil, the default position should
be that it is possible.  

Moreover, we might ask whether our ignorance of goods higher up in the
hierarchy of goods beyond the ordinary goods is not itself evidence against
the existence of such goods. Here, I think the answer is that it is very
little evidence. We would expect any particular finite being to be able to
recognize only a finite number of types of good, and thus the fact that there
are only a finite number of goods that we recognize is very little evidence
against the hypothesis of the upward hierarchy of goods.

There is, however, a difference between the cases. Many great ordinary goods
that dwarf trivial evils, like the courage of a Socrates, are known to us. Few
if any finite goods that dwarf horrendous evils are known to us. Nonetheless,
if theism is true, it is very likely that such goods are possible. And since
the argument from evil is addressed against the theist, it seems fair for the
theist to invoke that hierarchy.

Unknown  
  
I wouldn't say "forced" is the correct word here, but it is true that God
_necessarily_ loves you. So, in a sense, God _did/does_ have to love you.

However, if God exists, we would _expect_ there to be an unbounded upward
qualitative hierarchy of possible finite goods. God is infinitely good, and
finite goods are participations in God, so we would expect a hierarchy of
qualitatively greater and greater types of good that reflect God’s infinite
goodness better and better.

Nobody seriously runs an argument against the existence of God from _trivial_
evils, like hangnails or mosquito bites.

Why not? Here is a hypothesis. There are so very many possible much greater
goods—goods qualitatively and not just quantitatively much greter—that it
would be easy to suppose that God’s permitting the trivial evil could promote
or enhance one of these goods to a degree sufficient to yield justification.

There seems to be a legitimate reason to allow evil choices to exist and to
allow those evil choices to affect other people. Seemingly, actions wouldn't
mean much (such as freely loving God) if we were all in cubicles and couldn't
actually interact with people negatively. Free love and free hatred seems to
be two sides of the same coin. I would also say that there are legitimate
reasons for suffering to exist. Perhaps the world would have less good in it
if there is no suffering. Would God permit suffering to allow more good? I
don't see a reason as to why He wouldn't. Theodicies and defenses are very
interesting, though it seems to me that, intuitively at least, most theodicies
seem to deal with the problem of trivial evils.

Perhaps not negatively, but certainly 'without love'. It seems to me that God
did not have to create you and thus God did not have to love you. If God were
forced to love you or loved you randomly, I wouldn't consider God to be a
moral being. I also wouldn't consider love to be a moral good if it were
forced. Therefore, it seems possible that evil actions are merely privations
of good actions, and thus one cannot choose to do "nothing" because "nothing"
is a lack of good, which seems to be some sort of evil. Perhaps one could make
a distinction between evils, but it seems that this might be a better
distinction that the one I previously drew.

So if God exists, we would expect there to be unknown possible finite goods
that are related to the known horrendous evils in something like the
proportion in which the known great finite goods are related to the known
trivial evils. Thus, if God exists, there very likely is  
an upward hierarchy of possible goods to which the horrendous evils of this
life stand like a mosquito bite to the courage of a Socrates. If we believe in
this hierarchy of goods, then it seems we should be no more impressed by the
atheological evidential force of horrendous evils than the ordinary person is
by the atheological evidential force of trivial evils.

Unknown  
  
So, God's actions do not mean much if He can't interact with people
negatively?

On the other hand, if we think of _horrendous_ evils, like the torture of
children, it is difficult to think of much greater goods. Maybe with
difficulty we can come up with one or two possibilities, but not enough to
make it _easy_ to suppose a justification for God’s permission of the evil in
terms of the goods.

1) I think the participationism explanation is problematic, since by that same
logic one could maybe deny one truly possesses being or goodness or value; one
could even deny one causes anything at all, concluding occasionalism. But we
do have our own being & goodness, so it's not the divine being & goodness in
us, per Aquinas.  
  
  
2) In fact, there's a real sense in which our actions & thereby
accomplishments are uniquely our own in a way they aren't God's precisely
because our actions are rooted in our secondary causality, which is distinct
from God's causality. And secondary causality being distinct from primary
causality, the causal responsibility for secondary causal acts is in the
creature, since the primary causality of God in sustaining our actions in
existence only determins them insofar as they exist, but we determine them
insofar as whether we cause them to occur (the occurrence of them is distinct
from their basic existence) and what we cause to occur.  
  
3) Additionally, there may be a difference between bragging and general pride
- some moral theology manuals seem to suggest being proud of one's
accomplishments isn't sinful, because that's distinct from bragging morally.
And one can't be proud of things one doesn't truly possess or cause in some
way.  
  
Of course, other sources say any form of pride is sinful, so this isn't a
clear issue it seems - but if one did take the route that some forms of pride
ARE morally okay, then this makes the thesis we don't possess anything,
whether being or our actions, unlikely.

Premise (1) is controversial. The ancient Greeks would have denied it. But I
think the reason they denied it is that they didn’t have the examples that the
Christian tradition does, highly attractive examples examples of accomplished
lives of great humility.

@Alex I think an important nuance here is that in this specific view of
participation, goodness-by-participation isn't actually **true or intrinsic**
goodness. But this is problematic because we do have other verses affirming
the true goodness of creatures, such as Genesis 1 explicitly affirms this, as
does 1 Timothy 4 (first four verses on marriage & food) and Matthew 10
(sparrows).  
  
The main problem would be that this view of participationism _wouldn't just
make it impossible_ to take pride or "brag" about something we did (which I
can concede is wrong), but that it also makes it impossible to actually say we
are good or have value as well. In the **same manner & for the same reasons**
we can't take pride in or brag about our actions, we also can't affirm we are
**good,** or have being, and this seems to be a big problem with that specific
account.  
  
And it's also important to point out that Scripture often uses **hyperbolic
negation** or hyperbolic merism - God for example in Jeremiah 7:22 apparently
denies He ever commanded the Hebrews to do sacrifice, yet that's not a literal
negation but a hyperbolic one to point out that loving God is more important -
rhetorically DENYING one thing to point out the greater importance of another,
without intending to truly deny the importance of the secondary. Same thing
with loving Christ & hating one's parents - intentionally hyperbolic contrasts
that actually convey a hierarchy of love, but not pure exclusivity.  
  
So basing a very specific view of participationism (because not all models of
participationism would agree that we aren't actually truly good) on a phrase
that is likely using intentional rhetorical hyperbole is more speculative than
solid.

Wesley:  
  
"by that same logic one could maybe deny one truly possesses being or goodness
or value"  
  
There is precedent for saying something like that. "No one is good but God
alone" (Mark 10:8).  
  
There has got to be a sense, and a very important one, in which what Jesus
says is true. But there is also a sense in which we are good--but only good-
by-participation.

Alex  
  
I have already answered this in my first reply. For the individuals you
describe, humililty is not appropriate.

Here’s another way to think about it. Given (1) and (3), we need an
explanation of how it is that humility is an appropriate attitude for a highly
accomplished individual. Classical theism’s doctrine of participation provides
such an explanation: all the efforts and all the accomplishments are not truly
theirs but a participation in God’s perfection.

Here’s the thought behind premise (2). If there is no God, then highly
accomplished individuals have much to brag about. Many of their
accomplishments are primarily _theirs_.

Walter:  
  
Good point.  
  
I propose to revise premise 2 to read:  
2*. There are some highly accomplished individuals such that for them humility
is an appropriate attitude only if God exists.  
  
Then we don't even need premise 3.  
  
To respond to your point, now, I suggest that we think of highly accomplished
individuals who did not have many of the kinds of advantages you list, and who
were hindered in various ways (e.g., by racism, sexism, poverty, war, etc.).
While no doubt everyone got some help from other humans, in some cases a
realistic appraisal of the degree of that help, especially when combined with
the degree to which fellow humans hindered the individual, will not suffice
for humility to be appropriate *if* God does not exist.

This seems question-begging. It's not because highly accomplished individuals
have a lot to brag about that they can brag about everything.  
  
Suppose you think this is a great argument, you certainly have reason to brag
about it, but even if there is no God, you also have reason to be humble,
because you may have been the first to come up with this particular argument,
but you could only have built a successful argument because of your education,
because of what you have studied, read, even the way your parents raised you.  
Nothing we do is exclusively our own accomplishment.  
Now if you don't agree with this, you don't have grounds for premise (1),
because if something really is your own accomplishment, humililty is not
appropriate because there is nothing wrong with being proud of what you have
accomplished.  
  
  
  

Unjust laws have no normative force, and stupid ends have no normative force,
either.

Suppose your end is irrationality. Is it really true that you _should_ adopt
the means to that, such as reasoning badly? Surely not! Instead, you should
reject the end.

I am inclined to think (1) is false if by “end” is meant the end the agent
actually adopts, as opposed to a natural end of the agent. If your ends are
sufficiently irrational, adopting means appropriate to them may be less
rational than adopting means inappropriate to them.

But what is wrong with being such that you adopt means inappropriate to your
ends is not necessarily the means—it could be the ends.

The amount of things one person can do with enough time is insane. I wouldn't
believe you if you told me that one person can get two PhDs, teach classes in
metaphysics, run a blog with constantly new and exciting arguments and
positions, is called one of the foremost Christian philosophers currently
alive, and STILL has time to measure bicycle energy output. I am convinced
that Dr. Pruss is some sort of superhuman.

[Adapting](https://www.instructables.com/Playing-NES-Power-Pad-Games-in-
Emulation/) Dance Dance Revolution and other mat controllers to work as NES
Power Pad controllers for emulation.

I think (1) is something that everyone should accept. Even consequentialists
can and should accept (1) (though utilitarian consequentialists have too
shallow an axiology to make (1) true). But natural law theorists might add a
further claim to (1): the left-hand-side is true because the right-hand-side
is true.

An action is right (respectively, wrong) if and only if it is
noninstrumentally good (respectively, bad) to do it.

This is compatible with there being cases where it is bad for one to do the
right thing. Thus, refraining from stealing the money that one would need to
sign up for a class on virtue is right and noninstrumentally good, but if the
class is really effective then stealing the money might be instrumentally good
for one, though noninstrumentally ba.

The title of this post contradicts the title of [another recent
post](http://alexanderpruss.blogspot.com/2022/12/the-right-cannot-be-derived-
from-good.html), but the contents do not.

There is a way to connect the right and wrong with the good and bad:

Given a scoring rule _s_ , let the expected score function _G_ _s_ on the
probabilities on _H_ be defined by _G_ _s_ ( _p_ ) = _E_ _p_ _s_ ( _p_ ), with
the same extension to probabilities on _F_ as scores had.

_p_ ′( _A_ ∩ _Z_ _c_ ) = (1− _t_ ) _p_ 2( _A_ )

To see that (2) cannot be reversed, note that the only non-trivial partition
is {{0}, {1}}. If our current probability for 1 is _x_ , the expected score
upon learning where we are is _x_ _T_ (1) + (1− _x_ ) _F_ (0). Strict open-
mindedness thus requires precisely that _x_ _T_ ( _x_ ) + (1− _x_ ) _F_ ( _x_
) < _x_ _T_ (1) + (1− _x_ ) _F_ (0) whenever _x_ is neither 0 nor 1. It is
clear that this is not enough for convexity—we can have wild oscillations of
_T_ and _F_ on (0,1) as long as _T_ (1) and _F_ (1) are large enough.

Say that the scoring rule _s_ is _open-minded_ provided that for any
probability _p_ on _F_ and any finite partition _V_ of _Ω_ into events in _F_
with non-zero _p_ -probability, the _p_ -expected score of finding out where
in _V_ we are and conditionalizing on that is at least as big as the current
_p_ -expected score. If the scoring rule is open-minded, then a Bayesian
conditionalizer is never precluded from accepting free information. Say that
the scoring rule _s_ is _strictly_ open-minded provided that the _p_ -expected
score increases of finding out where in _V_ we are and conditionalizing
increases whenever there is at least one event _E_ in _V_ such that _p_ (⋅|
_E_ ) differs from _p_ on _H_ and _p_ ( _E_ ) > 0.

for any _A_ ∈ _H_. Then _p_ ′ is a probability on the algebra generated by _H_
and _Z_ extending _p_. Extend it to a probability on _F_ by Hahn-Banach. By
open-mindedness:

Let’s think about this suggestion. One of the most important uses of scoring
rules could be to evaluate the expected value of an experiment prior to doing
the experiment, and hence decide which experiment we should do. If we think of
an experiment as a finite partition _V_ of the probability space with each
cell having non-zero probability by one’s current lights _p_ , then the
expected value of the experiment is:

_G_ _s_ ( _p_ ) ≤ _t_ _G_ _s_ ( _p_ 1) + (1− _t_ ) _G_ _s_ ( _p_ 2).

The scoring rule _s_ is _proper_ provided that _E_ _p_ _s_ ( _q_ ) ≤ _E_ _p_
_s_ ( _p_ ) for all _p_ and _q_ , and strictly so if the inequality is strict
whenever _p_ ≠ _q_. Propriety says that one never expects a different
probability from one’s own to have a better score (if one did, wouldn’t one
have switched to it?).

It is easy to see that adding (3) to our assumptions doesn’t help reverse (1).

∑ _A_ ∈ _V_ _p_ ( _A_ ) _E_ _p_ _A_ _s_ ( _p_ _A_ ) = ∑ _A_ ∈ _V_ _G_ _s_ (
_p_ _A_ ),

However, at least in the case where _Ω_ is finite, [it is known
that](https://www.bowaggoner.com/blog/2016/09-22-proper-scoring-
rules/index.html) any (strictly) convex _G_ _s_ is equal to _G_ _u_ for a some
(strictly) proper scoring rule _u_. So we don’t really gain that much
generality by moving from propriety of _s_ to convexity of _G_ _s_. Indeed,
the above observations show that for finite _Ω_ , a (strictly) open-minded way
of evaluating the expected epistemic values of experiments in a setting rich
enough to satisfy (3) is always generatable by a (strictly) proper scoring
rule.

_G_ _s_ ( _p_ ) ≤ _t_ _G_ _s_ ( _p_ 1) + (1− _t_ ) _G_ _s_ ( _p_ 2)

_G_ _s_ ( _p_ ′) ≤ _p_ ′( _Z_ ) _E_ _p_ ′ _Z_ _s_ ( _p_ ′ _Z_ ) + _p_ ′( _Z_
_c_ ) _E_ _p_ ′ _Z_ _c_ _s_ ( _p_ ′ _Z_ _c_ ).

Nonetheless, (2) can be reversed (both in the strict and non-strict versions)
on the following technical assumption:

Since open-mindedness is pretty plausible to people of a Bayesian persuasion,
this means that convexity of _G_ _s_ can be motivated independently of
propriety. Perhaps instead of focusing on propriety of _s_ as much as the
literature has done, we should focus on the convexity of _G_ _s_?

where _p_ _A_ is the result of conditionalizing _p_ on _A_. In other words, to
evaluate the expected values of experiments, all we care about is _G_ _s_ ,
not _s_ itself, and so the convexity of _G_ _s_ is a very natural condition:
we are never oligated to refuse to know the results of free experiments.

Suppose we have a probability space _Ω_ with algebra _F_ of events, and a
distinguished subalgebra _H_ of events on _Ω_. My interest here is in accuracy
_H_ -scoring rules, which take a (finitely-additive) probability assignment
_p_ on _H_ and assigns to it an _H_ -measurable score function _s_ ( _p_ ) on
_Ω_ , with values in [−∞, _M_ ] for some finite _M_ , subject to the
constraint that _s_ ( _p_ ) is _H_ -measurable. I will take the score of a
probability assignment to represent the epistemic utility or accuracy of _p_.

For a probability _p_ on _F_ , I will take the score of _p_ to be the score of
the restriction of _p_ to _H_. (Note that any finitely-additive probability on
_H_ extends to a finitely-additive probability on _F_ by Hahn-Banach theorem,
assuming Choice.)

There is an event _Z_ in _F_ such that _Z_ ∩ _A_ is a non-empty proper subset
of _A_ for every non-empty member of _H_.

But _p_ ′( _Z_ ) = _p_ ( _Ω_ ∩ _Z_ ) = _t_ and _p_ ′( _Z_ _c_ ) = 1 − _t_.
Moreover, _p_ ′ _Z_ = _p_ 1 on _H_ and _p_ ′ _Z_ _c_ = _p_ 2 on _H_. Since _H_
-scores don’t care what the probabilities are doing outside of _H_ , we have
_s_ ( _p_ ′ _Z_ ) = _s_ ( _p_ 1) and _s_ ( _p_ ′ _Z_ _c_ ) = _s_ ( _p_ 2) and
_G_ _s_ ( _p_ ′) = _G_ _s_ ( _p_ ). Moreover our scores are _H_ -measurable,
so _E_ _p_ ′ _Z_ _s_ ( _p_ 1) = _E_ _p_ 1 _s_ ( _p_ 1) and _E_ _p_ ′ _Z_ _c_
_s_ ( _p_ 2) = _E_ _p_ 2 _s_ ( _p_ 2). Thus (9) becomes:

Neither implication can be reversed. To see this, consider the single-
proposition case, where _Ω_ has two points, say 0 and 1, and _H_ and _F_ are
the powerset of _Ω_ , and we are interested in the proposition that one of
these point, say 1, is the actual truth. The scoring rule _s_ is then
equivalent to a pair of functions _T_ and _F_ on [0,1] where _T_ ( _x_ ) = _s_
( _p_ _x_ )(1) and _F_ ( _x_ ) = _s_ ( _p_ _x_ )(0) where _p_ _x_ is the
probability that assigns _x_ to the point 1. Then _G_ _s_ corresponds to the
function _x_ _T_ ( _x_ ) + (1− _x_ ) _F_ ( _x_ ), and each is convex if and
only if the other is.

This technical assumption basically says that there is a non-trivial event
that is logically independent of everything in _H_. In real life, the
technical assumption is always satisfied, because there will always be
something independent of the algebra _H_ of events we are evaluating
probability assignments to (e.g., in many cases _Z_ can be the event that the
next coin toss by the investigator’s niece will be heads). I will prove that
(2) can be reversed in the Appendix.

Hence we have convexity. And given strict open-mindedness, the inequality will
be strict, and we get strict convexity.

To see that the non-strict version of (1) cannot be reversed, suppose ( _T_ ,
_F_ ) is a non-trivial proper scoring rule with the limit of _F_ ( _x_ )/ _x_
as _x_ goes to 0 finite. Now form a new scoring rule by letting _T_ * ( _x_ )
= _T_ ( _x_ ) + (1− _x_ ) _F_ ( _x_ )/ _x_. Consider the scoring rule ( _T_
*,0). The corresponding function _x_ _T_ * ( _x_ ) is going to be convex, but
( _T_ *,0) isn’t going to be proper unless _T_ * is constant, which isn’t
going to be true in general. The strict version is similar.

Assume open-mindedness. Let _p_ 1 and _p_ 2 be two distinct probabilities on
_H_ and let _t_ ∈ (0,1). We must show that if _p_ = _t_ _p_ 1 \+ (1− _t_ ) _p_
2, then

with the inequality strict if the open-mindedness is strict. Let _Z_ be as in
(3). Define

One of the most problematic aspects of some science practice is a cut-off, say
at 95%, for the evidence-based confidence needed for publication.

To a first approximation, the problem is _really, really_ bad. Insofar as
publication is the relevant reward, it is a reward independent of the truth of
the matter! In other words, the scoring rule has a reward for gaining
probability 0.95 (say) in the hypothesis, regardless of whether the hypothesis
is true or false.

Interesting. But it should be noted the situation is more complicated. For
example, there are professional penalties for being caught p-hacking.

We can see that something is problematic if we think about cases like this.
Suppose your current level of confidence is just slightly above the threshold,
and a graduate student in your lab proposes to do one last experiment in her
spare time, using equipment and supplies that would otherwise go to waste.
Given the reward structure, it will likely make sense for you to refuse this
free offer of additional information. If the experiment favors your
hypothesis, you get nothing out of it—you could have published without it, and
you’d still have the same longer term rewards available. But if the experiment
disfavors your hypothesis, it will likely make your paper unpublishable (since
you were at the threshold), but since it’s just one experiment, it is unlikely
to put you into the position of yet being able to publish a paper against the
hypothesis. At best it loses you the risk of the small negative reputation for
having been wrong, and since that’s a small penalty, and an unlikely one
(since most likely your hypothesis _is_ true by your data), so that’s not
worth it. In other words, the the structure rewards you for ignoring free
information.

What’s the solution? Maybe it’s this: reward people for publishing lots of
data, rather than for the data showing anything interestingly, and do so
sufficiently that it’s always worth publishing more data?

I just realized, with the help of a mention of _p_ -based biases and improper
scoring rules somewhere on the web, that what is going on here is precisely a
problem of a reward structure that does not result in a proper scoring rule,
where a proper scoring rule is one where your current probability assignment
is guaranteed to have an optimal expected score according to that very
probability assignment. Given an improper scoring rule, one has a perverse
incentive to change one’s probabilities without evidence.

If your probability meets the threshold for publication and you’re wrong, at
worst small negative.

How can we fix this? We simply cannot realistically fix it if we have a high
probability threshold for publication. The only way to fix it while keeping a
high probability threshold would be by having a ridiculously high penalty for
being wrong. But we should’t do stuff like sentencing scientists to jail for
being wrong (which has
[happened](https://www.scientificamerican.com/article/italian-scientists-
get/)). Increasing the probability threshold for publication would only
require the penalty for being wrong to be increased. Decreasing probability
thresholds for publication helps a little. But as long as there is a larger
reputational benefit from getting things right than the reputational harm from
getting things wrong, we are going to have perverse incentives from a
probability threshold for publication bigger than 1/2, no matter where that
threshold lies. (This follows from [Fact
2](http://alexanderpruss.blogspot.com/2023/01/making-single-proposition-
scoring-rules.html) in my recent post, together with the observation that
[Schervish’s](https://projecteuclid.org/journals/annals-of-
statistics/volume-17/issue-4/A-General-Method-for-Comparing-Probability-
Assessors/10.1214/aos/1176347398.full) characterization of scoring rules shows
implies that any reward function corresponds to a unique up to additive
constant penalty function.)

This is not a proper scoring rule. It’s not even close. To make it into a
proper scoring rule, the penalty for being wrong at the threshold would need
to be way higher than the reward for being right. Specifically, if the
threshold is _p_ (say 0.95), then the ratio of reward to penalty needs to be
(1− _p_ ) : _p_. If _p_ = 0.95, the reward to penalty ratio would need to be
1:19. If _p_ = 0.99, it would need to be a staggering 1:99, and if _p_ = 0.9,
it would need to be a still large 1:9. We are very, very far from that. And
when we add the truth-independent reward for publication, things become even
worse.

Fortunately, it’s not quite so bad. Publication is the short-term reward. But
there are long-term rewards and punishments. If one publishes, and later it
turns out that one was right, one may get significant social recognition as
the _discoverer_ of the truth of the hypothesis. And if one publishes, and
later it turns out one is wrong, one gets some negative reputation.

However, notice this. Fame for having been right is basically independent of
the exact probability of the hypothesis one established in the original paper.
As long as the probability was sufficient for publication, one is rewarded for
fame. Thus if it turns out that one was right, one’s long-term reward is fame
if and only if one’s probability met the threshold for publication and one was
right. And one’s penalty is some negative reputation if and only if one’s
probability met the threshold for publication and yet one was wrong. But note
that scientists are actually extremely forgiving of people putting forward
evidenced hypotheses that turn out to be false. Unlike in history, where some
people live on in infamy, scientists who turn out to be wrong do not suffer
infamy. At worst, some condescension. And it barely varies with your level of
confidence.

Yeah, but note that the situation I discuss -- refusing more experiments once
you reach the threshold -- is not exactly p-hacking. First, I don't know that
actual p-hacking really yields an evidence-based probability of 0.95. What I
am worried about happens even if you get a genuine bona fide evidence-based
probability of 1-p.  
  
Second, the particular abuse that I mention probably doesn't fall under
p-hacking as one might think it's sound experimental design to set up a
stopping point in advance, and then stop. But given a strictly proper scoring
rule, it's always worth getting more data.

I don’t think so. Consider that prudentially we also pursue goods of physical
health. However, norms of physical health are not a species of prudential
norms. It is the medical professional who is the expert on the norms of
physical health, not the prudent person as such. Prudential norms apply to
voluntary behavior as such, while the norms of physical health apply to the
body’s state and function. We might say that norms of the voluntary pursuit of
the fulfillment of the norms of physical health are prudential norms, but the
norms of physical health themselves are not prudential norms. Similarly, the
norms of the voluntary pursuit of the fulfillment of epistemic norms are
prudential norms, but the epistemic norms themselves are no more prudential
norms than the health norms are.

We think highly morally of teachers who put an enormous effort into getting
their students to know and understand the material. Moreover, we think highly
of these teachers regardless of whether they are in a discipline, like some
branches of engineering, where the knowledge and understanding exists
primarily for the sake of non-epistemic goods, as when they are in a
discipline, like cosmology, where the knowledge and understanding is primarily
aimed at epistemic goods.

Does this mean that epistemic norms are just a species of prudential norms?

The virtues and vices in disseminating epistemic goods are just as much moral
virtues and vices as those in disseminating other goods, such as food,
shelter, friendship, or play, and there need be little difference in kind. The
person who is jealous of another’s knowledge has essentially the same kind of
vice as the one who is jealous of another’s physical strength. The person
generous with their time in teaching exhibits essentially the same virtue as
the one generous with their time in feeding others.

There is, thus, no significant difference in kind between the pursuit of
epistemic goods and the norms of the pursuit of other goods. We not
infrequently have to weigh one against the other, and it is a mark of the
virtuous person that they do this well.

But if this is all correct, then by parallel we should not make a significant
distinction in kind between the pursuit of epistemic goods for oneself and the
pursuit of non-epistemic goods for oneself. Hence, norms governing the pursuit
of knowledge and understanding seem to be just a species of prudential norms.

Now suppose that _x_ < _a_. Then the right-hand-side is zero. And the left-
hand-side is zero unless _y_ ≥ _a_. So suppose _y_ ≥ _a_. Since _T_ and _F_
are constant on [ _a_ ,1], we only need to check (1) at _y_ = 1. At _y_ = 1,
the left-hand-side of (1) is _x_ (1− _a_ )/ _a_ − (1− _x_ ) ≤ 0 if _x_ < _a_.

I'm not a philosopher, but causal finitism would seem to be a different beast
from "possibility" finitism.

I was going to offer a response to Zsolt Nagy, but I think it's just for the
best that you blocked him, Dr. Pruss. Hopefully, you can block his IP from
commenting. You might have to block the IP of VPNs as well. I don't know if
that's possible, though.

I wasn't attempting a general theory of scoring functions, just wanted to
provide an easy way to generate them, including the common ones.  
  
I misunderstood your intentions: you want custom scoring rules for a
particular P; I thought you were looking for general scoring rules that could
be used for any P.  
For example, if you want to evaluate your weatherman, you probably want to
evaluate his ability on all kinds of weather not just e.g. snow.  
  
Also, you seem to want to derive scoring rules from a RV V dependent on the
event P.  
The scheme is:  
1\. estimate x as Pr(P);  
2\. use estimate x to calculate the optimal strategy;  
3\. use the actual payoff V as the score.  
  
That should be guaranteed to form a proper scoring rule,  
since the optimization is built in at step 2.  
  
Is that right?

_T_ ( _x_ ) = ((1− _x_ )/ _x_ ) + ∫ _a_ _x_ _u_ −2 _d_ _u_ = ((1− _x_ )/ _x_ )
− ( _x_ −1− _a_ −1) = _a_ −1 − 1.

Thus, _T_ = ((1− _a_ )/ _a_ ) ⋅ 1[ _a_ ,0]. Now let’s check if we have the
propriety condition:

I’ve been playing with how to construct proper scoring rules for a single
proposition, and I found two nice ways that are probably in the literature but
I haven’t seen explicitly. First, let _F_ be any monotone (not necessarily
strictly) decreasing function on [0,1] that is finite except perhaps at 1.
Then let:

It looks like my method does generate every proper scoring rule. There is an
old paper of Schervish which in Theorem 4.2 and Lemma A.7 gives a
characterization of all scoring rules. One can use the characterization to
show that for every T there is a unique (up to additive constant) F and vice
versa. https://projecteuclid.org/journals/annals-of-
statistics/volume-17/issue-4/A-General-Method-for-Comparing-Probability-
Assessors/10.1214/aos/1176347398.full

"Second, let T be any monotone increasing function on [0,1] that is finite
except perhaps at − 1."  
  
Perhaps you meant  
"Second, let T be any monotone increasing function on [0,1] that is finite
except perhaps at 1." Is -1 in the domain of T?  
  
Also, if F is assumed to be monotone decreasing, the case F=c is not
consistent with that hypothesis.  
  
Thanks for this post, I found the idea of proposition scoring to be very
interesting.  

Here’s a sketch of the proof of Fact 1. Note first that if _F_ = _c_ is
constant, then _T_ _F_ ( _x_ ) = _c_ − ((1− _x_ )/ _x_ ) _c_ \+ _c_ ( _x_
−1−(1/2)−1) = _c_ \+ _c_ − 2 _c_ = 0 for all _x_. Since the map _F_ ↦ _T_ _F_
is linear, it follows that if _F_ and _H_ differ by a constant, then _T_ _F_
and _T_ _H_ are the same. Thus subtracting a constant from _F_ , we can assume
without loss of generality that _F_ is non-positive.

for all _x_ and _y_. If you assign probability _x_ to _p_ , then _x_ _T_ ( _y_
) + (1− _x_ ) _F_ ( _y_ ) measures your expected value of the score for
someone who assigns _y_. The propriety condition thus says that by your lights
there isn’t a better probability to assign. After all, if there were, wouldn’t
you assign it?

Kerry:  
  
I meant 0, not -1. Fixed. Thanks!  
  
And by by "increasing" I meant "non-strictly". I.e., "non-decreasing".  
  
Andrew:  
  
The symmetry assumption is natural, but not inevitable. Here is one way to
construct proper accuracy scoring rules. You are going to be given a choice
between a number of different games, each of which has an outcome that depends
on the proposition q. You choose the game with the highest expected outcome
according to your probability you assigned to q. Your score then is the actual
outcome of the game. (You can get different scoring rules for different tie-
breaking procedures for equal expected outcomes.) This kind of a scoring rule
tends not to satisfy the symmetry condition. For instance, suppose you have a
choice between two games, one which pays $1 on q and $2 on ~q and one which
pays $3 on q and $1 on ~q. Let p be your probability of q. Then you will
choose game 1 if p is less than 1/3 and game 2 if p is more than 1/3. Then
T(p)=1 if p<1/3 and T(p)=3 if p>1/3, while F(p)=2 if p<1/3 and F(p)=1 if
p>1/3. (And what we have at 1/3 depends on the tie-breaking choice.) Then we
don't have your symmetry condition.  
  
Another family of cases of asymmetry is like this. If there is objective
morality (or God or a lawlike universe, etc.), there is a lot of value in
being nearly certain that there is objective morality (etc.) But if there is
no objective morality (or no God or no lawlike universe, etc.), there is
little value in being nearly certain of that.

Andrew:  
  
You should make the integral defining F go from something other than 0, as the
integral from 0 to x of 1/x is infinite.

Corrected:  
  
Of course assigning probability x to event P is equivalent to assigning 1-x to
P'.  
So it's natural to assume that  
F(x) = T(1-x).  
If we further assume that F is differentiable, then applying Newton's
criterion for the extremum, i.e. setting the derivative w.r.t. y to zero, we
get  
x T'(y) - (1-x) T'(1-y) = 0  
should hold when y=x, or  
x T'(x) = (1-x) T'(1-x).  
Define  
g(x) = x T'(x),  
then the previous condition is equivalent to  
g(x) = g(1-x),  
i.e. g is symmetric on the interval [0,1].  
  
Given any such symmetric "kernel" g, we can then recover T as  
T(x) = \int g(x)/x dx.  
For example, if g(x)=A a constant, then T(x) = A log(x) + C.  
Choosing the A=+1 so that the extremum is a max rather than a min, and setting
C=0,  
we have the negative of the information content of the probability.  
  
In other words, score the estimate by how much information it would cost.

Btw, please remove my first comment - it was superseded by a correction.

_T_ _F_ ( _x_ ) = _F_ (1/2) − ((1− _x_ )/ _x_ ) _F_ ( _x_ ) − ∫1/2 _x_ _u_ −2
_F_ ( _u_ ) _d_ _u_.

In other words, to generate a proper scoring rule, we just need to choose one
of the two functions making up the scoring rule, make sure it is monotone in
the right direction, and then we can generate the other function.

Sure, the possibilities are infinite.  
  
I hadn't heard of the name Brier before, but that rule is equivalent to  
T(x) = -(1-x)^2,  
obtained when the kernel is  
g(x) = x(1-x).

By the way, I suspect that all single-proposition scoring rules are generated
by the procedure in my post, up to an additive constant, but I don't have a
proof yet. I can show that the differences have to have zero derivative almost
everywhere, but that's not enough.

**Fact 1:** The pair ( _T_ _F_ , _F_ ) is a proper scoring rule.

Since F is monotone decreasing, we already know that F is differentiable
almost everywhere in [0,1].

Fact 2 follows from Fact 1 together with the observation that ( _T_ , _F_ ) is
proper if and only if ( _F_ *, _T_ *) is proper, where _T_ * ( _x_ ) = _T_ (1−
_x_ ) and _F_ * ( _x_ ) = _F_ (1− _x_ ).

_x_ _T_ ( _x_ ) + (1− _x_ ) _F_ ( _x_ ) ≥ _x_ _T_ ( _y_ ) + (1− _x_ ) _F_ (
_y_ )

**Fact 2:** The pair ( _T_ , _F_ _T_ ) a proper scoring rule.

_x_ _T_ ( _y_ ) + (1− _x_ ) _F_ ( _y_ ) ≤ _x_ _T_ ( _x_ ) + (1− _x_ ) _F_ (
_x_ ).

Second, let _T_ be any monotone increasing function on [0,1] that is finite
except perhaps at 0. Let:

We can then approximate _F_ by functions of the form ∑ _i_ _c_ _i_ 1[ _a_ _i_
,1] with _c_ _i_ non-positive (here I have to confess to not having checked
all the details of the approximation) and by linearity we only need to check
propriety for _F_ = − 1[ _a_ ,0]. If _a_ = 0, then _F_ is constant and _T_ _F_
will be zero, and we will trivially have propriety. So suppose _a_ > 0. Let
_T_ ( _x_ ) = − ((1− _x_ )/ _x_ ) _F_ ( _x_ ) − ∫0 _x_ _u_ −2 _F_ ( _u_ ) _d_
_u_. This differs by a constant from _T_ _F_ , so ( _T_ _F_ , _F_ ) will be
proper if and only if ( _T_ , _F_ ) is. Note that _T_ ( _x_ ) = 0 for _x_ <
_a_ and for _x_ ≥ _a_ we have:

_F_ _T_ ( _x_ ) = _T_ (1/2) − ( _x_ /(1− _x_ )) _T_ ( _x_ ) + ∫1/2 _x_ 1/(1−
_u_ )2 _T_ ( _u_ ) _d_ _u_.

Suppose first that _x_ ≥ _a_. Then the right-hand-side is _x_ (1− _a_ )/ _a_ −
(1− _x_ ). This is non-negative for _x_ ≥ _a_ , and the left-hand-side of (1)
is zero if _y_ < _a_ , so we are done if _y_ < _a_. Since _T_ and _F_ are
constant on [ _a_ ,1], the two sides of (1) are equal for _y_ ≥ _a_.

Accuracy scoring rules measure the value of your probability assignment’s
closeness to the truth. A scoring rule for a single proposition _p_ can be
thought of as a pair of functions, _T_ and _F_ on the interval [0,1] where _T_
( _x_ ) tells us the score for assigning _x_ to _p_ when _p_ is true and _F_ (
_x_ ) tells us the score for assigning _x_ to _p_ when _p_ is false. The
scoring rule is proper provided that:

Then at least 7/8 of the time, they will inform me that the conjunction is
false. That’s a little bit of evidence against _p_. I do a Bayesian update on
this evidence, and my posterior credence will be 0.9897, which is not enough
for knowledge. Thus, with at least 7/8 reliability, I can lose my knowledge.

In the _Meno_ , when Socrates talks about knowledge as something that is tied
down (like the statues of Daedalus), the thing that does the tying down
doesn’t seem to be 20th century ‘justification’ or anything about reaching a
certain credence but “an account of the reason why”:  
  
“True opinions . . . are not willing to remain long, and they escape from a
man’s mind, so that they are not worth much until one ties them down by
(giving) an account of the reason why [τις αὐτὰς δήσῃ αἰτίας λογισμῷ]. . . .
After they are tied down, in the first place they become knowledge, and then
they remain in place” (97e–98a).  
  
I wonder if we get more stability when we do this route than the
justification/credence route.

Maybe Socrates is more talking about understanding than knowledge, something
like medieval scientia or Aristotle's derivation from essence.  
  
If you see someone doing a bizarre thing, you have no idea why they are doing
it, but you can see and hence know that they are doing it. Conversely, one can
give an account of why something is so without knowing the thing: for
instance, suppose the police have a strong suspicion that Alice killed Bob,
and they know exactly what motive Alice had to kill Bob, but they don't have
much evidence to back up the suspicion. In that case, they can give an account
of the reason why, but they don't have knowledge of why Alice killed Bob, even
if in fact she did.

Inform me whether the following conjunction is true: all coins landed heads
and _p_ is true.

This method only works if my credence is slightly above what’s needed for
knowledge. If what’s needed for knowledge is 0.990, then as soon as my
credence rises to 0.995, there is no rational method with reliability better
than 1/2 for making me lose the credence needed for knowledge (this follows
from Proposition 1 [here](https://philpapers.org/archive/PRUBSA.pdf)). So if
you find yourself coming to know something that you don’t want to know, you
should act fast, or you’ll have so much credence you will be beyond rational
help. :-)

More seriously, we think of knowledge as something stable. But since evidence
comes in degrees, there have got to be cases of knowledge that are quite
unstable—cases where one “just barely knows”. It makes sense to think that if
knowledge has some special value, these cases have rather less of it. Maybe
it’s because knowledge comes in degrees, and these cases have less knowledge.

Sometimes we know things we wish we didn’t. In some cases, without any
brainwashing, forgetting or other irrational processes, there is a fairly
reliable way to make that wish come true.

Or maybe we should just get rid of the concept of knowledge and theorize in
terms of credence, justification and truth.

Suppose that a necessary condition for knowing is that my evidence yields a
credence of 0.9900, and that I know _p_ with evidence yielding a credence of
0.9910. Then here is how I can rid myself of the knowledge fairly reliably. I
find someone completely trustworthy who would know for sure whether _p_ is
true, and I pay them to do the following:

I expect that is what Socrates is getting at. My thought is that this might be
the reason we think that knowledge is something that is tied down—because
scientia or derivation from essence or something like that is stable, even if
‘having passed the evidential threshold’ isn’t.

If we think that it’s never rational for a rational agent to refuse free
information, then the above argument can be made rigorous to establish that
any discontinuous rise in the epistemic value of credence at the point at
which knowledge of a truth is reached is exactly mirrored by a discontinuous
fall in the epistemic value of a state of credence where seeming-knowledge of
a falsehood is reached. Moreover, the rise and the fall must be in the ratio 1
− _r_ : _r_ where _r_ is the knowledge threshold. Note that for knowledge, _r_
is plausibly pretty large, around 0.95 at least, and so the ratio between the
special value of knowledge of a truth and the special disvalue of evidence-
sufficient-for-knowledge for a falsehood will need to be at most 1:19. This
kind of a ratio seems intuitively implausible to me. It seems unlikely that
the special disvalue of evidence-sufficient-for-knowledge of a falsehood is an
order of magnitude greater than the special value of knowledge. This
contributes to my scepticism that there is a special value of knowledge.

But that’s not quite right. For from Alice’s point of view, because the
threshold for knowledge is not 1, there is a real possibility that _p_ is
false. But it may be that just as there is a discontinuous gain in epistemic
value when your (rational) credence becomes sufficient for knowledge of
something that is in fact true, it may be that there is a discontinuous loss
of epistemic value when your credence becomes sufficient for knowledge of
something false. (Of course, you can’t know anything false, but you can have
evidence-sufficient-for-knowledge with respect to something false.) This is
not implausible, and given this, by looking at the information, by her lights
Alice also has a chance of a significant gain in value due to losing the
illusion of knowledge in something false.

Plausibly, then, if we imagine Alice has some evidence for a truth _p_ that is
insufficient for knowledge, and slowly and continuously her evidence for _p_
mounts up, when the evidence has crossed the threshold needed for knowledge,
the value of Alice’s state with respect to _p_ will have suddenly and
discontinuously increased.

_s_ 1( _x_ , _t_ ) = 0 if 1 − _r_ ≤ _x_ ≤ _r_

Can we rigorously model this kind of an epistemic value assignment? I think
so. Consider the following discontinuous accuracy scoring rule _s_ 1( _x_ ,
_t_ ), where _x_ is a probability and _t_ is a 0 or 1 truth value:

Suppose that _a_ and _b_ are positive and _a_ / _b_ = (1− _r_ )/ _r_. Then if
my scribbled notes are correct, it is straightforward but annoying to check
that _s_ 1 is proper, and it has a discontinuous reward _a_ for meeting
threshold _r_ with respect to a truth and a discontinuous penalty  − _a_ for
meeting threshold _r_ with respect to a falsehood. To get a strictly proper
scoring rule, just add to it any strictly proper continous accuracy scoring
rule (e.g., Brier).

_s_ 1( _x_ , _t_ ) = _a_ if _r_ < _x_ and _t_ = 1 or if _x_ < 1 − _r_ and _t_
= 0

Suppose there is a distinctive and significant value to knowledge. What I mean
by that is that if two epistemic are very similar in terms of truth, the level
and type of justification, the subject matter and its relevant to life, the
degree of belief, etc., but one is knowledge and the other is not, then the
one that is knowledge has a significantly higher value because it is
knowledge.

_s_ 1( _x_ , _t_ ) = − _b_ if _r_ < _x_ and _t_ = 0 or if _x_ < 1 − _r_ and
_t_ = 1.

This hypothesis initially seemed to me to have an unfortunate consequence.
Suppose Alice has just barely exceeded the threshold for knowledge of _p_ ,
and she is offered a cost-free piece of information that may turn out to
slightly increase or slightly decrease her overall evidence with respect to
_p_ , where the decrease would be sufficient to lose her knowledge of _p_
(since she has only “barely” exceeded the evidential threshold for knowledge).
It seems that Alice should refuse to look at the information, since the
benefit of a slight improvement in credence if the evidence is non-misleading
is outweighed by the danger of a significant and discontinuous loss of value
due to loss of knowledge.

In [a recent post](http://alexanderpruss.blogspot.com/2023/01/knowing-you-
will-soon-have-enough.html), I noted that it is possible to cook up a Bayesian
setup where you don’t meet some threshold, say for belief or knowledge, with
respect to some proposition, but you do meet the same threshold with respect
to the claim that after you examine a piece of evidence, then you _will_ meet
the threshold. This is counterintuitive: it seems to imply that you can know
that you will have enough evidence to know something even though you don’t
yet. In a comment, Ian noted that one way out of this is to say that beliefs
do not correspond to sharp credences. It then occurred to me that one could
use the setup to probe the question of how sharp our credences are and what
the thresholds for things like belief and knowledge are, perhaps
complementarily to the considerations in [this
paper](https://philarchive.org/rec/PRUBSA).

Let _α_ > 0 be the “squishiness” of our credences. Let’s say that for one
credence to be definitely bigger than another, their difference has to be at
least _α_ , and that to definitely meet (fail to meet) a threshold, we must be
at least _α_ above (below) it. We assume that our threshold _r_ is definitely
less than one: _r_ \+ _α_ ≤ 1.

Note that the squishiness of our credences likely varies with where the
credences lie on the line from 0 to 1, i.e., varies with respect to the
relevant threshold. For we _can_ tell the difference between 0.999 and 1.000,
but we probably can’t tell the difference between 0.700 and 0.701. So the
squishiness should probably be counted relative to the threshold. Or perhaps
it should be correlated to log-odds. But I need to get to looking at grad
admissions files now.

we meet _r_ with respect to the proposition that we will meet the threshold
with respect to _p_ after we examine evidence _E_.

We thus have our joint constraint on the squishiness of our credences: bad
things happen if our credences are so precise as to make (6) true with respect
to a threshold _r_ for which we don’t want (a)–(c) to definitely hold. The
easiest scenario for making (a)–(c) definitely hold will be a binary test with
no false negatives.

What does this tell us about _r_ and _α_? We can actually figure this out.
Consider a test for _p_ that have no false negatives, but has a false positive
rate of _β_. Let _E_ be a positive test result. Our best bet to generating a
counterexample to (a)–(c) will be if the priors for _p_ are as close to _r_ as
possible while yet definitely below, i.e., if the priors for _p_ are _r_ −
_α_. For making the priors be that makes (c) easier to definitely satisfy
while keeping (b) definitely satisfied. Since there are no false negatives,
the posterior for _p_ will be:

_P_ ( _p_ | _E_ ) = _P_ ( _p_ )/ _P_ ( _E_ ) = ( _r_ − _α_ )/( _r_ − _α_ + _β_
(1−( _r_ − _α_ ))).

Let _z_ = _r_ − _α_ \+ _β_ (1−( _r_ − _α_ )) = (1− _β_ )( _r_ − _α_ ) + _β_.
This is the prior probability of a positive test result. We will definitely
meet _r_ on a positive test result just in case we have ( _r_ − _α_ )/ _z_ =
_P_ ( _p_ | _E_ ) ≥ _r_ \+ _α_ , i.e., just in case

(We definitely won’t meet _r_ on a negative test result.) Thus to get (c)
definitely true, we need (3) to hold as well as the probability of a positive
test result to be at least _r_ \+ _α_ :

It’s in fact not hard to see that (6) is necessary and sufficient for the
existence of a case where (a)–(c) definitely hold.

For suppose we have a credence threshold _r_ and that our intuitions agree
that we can’t have a situation where:

We thus have our joint constraint on the squishiness of our credences: bad
things happen if our credences have a level of precision equal to the right-
hand-side of (6). What exactly that says about _α_ depends on where the
relevant threshold lies. If the threshold _r_ is 1/2, the squishiness _α_ is
0.15. That’s surely higher than the actual squishiness of our credences. So if
we are concerned merely with the threshold being more-likely-than-not, then we
can’t avoid the paradox, because there will be cases where our credence is
definitely below the threshold and it’s definitely above the threshold that
examination of the evidence will push us about the threshold.

Note that by appropriate choice of _β_ , we can make _z_ be anything between
_r_ − _α_ and 1, and the right-hand-side of (3) is at least _r_ − _α_ since
_r_ \+ _α_ ≤ 1. Thus we can make (c) definitely hold as long as the right-
hand-side of (3) is bigger than or equal to the right-hand-side of (4), i.e.,
if and only if:

But what’s a reasonable threshold for belief? Maybe something like 0.9 or
0.95. At _r_ = 0.9, the squishiness needed for paradox is _α_ = 0.046. I
suspect our credences are more precise than that. If we agree that the
squishiness of our credences is less than 4.6%, then we have an argument that
the threshold for belief is _more_ than 0.9. On the other hand, at _r_ = 0.95,
the squishiness needed for paradox is 2.4%. At this point, it becomes more
plausible that our credences lack that kind of precision, but it’s not clear.
At _r_ = 0.98, the squishiness needed for paradox dips below 1%. Depending on
how precise we think our credences are, we get an argument that the threshold
for belief is something like 0.95 or 0.98.

The difficult case, however, is when people’s consciences are mistaken to such
a degree that conscience requires them to do something that unjustly harms
others. (A less problematic mistake is when conscience is mistaken to such a
degree that conscience requires them to do something that’s permissible, but
not wrong. In those cases, tolerance is clearly called for. We shouldn’t
pressure vegetarians to eat animals even if their conscientious objection to
eating animals happens to be mistaken.)

When someone’s conscience mistakenly requires something that violates an
objective moral rule, there is a two-fold benefit to that person from a law
incentivizing following the moral rule. The law is a teacher, and the state’s
disapproval may change one’s mind about the matter. And even if it a harm to
one to violate conscience, it is also a harm to one to do something wrong even
inculpably. Thus, the harm of violating conscience is somewhat offset by the
benefit from not doing something else that is wrong.

Now, acting against one’s conscience _is_ always wrong, and is almost always
_culpably_ wrong. For the most common case when doing something wrong isn’t
culpable is that one is ignorant of the wrongness, but when one acts against
one’s conscience one surely isn’t ignorant that one is acting against
conscience, and that we ought follow our conscience is obvious.

In cases where doing wrong and suffering wrong are of roughly the same order
of magnitude, it is very intuitive that we should prevent the suffering of
wrong rather than the doing of wrong. Imagine that Alice is drowning while at
the same time Bob is getting ready to assassinate a politician, but we know
for sure that Bob’s bullets have all been replaced with blanks. If our choice
is whether to try to dissuade Bob from attempting murder or keep Alice from
drowning, we should keep Alice from drowning, evne if on the Socratic view the
harm to Bob from attempting murder will be greater than that to Alice from
drowning. (I am assuming that in this case the two harms are nonetheless of
something like the same order of magnitude.)

In some cases the person of mistaken conscience will still do the wrong deed
despite the law’s contrary incentive. In such a case, both the perpetrator and
the victim may be slightly better off for the law. The victim has a dignitary
benefit from the very fact that the state says that the harm was unlawful.
That dignitary benefit may be a cold comfort if the victim suffered a grave
harm, but it is still a benefit. And the perpetrator is slightly better off,
because following one’s conscience _against_ external pressure has an element
of admirability even when the conscience is mistaken.

One might think that what I said earlier implies that in this difficult case
the state should always allow people to follow their conscience, because after
all it is worse to do wrong—and violating conscience is wrong—than to have
wrong done to one. But that would be absurd and horrible—think of a racist
murderer whose faulty conscience requires them to kill.

Nonetheless, there will be cases where these considerations do not suffice,
and the law should be tolerant of mistaken conscience.

A reasonable optimism says that in most cases most people’s consciences are
correct. Thus typically we would expect that most violators of a legitimate
law will not be acting out of conscience—for a necessarily condition for the
legitimacy of a law is that it does not conflict with a correct conscience.
Thus, even if there is the rare murderer acting from mistaken conscience, most
murderers act against conscience, and by incentivizing abstention from murder,
in most cases the law _helps_ people follow their conscience, and the small
number of other cases can be tolerated as a side effect. Thus the
considerations of conscience favor intolerant laws in such cases. Nonetheless,
there are cases where most violators of a law would likely be acting from
conscience. Thus, if we had a law requiring eating meat, we would expect that
most of the violators would be conscientious. Similarly, a law against
something—say, the wearing of certain clothes or symbols—that is rarely done
except as a religious practice would likely be a law most violators of which
would be conscientious.

Now, the state, just like individuals, should _ceteris paribus_ avoid causing
grave harm. Hence, the state should generally avoid getting people to do
things that violate their conscience in major matters.

That said, I think a qualification is plausible. Some wrongdoings are minor,
and in those cases the harm to the wrongdoer may be minor as well. But in any
case, to get someone to act against their conscience in a matter that
according to their conscience is major is to do them grave harm, a harm not
that different from death.

One of the central insights of Western philosophy, beginning with Socrates,
has been that few if any things are as bad for an individual as culpably doing
wrong. It is better, we are told through much of the Western philosophical
tradition, that it is better to suffer than do injustice.

In a just defensive war, to refuse to fight to defend one’s fellow citizens
without special reason (perhaps priests and doctors should not kill) is wrong.
But a grave harm is done to a conscientious objector who is gotten to fight by
legal incentives. Let’s think through the five considerations above. The first
mainly applies to laws prohibiting a behavior rather than ones requiring a
behavior. Short of brainwashing, it is impossible to _make_ someone fight. (We
could superglue their hands to a gun, and then administer electric shocks
causing their fingers to spasm and fire a bullet, but that wouldn’t count as
_fighting_.) The second applies somewhat: we do need to weigh the harms to
innocent citizens from enemy invaders, harms that might be prevented if our
conscientious objector fought. But note that there is something rather
speculative about these harms. Someone who fights contrary to conscience is
unlikely to be a very effective fighter, and it is far from clear that their
military activity would actually prevent any actual harm to innocents. Now,
regarding the third consideration, one can design a conscription law with an
exemption that few who aren’t conscientious objectors would take advantage of.
One way to do this is to require evidence of one’s conscience’s objection to
fighting (e.g., prior membership in a pacifist organization). Another way is
to impose non-combat duties on conscientious objectors that are as onerous and
maybe as dangerous as combat would be. Regarding the fourth consideration, it
seems unlikely that a typical conscientious objector’s objections to war would
be changed by legal penalties. And the fifth seems a weak consideration in
general. Putting all these together, we do not outweigh the _prima facie_
considerations against pressuring conscientious objectors to act against their
(mistaken) conscience from the harms in going against conscience.

The harm of violating one’s conscience only happens to one if one _willingly_
violates one’s conscience. If law enforcement physically prevents me from
doing something that conscience requires from me, then I haven’t suffered the
harm. Thus, interestingly, the consideration I sketched against violating
one’s conscience does not apply when one is literally forced (fear of
punishment, unless it is severe enough to suspend one’s freedom of will, does
not actually _force_ , but only incentives).

If the test comes out positive, it will be another piece of evidence for the
hypothesis that I have _C_ , and it will push me over the edge to belief that
I have _C_.

I don’t know that I have _C_ but I know that I will know.

Ian,  
  
I hadn't thought about this in the context of reflection. But technically
there is no counterexample here to van Fraassen's reflection principle, which
only says that P(H | credence at t will be p) = p. My examples satisfy that,
assuming full transparency about my own credences and full certainty that I
will conditionalize (cf.: https://jonathanweisberg.org/pdf/C_R_and_SKv2.SP.pdf
).  
  
The original reflection principle implies that my credence should be the
expected value of my future credences (at a fixed future time; we might also
generalize to any martingale stopping time). This is not a problem in my
cases, because although it is likely that my future credence will be bigger
than it is now, there is a small chance that my future credence will plummet
to zero due to a negative test result, as William notes.  
  
Upon further thought, I think it can be quite reasonable to say: "I know that
tomorrow I will have knowledge-level evidence for p, but I don't yet." And if
so, then my subsequent post on squishiness, even if technically correct, is
moot.

If the test comes out positive, I will have enough evidence to know that I
have _C_.

I’d take this as an argument against the view that belief is (or is justified
by) credence above a threshold.  
  
But that view seems implausible in any case. It’s pretty much only in gambling
setups with well-defined chances that we have sharp credences. But we all have
vast numbers of beliefs (taken in an informal ‘folk’ sense). So it seems that
some other approach is needed.  
  
A thorough Bayesian would be untroubled by this example. She would have her
credences, and that would be enough. She would not care whether they exceeded
some arbitrary threshold for belief. She would be similarly untroubled by the
lottery paradox. But no one in practice is a thorough Bayesian, or even close.
So again, it seems that some other approach is needed.

Still, there is something odd about (5). It’s a bit like the line:

I don’t yet have enough evidence to know that I have _C_ , but I know that in
a moment I will.

I have enough evidence to believe that the test would come out positive.

But (6) is absurd: if I know that I will know something, then I am in a
position to know that the matter is so, since that I will know _p_ entails
that _p_ is true (assuming that _p_ doesn’t concern an open future). However,
there is no similar absurdity in (5). I may know that I will have enough
evidence to know _C_ , but that’s not the same as knowing that I will _know_
_C_ or even be in a position to know _C_. For it is possible to have enough
evidence to know something without being in a position to know it (namely,
when the thing isn’t true or when one is Gettiered).

Suppose I am just the slightest bit short of the evidence needed for belief
that I have some condition _C_. I consider taking a test for _C_ that has a
zero false negative rate and a middling false positive rate—neither close to
zero nor close to one. On reasonable numerical interpretations of the previous
two sentences:

To see that (1) is true, note that the test is certain to come out positive if
I have _C_ and has a significant probability of coming out positive even if I
don’t have _C_. Hence, the probability of a positive test result will be
significantly higher than the probability that I have _C_. But I am just the
slightest bit short of the evidence needed for belief that I have _C_ , so the
evidence that the test would be positive (let’s suppose a deterministic
setting, so we have no worries about the sense of the subjunctive conditional
here) is sufficient for belief.

If I am rational, my beliefs will follow the evidence. So if I am rational, in
a situation like the above, I will take myself to have a way of bringing it
about that I believe, and do so rationally, that I have _C_. Moreover, this
way of bringing it about that I believe that I have _C_ will itself be
perfectly rational if the test is free. For of course it’s rational to accept
free information. So I will be in a position where I am rationally able to
bring it about that I rationally believe _C_ , while not yet believing it.

**Appendix:** Suppose the threshold for belief or knowledge is _r_ , where _r_
< 1. Suppose that the false-positive rate for the test is 1/2 and the false-
negative rate is zero. If _E_ is a positive test result, then _P_ ( _C_ | _E_
) = _P_ ( _C_ ) _P_ ( _E_ | _C_ )/ _P_ ( _E_ ) = _P_ ( _C_ )/ _P_ ( _E_ ) = 2
_P_ ( _C_ )/(1+ _P_ ( _C_ )). It follows by a bit of algebra that if my prior
_P_ ( _C_ ) is more than _r_ /(2− _r_ ), then _P_ ( _C_ | _E_ ) is above the
threshold _r_. Since _r_ < 1, we have _r_ /(2− _r_ ) < _r_ , and so the story
(either in the belief or knowledge form) works for the non-empty range of
priors strictly between _r_ /(2− _r_ ) and _r_.

In other words, oddly enough, just prior to getting the test results I can
reasonably say:

To be clear, I don’t doubt that the example works. It does, and it illustrates
a genuine problem with the view that belief is (or is justified by) credence
above a threshold. Specifically, this violates an apparently natural
reflection principle, that if I rationally believe that I will rationally come
to believe, then I should believe now. Issues like this seem unavoidable with
a threshold (other than 1). The lottery paradox illustrates a different one:
violation of logical closure. Of course, you may be prepared to live with such
issues - maybe beliefs don’t have to be logically consistent.  
  
  
My problem with the threshold approach is more basic. If I don’t have a
credence, I can’t apply the threshold test. If I do have a credence, then
noting whether it is above or below a threshold adds nothing useful. One
response to this is straight Bayesianism, which rejects any role for belief
(beyond credence 1). Another is that credence and belief are separate and
related, but not straightforwardly. In the example, I would say that I
_believe_ the relevant medical information about the chance that I have C and
about the reliability of the test, and set my _credences_ accordingly.

In fact, the same thing can be said about knowledge, assuming there is
knowledge in lottery situations. For suppose that I am just the slightest bit
short of the evidence needed for _knowledge_ that I have _C_. Then I can set
up the story such that:

Ian:  
  
I don't think the worries about sharpness of credences apply. For we can
suppose that the cases we apply the argument to are precisely ones with
sufficiently sharp credences. They might be cases concerning gambling
scenarios, or they could be medical cases where the only relevant data one has
is statistics from the most recent publications on the subject.  
  
I think all we really need for the argument is that in certain kinds of cases
meeting a credence threshold is what makes the difference between not having a
belief (or a justified belief) and having a belief (or a justified belief).  
  
By the way, suppose we think that the argument doesn't work for sharpness
reasons. Let's say that u is a level of sharpness such that it only makes
sense to say that you've met the threshold if you are at r+u or higher and
that you haven't met it if you are at r-u or lower, and if you're between r-u
and r+u it's indeterminate. Then working through the Bayesian details should
provide one with an interesting joint constraint on r and u. And then we can
ask see if it's reasonable to think that r and u actually satisfy the joint
constraint.

To see that (2) is true, note that given that the false negative rate is zero,
and the false positive rate is not close to one, I will indeed have non-
negligible evidence for _C_ if the test is positive.

I have enough evidence to _know_ that the test would come out positive,

As long as the test could be a true negative, you could always be surprised
and update to P of 0.0 if the test is negative. So waiting on the test before
you update is justified.

I have a problem with (2). (2) only follows if a complete explanation is
possible, which is kind of what you want to prove. So, it seems to me (2) begs
the question.  
If the 'water' is a brute fact, 'the earth floats on water' is as far as an
explanation can get. In a way it is merely partial but it isn't actually part
of something complete because there isn't something complete.  
That's why (1) is 'kind of right' but it isn't completely right.  
  
  
  

When you say grounding should not be thought of as a real relation do you mean
that the way grounding connects things and the way relations connects things
are different? Or do you mean that grounded things are not really distinct
from their grounds in extra mental reality? I apologize if I am
misunderstanding something, it just seems to me that grounding must be a real
relation in extra mental reality. For example God seems to ground our
existence in some way but we are really related to and therefore really
distinct from God. Also substances seem to ground their accidents but are
really distinct from and therefore really related to their accidents. But then
if grounding is in fact a real relation then the monism argument seems to
work.

Dr Pruss, do you think that Bradley type regresses must also stop in some sort
of self explanatory fact? For example instead of having an infinite regress of
relations being related to their relata maybe it is somehow self explanatory
that the relation is related to its relata?

At least in some cases there seems to be no relation between x and y when x
grounds y. For in some cases of where x grounds y, there is nothing to y other
than x.  
  
Consider the fact that the moon is round or cubical. This is grounded in the
fact that the moon is round. But are there two facts here *and* a relation
between them? I doubt it.

I was thinking of an argument I had seen by Della Rocca where he attempts to
show that relations cannot exist and therefore we should accept monism. If I
understand it correctly the idea is that relations depend on their relata for
their existence and therefore are grounded in one or more of their relata.
Then by asking what this grounding relation depends on it would start an
infinite regress of relations.

A partial explanation is one that is a part of a complete explanation.

I am not sure I buy (1). But it sounds kind of right to me now. Additionally,
(3) kind of sounds correct on its own. If _A_ causes _B_ and _B_ causes _C_
but there is no explanation of _A_ , then it seems that _B_ and _C_ are really
unexplained. Aristotle notes that there was a presocratic philosopher who
explained why the earth doesn’t fall down by saying that it floats on water,
and he notes that the philosopher failed to ask the same question about the
water. I think one lesson of Aristotle’s critique is that if it is unexplained
why the water doesn’t fall down it is unexplained why the earth falls down.

An explanation going back to something self-explanatory involves the activity
of a necessary being.

Trevor:  
  
I don't know why we need to say that the relation is related to the relata.
The relation relates the relata.

Any explanation for an event _E_ that does not go all the way back to
something self-explanatory is merely partial.

So, if any event _E_ has an explanation, it has an explanation going all the
way back to something self-explanatory. (1,2)

What am I? A herring-eater, a husband, a badminton player, a philosopher, a
father, a Canadian, a six-footer and a human are all correct answers. There is
an ancient form of the “What is _x_?” question where we are looking for a
“central” answer, and of course “a human” is usually taken to be that one.
What makes for that answer being central?

Necessarily, _x_ is a good _F_ if and only if _x_ is good.

On the "male human" point: it does seem like I can say "I am good and a man
iff I am a good man." But then (if we take the above approach) it seems like I
am centrally (really, deep-down) a man, not merely a human. This seems to sit
less comfortably with the Athanasian point. Perhaps this depends on the notion
of gender-specific norms? I'm not enough of a philosopher of gender to know
the literature on things like that.  
  

**Response:** I deny that it’s absurd. It’s just that all the electrons we
meet _are_ good at electronicity.

And yet “a virtuous human” would not be the answer to the ancient “What am I
centrally, really, deep-down, essentially?” question even if I were in fact a
virtuous human.

Necessarily, I am good and a human if and only if I am a good human.

_x_ is “centrally, really, deep-down, essentially” _F_ just in case what it is
for _x_ to be good is for _x_ to be a good _F_.

Sometimes the word “essential” is thrown in: I am _essentially_ human. But
what does that mean? In contemporary analytic jargon, it just means that I
cannot exist without being human. But the “central” answer to “What am I?” is
not just an answer that states a property that I cannot lack. There are, after
all, many such properties essential in such a way that do not answer the
question. “Someone conceived in 1972” and “Someone in a world where 2+2=4”
attribute properties I cannot lack, but are not the central answers.

Maybe, but can you say: What it is for you to be good is for you to be a good
man?  
  
Maybe the issue is that there is an ambiguity. A man (in the relevant sense)
is an adult male human. But "good adult male human" is a bit ambiguous: is one
good at the adulting, the maleness or the humanness, or at all three?  
  
I would say that what makes you be good is that you are good at being human,
and given that you are a man, this may have certain implications that it
wouldn't have if you weren't a man. (E.g., that you are a man makes it bad for
you to say "I am not a man".)

So the sense of the “What am I centrally, really, deep-down, essentially?”
question isn’t just modal. What is it?

_x_ is good and _x_ is _F_ if and only if _x_ is a good _F_

Andrew:  
  
Nice paper!  
  
But I think classification is not what I am getting at. I am aptly classified
as a male human, a human, a mammal, a vertebrate, an animal, an organism, and
an enmattered thing. However, I think only "human" is the answer to the
question I am after, the "what am I really" question. I am looking for a
particular "deep" classification. This is neither the most general
classification nor the most specific one, but the one that captures what most
deeply I am.  
  
On Athanasian theology, what makes me be saved is that the second person of
the Trinity became human just as I am human. He also became a male human just
as I am a male human and he became a vertebrate just as I am a vertebrate. But
these things do not save me. What saves me is his co-humanity, not his co-
vertebricity or co-animality or co-maleness. There is something deeper about
humanity than about any of these other things.  
  
This may be ethically important, too. Should I care more for a squid or a
mouse, if their intelligence is equal? The mouse is a fellow vertebrate, but
so what? It makes no difference. Similarly, that someone is a fellow male
matters not a whit. However, that someone is a fellow human, that seems to
matter.

In other words, if I am a human, being a good human explains my being good. On
the other hand, even if I were a virtuous human, my being a good virtuous
human would not explain my being good. For redundancy is to be avoided in
explanation, and “good virtuous human” is redundant.

But the same is not true for any other attribute besides “human” among those
of the first paragraph. I can be good and a badminton player while not being a
good badminton player, and I can be a good herring-eater without being good
and a herring-eater. And I have no idea what it is to be a good six-footer,
but perhaps the fact that I am a quarter of an inch short of six feet right
now makes me not be one. (In some cases one direction may hold. It may be that
if I am good and a human, then I am a good human.)

In other words, that which I am centrally, really, deep-down and essentially
is that which sets the norms for me to be good _simpliciter_.

**Objection 2:** “Good” is attributive, and hence there is no such thing as
being good _simpliciter_.

So our initial account of what is being asked for is that we are asking for an
attribute _F_ such that:

Necessarily, if I am human, what it is for me to be good is for me to be a
good human.

Necessarily, I am good and a virtuous human if and only if I am a good
virtuous human.

But perhaps we can do better. The necessary biconditional (2) holds in the
case “virtuous human”, but in a kind of trivial way: “a good virtuous human”
is repetition. I think that, as often, we need to pass from a modal to a
hyperintensional characterization. Consider that not only is (1) true, but
also:

is not _generally_ logically valid. But some instances of a schema can be
logically valid even if the schema is not logically valid in general.

Another interpretation of the question of what we are is that it is asking for
*classification*. Generics can be useful here, because they (a) are apt
answers to classification questions and (b) still allow for exceptions. The
former feature makes them substantive. The latter makes them modest and immune
to certain styles of counterexample. For more on this (with respect to the
question of what *we* are, but the tools at play are widely applicable), see:
https://andrewmbailey.com/GenericAnimalism.pdf

**Objection 1:** Some things are “centrally” electrons, but something’s being
good at electronicity is absurd.

Ian:  
  
I am somewhat inclined to say that a high credence just *is* what a belief is.  
  
THToD:  
  
Like you and Alston, I am inclined to agree that "belief" is at best an
awkward way to describe God's cognition. However, I think a major part of the
reason why God doesn't fundamentally have beliefs is that there is no room for
a gap in God between cognition and reality. Whether an open theist can have
the "no gap" view of God depends on the variety of open theism. If the open
theist is an open futurist, who thinks there are no facts in reality about
future contingents, then they can have the "no gap" view. But an open theist
like Swinburne or van Inwagen who thinks there are facts about future
contingents and God doesn't know them thinks there is a gap between God's
knowledge and reality, and now God is sounding very much like us -- there is
stuff that God is uncertain about.

Alex  
  
As Ian says, (1) applies to finite creatures, it doesn't apply to an
omniscient being. On open theism, God knows everything that can be known, but
libertarian free choices cannot be known in advance. If God were to "guess"
what I will do tomorrow, of course on open theism, he could turn out to be
wrong, but it would be a guess and not a belief.

Speaking of "God's beliefs" seems about as coherent as discussing what God ate
for lunch. He's not a finite creature.

I suppose the best way out is for the open theist to deny (1).

A rational being believes anything that they take to have probability bigger
than 1 − (1/10100) given their evidence.

I’d reject (1) in any case. Perfectly rational beings would have no use for
belief. They would have consistent credences (which could be 0 or 1) for
everything. On this view, beliefs are a shortcut that we, with our limited
mental capacity, use to reduce processing load.

These three theses, together with some auxiliary assumptions, yield a serious
problem for open theism.

Consider worlds created by God that contain four hundred people, each of whom
has an independent 1/2 chance of freely choosing to eat an orange tomorrow
(they love their oranges). Let _p_ be the proposition that at least one of
these 400 people will freely choose to eat an orange tomorrow. The chance of
not- _p_ in any such world will be (1/2)400 < 1/10100. Assuming open theism,
so God doesn’t just directly know whether _p_ is true or not, God will take
the probability of _p_ in any such world to be bigger than 1 − (1/10100) and
by (1) God will believe _p_ in these worlds. But in some of these worlds, that
belief will turn out to be false—no one will freely eat the orange. And this
violates (3).

But there is a problem here. For even if there is a “success value” in
accomplishing a self-set goal, the strength of the reasons for pursuing the
follow-through is also proportioned to facts independent of this exercise of
normative power. Rather, the reasons for pursuing the follow-through will
include the internal and external goods of victory (winning as such, prizes,
adulation, etc.), and these are independent of one’s setting follow-through as
one’s goal.

Yet it seems that whatever is intended is intended as a means or an end. One
might reject this principle, taking follow-through to be a counterexample.

Maybe we should say this. Even if all intentional action is end-directed,
there are two kinds of reasons for an action: the reasons that come from the
value of the end and the reasons that come from the value of the pursuit of
that end. In the case of follow-through, there may be a fairly trivial success
value in the follow-through—a success value that comes from one’s exercise of
normative power in adopting the follow-through as one’s end—but that success
value provides only fairly trivial reasons. However, there can be
significantly non-trivial reasons for one’s pursuing that end, reasons
independent of that end.

As far as I know, in all racquet sports players are told to follow-through: to
continue the racquet swing after the ball or shuttle have left the racquet.
But of course the ball or shuttle doesn’t care what the racquet is doing at
that point. So what’s the point of follow-through? The usual story is this: by
aiming to follow-through, one hits the ball or shuttle better. If one weren’t
trying to follow-through, the swing’s direction would be wrong or the swing
might slow down.

Clearly the follow-through is _intended_ —it’s consciously planned, aimed at,
etc. But it need not be a means to anything one cares about in the game
(though, of course, in some cases it can be a means to impressing the
spectators or intimidating an opponent). But is it an end? It seems pointless
as an end!

Another move is this. We actually have a normative power to make something
_be_ an end. And then it becomes genuinely worth pursuing, because we have
adopted it as an end. So the player first exercises the normative power to
make follow-through be an end, and then pursues that end as an end.

This is interesting action-theoretically. The follow-through appears
pointless, because the agent’s interest is in what happens _before_ the
follow-through, the impact’s having the right physical properties, and yet
there is surely no backwards causation here. But there not appear to be an
effective way to reliably secure these physical properties of the impact
except by trying for the follow-through. So the follow-through itself is
pointless, but one’s _aiming at_ or _trying for_ the follow-through very much
has a point. And here the order of causality is respected: one swings aiming
at the follow-through, which causes an impact with the right physical
properties, and the swing then continues on to the “pointless” follow-through.

Alex  
  
How does it follow from the idea that God is not simple that God’s beliefs
change as the reality they are about changes?  
  
If (2) is true, isn't that a problem for divine simplicity as well? For on
divine simplicity, God id identical to God's beliefs, so if (2) is true it
seems that God changes as the reality changes, which conradicts divine
simplicity as well as divine immutability.

I agree with Walter. It seems to me that 2 is more directly incompatible with
God's atemporality than with his lack of complexity as such. An atemporal God
who sees Creation across its history in toto as a block, whether He is simple
or complex, has only true beliefs about everything that has been or will be,
and about when each fact will be true for finite creatures who are temporal.
But his beliefs don't change.  
  
Although a God who changed with and experienced time such that his beliefs
also changed might be thus considered complex across time by having temporal
parts, that would seem to be assuming an A-theory of time for all (including
God), then importing a B perspective at the end, incoherently. So, simplicity
of essence (at any point in time) and atemporality could be distinguished I
suppose, such that a person could affirm both simplicity and temporality of
God. I'm not saying that any open theist holds this opinion, or should, but it
seems a logical possibility from within that framework.

I agree that one could hold 1 without 2. But my argument is against people who
hold the package of 1 and 2. Think here of process theologians, and their more
orthodox cousins.

How? Easy. I am now sitting, and God knows that. So, a part of God is the
belief that I am sitting. But I can destroy that belief of God’s by standing
up! For as soon as I stand up, the belief that I am sitting will no longer
exist. But on the view in question, God’s beliefs are parts of him. So by
standing up, I would bring it about that a part of God doesn’t exist.

I'm not sure that the creaturely changes are necessarily "destroying" (or
creating) parts of God (beliefs) on the open theists' assumption. I think they
are determining them. I will assume open theists who believe God is maximally
knowledgeable, that is, knows all that can be known, which does not include
perfect knowledge of the future according to them.  
  
On this set of assumptions God will still, however, have perfect foreknowledge
of all possible outcomes as he can understand all possible cause/effect
branches as First Cause. He just doesn't know which path along the branching
options will eventuate before the time that they do. This means his knowledge
grows only in the sense that he is able to pick out which of each fully
foreseen branch is actualised. In other words, the only way his knowledge
grows is by the steady lengthening of a "track" through a foreknown "space" of
possible world-states over time. In such a scenario, creatures with free will
would determine the direction of the track and thus the precise determination
of God’s knowledge of it, but they would not be adding to or subtracting from
God's beliefs in any quantitative sense. The addition that would occur comes
about automatically as time flows, no matter what finite agents decide.  
  
Please note, I do not hold this position, but it seems to me it is not guilty
of the exact absurdity alleged here, whatever its other flaws.  

I think one can hold that God's beliefs are accidents of his which are not
identical to him (rejecting a strong doctrine of divine simplicity) without
holding that they are "parts" of him in any sense that makes (3) or (4)
absurd. Even for created beings like us, the accident-substance relationship
does not appear to be at all the same as the part-whole relationship as common
sense conceives it. (I would find it weird to say that my beliefs are a part
of me in the same sense that my arm is a part of my body, for instance.)

God is not simple, and in particular God’s beliefs are proper parts of God.

It depends on the details of how they think beliefs work. But suppose they
hold to this picture: for each proposition p that God knows, there is God's
act of believing p. Moreover, these opponents of simplicity think that God's
act of believing p is a different act for each different p that God believes.
These different acts are on that view something like accidents of God. Thus,
by raising one's arm one brings it about that God's belief that one's arm is
in a lowered state does not exist and that God's belief that one's arm is in a
raised state does exist, and so one destroys a part of God and creates a new
one. (It is a part of the package that I am criticizing that propositions are
tensed, and so it's not enough for God to eternally know that at t1 the arm is
lowered and at t2 the arm is raised.)

And of course by standing up, I bring it about that a new divine belief
exists. So:

Nobody seriously runs an argument against the existence of God from _trivial_
evils, like hangnails or mosquito bites.

Unknown  
  
So, God's actions do not mean much if He can't interact with people
negatively?

Perhaps not negatively, but certainly 'without love'. It seems to me that God
did not have to create you and thus God did not have to love you. If God were
forced to love you or loved you randomly, I wouldn't consider God to be a
moral being. I also wouldn't consider love to be a moral good if it were
forced. Therefore, it seems possible that evil actions are merely privations
of good actions, and thus one cannot choose to do "nothing" because "nothing"
is a lack of good, which seems to be some sort of evil. Perhaps one could make
a distinction between evils, but it seems that this might be a better
distinction that the one I previously drew.

The reason why few people seriously run an argument from trivial evils is
simply because the case is much more obvious in the case of horrendous evil.  
But a truly opnipotent being should be able to accomplish Evert good without
any evil, unless this is logically impossible  
So, unless there is a convincing argument for why a certain good cannot
posdibly be accomplished without permitting evil, the default position should
be that it is possible.  

On the other hand, if we think of _horrendous_ evils, like the torture of
children, it is difficult to think of much greater goods. Maybe with
difficulty we can come up with one or two possibilities, but not enough to
make it _easy_ to suppose a justification for God’s permission of the evil in
terms of the goods.

Why not? Here is a hypothesis. There are so very many possible much greater
goods—goods qualitatively and not just quantitatively much greter—that it
would be easy to suppose that God’s permitting the trivial evil could promote
or enhance one of these goods to a degree sufficient to yield justification.

Unknown  
  
I wouldn't say "forced" is the correct word here, but it is true that God
_necessarily_ loves you. So, in a sense, God _did/does_ have to love you.

There is, however, a difference between the cases. Many great ordinary goods
that dwarf trivial evils, like the courage of a Socrates, are known to us. Few
if any finite goods that dwarf horrendous evils are known to us. Nonetheless,
if theism is true, it is very likely that such goods are possible. And since
the argument from evil is addressed against the theist, it seems fair for the
theist to invoke that hierarchy.

Moreover, we might ask whether our ignorance of goods higher up in the
hierarchy of goods beyond the ordinary goods is not itself evidence against
the existence of such goods. Here, I think the answer is that it is very
little evidence. We would expect any particular finite being to be able to
recognize only a finite number of types of good, and thus the fact that there
are only a finite number of goods that we recognize is very little evidence
against the hypothesis of the upward hierarchy of goods.

So if God exists, we would expect there to be unknown possible finite goods
that are related to the known horrendous evils in something like the
proportion in which the known great finite goods are related to the known
trivial evils. Thus, if God exists, there very likely is  
an upward hierarchy of possible goods to which the horrendous evils of this
life stand like a mosquito bite to the courage of a Socrates. If we believe in
this hierarchy of goods, then it seems we should be no more impressed by the
atheological evidential force of horrendous evils than the ordinary person is
by the atheological evidential force of trivial evils.

There seems to be a legitimate reason to allow evil choices to exist and to
allow those evil choices to affect other people. Seemingly, actions wouldn't
mean much (such as freely loving God) if we were all in cubicles and couldn't
actually interact with people negatively. Free love and free hatred seems to
be two sides of the same coin. I would also say that there are legitimate
reasons for suffering to exist. Perhaps the world would have less good in it
if there is no suffering. Would God permit suffering to allow more good? I
don't see a reason as to why He wouldn't. Theodicies and defenses are very
interesting, though it seems to me that, intuitively at least, most theodicies
seem to deal with the problem of trivial evils.

However, if God exists, we would _expect_ there to be an unbounded upward
qualitative hierarchy of possible finite goods. God is infinitely good, and
finite goods are participations in God, so we would expect a hierarchy of
qualitatively greater and greater types of good that reflect God’s infinite
goodness better and better.

for any _A_ ∈ _H_. Then _p_ ′ is a probability on the algebra generated by _H_
and _Z_ extending _p_. Extend it to a probability on _F_ by Hahn-Banach. By
open-mindedness:

Suppose we have a probability space _Ω_ with algebra _F_ of events, and a
distinguished subalgebra _H_ of events on _Ω_. My interest here is in accuracy
_H_ -scoring rules, which take a (finitely-additive) probability assignment
_p_ on _H_ and assigns to it an _H_ -measurable score function _s_ ( _p_ ) on
_Ω_ , with values in [−∞, _M_ ] for some finite _M_ , subject to the
constraint that _s_ ( _p_ ) is _H_ -measurable. I will take the score of a
probability assignment to represent the epistemic utility or accuracy of _p_.

Neither implication can be reversed. To see this, consider the single-
proposition case, where _Ω_ has two points, say 0 and 1, and _H_ and _F_ are
the powerset of _Ω_ , and we are interested in the proposition that one of
these point, say 1, is the actual truth. The scoring rule _s_ is then
equivalent to a pair of functions _T_ and _F_ on [0,1] where _T_ ( _x_ ) = _s_
( _p_ _x_ )(1) and _F_ ( _x_ ) = _s_ ( _p_ _x_ )(0) where _p_ _x_ is the
probability that assigns _x_ to the point 1. Then _G_ _s_ corresponds to the
function _x_ _T_ ( _x_ ) + (1− _x_ ) _F_ ( _x_ ), and each is convex if and
only if the other is.

There is an event _Z_ in _F_ such that _Z_ ∩ _A_ is a non-empty proper subset
of _A_ for every non-empty member of _H_.

with the inequality strict if the open-mindedness is strict. Let _Z_ be as in
(3). Define

The scoring rule _s_ is _proper_ provided that _E_ _p_ _s_ ( _q_ ) ≤ _E_ _p_
_s_ ( _p_ ) for all _p_ and _q_ , and strictly so if the inequality is strict
whenever _p_ ≠ _q_. Propriety says that one never expects a different
probability from one’s own to have a better score (if one did, wouldn’t one
have switched to it?).

_G_ _s_ ( _p_ ) ≤ _t_ _G_ _s_ ( _p_ 1) + (1− _t_ ) _G_ _s_ ( _p_ 2).

Since open-mindedness is pretty plausible to people of a Bayesian persuasion,
this means that convexity of _G_ _s_ can be motivated independently of
propriety. Perhaps instead of focusing on propriety of _s_ as much as the
literature has done, we should focus on the convexity of _G_ _s_?

Given a scoring rule _s_ , let the expected score function _G_ _s_ on the
probabilities on _H_ be defined by _G_ _s_ ( _p_ ) = _E_ _p_ _s_ ( _p_ ), with
the same extension to probabilities on _F_ as scores had.

Let’s think about this suggestion. One of the most important uses of scoring
rules could be to evaluate the expected value of an experiment prior to doing
the experiment, and hence decide which experiment we should do. If we think of
an experiment as a finite partition _V_ of the probability space with each
cell having non-zero probability by one’s current lights _p_ , then the
expected value of the experiment is:

This technical assumption basically says that there is a non-trivial event
that is logically independent of everything in _H_. In real life, the
technical assumption is always satisfied, because there will always be
something independent of the algebra _H_ of events we are evaluating
probability assignments to (e.g., in many cases _Z_ can be the event that the
next coin toss by the investigator’s niece will be heads). I will prove that
(2) can be reversed in the Appendix.

Nonetheless, (2) can be reversed (both in the strict and non-strict versions)
on the following technical assumption:

But _p_ ′( _Z_ ) = _p_ ( _Ω_ ∩ _Z_ ) = _t_ and _p_ ′( _Z_ _c_ ) = 1 − _t_.
Moreover, _p_ ′ _Z_ = _p_ 1 on _H_ and _p_ ′ _Z_ _c_ = _p_ 2 on _H_. Since _H_
-scores don’t care what the probabilities are doing outside of _H_ , we have
_s_ ( _p_ ′ _Z_ ) = _s_ ( _p_ 1) and _s_ ( _p_ ′ _Z_ _c_ ) = _s_ ( _p_ 2) and
_G_ _s_ ( _p_ ′) = _G_ _s_ ( _p_ ). Moreover our scores are _H_ -measurable,
so _E_ _p_ ′ _Z_ _s_ ( _p_ 1) = _E_ _p_ 1 _s_ ( _p_ 1) and _E_ _p_ ′ _Z_ _c_
_s_ ( _p_ 2) = _E_ _p_ 2 _s_ ( _p_ 2). Thus (9) becomes:

To see that the non-strict version of (1) cannot be reversed, suppose ( _T_ ,
_F_ ) is a non-trivial proper scoring rule with the limit of _F_ ( _x_ )/ _x_
as _x_ goes to 0 finite. Now form a new scoring rule by letting _T_ * ( _x_ )
= _T_ ( _x_ ) + (1− _x_ ) _F_ ( _x_ )/ _x_. Consider the scoring rule ( _T_
*,0). The corresponding function _x_ _T_ * ( _x_ ) is going to be convex, but
( _T_ *,0) isn’t going to be proper unless _T_ * is constant, which isn’t
going to be true in general. The strict version is similar.

where _p_ _A_ is the result of conditionalizing _p_ on _A_. In other words, to
evaluate the expected values of experiments, all we care about is _G_ _s_ ,
not _s_ itself, and so the convexity of _G_ _s_ is a very natural condition:
we are never oligated to refuse to know the results of free experiments.

For a probability _p_ on _F_ , I will take the score of _p_ to be the score of
the restriction of _p_ to _H_. (Note that any finitely-additive probability on
_H_ extends to a finitely-additive probability on _F_ by Hahn-Banach theorem,
assuming Choice.)

To see that (2) cannot be reversed, note that the only non-trivial partition
is {{0}, {1}}. If our current probability for 1 is _x_ , the expected score
upon learning where we are is _x_ _T_ (1) + (1− _x_ ) _F_ (0). Strict open-
mindedness thus requires precisely that _x_ _T_ ( _x_ ) + (1− _x_ ) _F_ ( _x_
) < _x_ _T_ (1) + (1− _x_ ) _F_ (0) whenever _x_ is neither 0 nor 1. It is
clear that this is not enough for convexity—we can have wild oscillations of
_T_ and _F_ on (0,1) as long as _T_ (1) and _F_ (1) are large enough.

_p_ ′( _A_ ∩ _Z_ _c_ ) = (1− _t_ ) _p_ 2( _A_ )

∑ _A_ ∈ _V_ _p_ ( _A_ ) _E_ _p_ _A_ _s_ ( _p_ _A_ ) = ∑ _A_ ∈ _V_ _G_ _s_ (
_p_ _A_ ),

It is easy to see that adding (3) to our assumptions doesn’t help reverse (1).

_G_ _s_ ( _p_ ) ≤ _t_ _G_ _s_ ( _p_ 1) + (1− _t_ ) _G_ _s_ ( _p_ 2)

Say that the scoring rule _s_ is _open-minded_ provided that for any
probability _p_ on _F_ and any finite partition _V_ of _Ω_ into events in _F_
with non-zero _p_ -probability, the _p_ -expected score of finding out where
in _V_ we are and conditionalizing on that is at least as big as the current
_p_ -expected score. If the scoring rule is open-minded, then a Bayesian
conditionalizer is never precluded from accepting free information. Say that
the scoring rule _s_ is _strictly_ open-minded provided that the _p_ -expected
score increases of finding out where in _V_ we are and conditionalizing
increases whenever there is at least one event _E_ in _V_ such that _p_ (⋅|
_E_ ) differs from _p_ on _H_ and _p_ ( _E_ ) > 0.

_G_ _s_ ( _p_ ′) ≤ _p_ ′( _Z_ ) _E_ _p_ ′ _Z_ _s_ ( _p_ ′ _Z_ ) + _p_ ′( _Z_
_c_ ) _E_ _p_ ′ _Z_ _c_ _s_ ( _p_ ′ _Z_ _c_ ).

However, at least in the case where _Ω_ is finite, [it is known
that](https://www.bowaggoner.com/blog/2016/09-22-proper-scoring-
rules/index.html) any (strictly) convex _G_ _s_ is equal to _G_ _u_ for a some
(strictly) proper scoring rule _u_. So we don’t really gain that much
generality by moving from propriety of _s_ to convexity of _G_ _s_. Indeed,
the above observations show that for finite _Ω_ , a (strictly) open-minded way
of evaluating the expected epistemic values of experiments in a setting rich
enough to satisfy (3) is always generatable by a (strictly) proper scoring
rule.

Hence we have convexity. And given strict open-mindedness, the inequality will
be strict, and we get strict convexity.

Assume open-mindedness. Let _p_ 1 and _p_ 2 be two distinct probabilities on
_H_ and let _t_ ∈ (0,1). We must show that if _p_ = _t_ _p_ 1 \+ (1− _t_ ) _p_
2, then

We can see that something is problematic if we think about cases like this.
Suppose your current level of confidence is just slightly above the threshold,
and a graduate student in your lab proposes to do one last experiment in her
spare time, using equipment and supplies that would otherwise go to waste.
Given the reward structure, it will likely make sense for you to refuse this
free offer of additional information. If the experiment favors your
hypothesis, you get nothing out of it—you could have published without it, and
you’d still have the same longer term rewards available. But if the experiment
disfavors your hypothesis, it will likely make your paper unpublishable (since
you were at the threshold), but since it’s just one experiment, it is unlikely
to put you into the position of yet being able to publish a paper against the
hypothesis. At best it loses you the risk of the small negative reputation for
having been wrong, and since that’s a small penalty, and an unlikely one
(since most likely your hypothesis _is_ true by your data), so that’s not
worth it. In other words, the the structure rewards you for ignoring free
information.

What’s the solution? Maybe it’s this: reward people for publishing lots of
data, rather than for the data showing anything interestingly, and do so
sufficiently that it’s always worth publishing more data?

Fortunately, it’s not quite so bad. Publication is the short-term reward. But
there are long-term rewards and punishments. If one publishes, and later it
turns out that one was right, one may get significant social recognition as
the _discoverer_ of the truth of the hypothesis. And if one publishes, and
later it turns out one is wrong, one gets some negative reputation.

I just realized, with the help of a mention of _p_ -based biases and improper
scoring rules somewhere on the web, that what is going on here is precisely a
problem of a reward structure that does not result in a proper scoring rule,
where a proper scoring rule is one where your current probability assignment
is guaranteed to have an optimal expected score according to that very
probability assignment. Given an improper scoring rule, one has a perverse
incentive to change one’s probabilities without evidence.

To a first approximation, the problem is _really, really_ bad. Insofar as
publication is the relevant reward, it is a reward independent of the truth of
the matter! In other words, the scoring rule has a reward for gaining
probability 0.95 (say) in the hypothesis, regardless of whether the hypothesis
is true or false.

However, notice this. Fame for having been right is basically independent of
the exact probability of the hypothesis one established in the original paper.
As long as the probability was sufficient for publication, one is rewarded for
fame. Thus if it turns out that one was right, one’s long-term reward is fame
if and only if one’s probability met the threshold for publication and one was
right. And one’s penalty is some negative reputation if and only if one’s
probability met the threshold for publication and yet one was wrong. But note
that scientists are actually extremely forgiving of people putting forward
evidenced hypotheses that turn out to be false. Unlike in history, where some
people live on in infamy, scientists who turn out to be wrong do not suffer
infamy. At worst, some condescension. And it barely varies with your level of
confidence.

How can we fix this? We simply cannot realistically fix it if we have a high
probability threshold for publication. The only way to fix it while keeping a
high probability threshold would be by having a ridiculously high penalty for
being wrong. But we should’t do stuff like sentencing scientists to jail for
being wrong (which has
[happened](https://www.scientificamerican.com/article/italian-scientists-
get/)). Increasing the probability threshold for publication would only
require the penalty for being wrong to be increased. Decreasing probability
thresholds for publication helps a little. But as long as there is a larger
reputational benefit from getting things right than the reputational harm from
getting things wrong, we are going to have perverse incentives from a
probability threshold for publication bigger than 1/2, no matter where that
threshold lies. (This follows from [Fact
2](http://alexanderpruss.blogspot.com/2023/01/making-single-proposition-
scoring-rules.html) in my recent post, together with the observation that
[Schervish’s](https://projecteuclid.org/journals/annals-of-
statistics/volume-17/issue-4/A-General-Method-for-Comparing-Probability-
Assessors/10.1214/aos/1176347398.full) characterization of scoring rules shows
implies that any reward function corresponds to a unique up to additive
constant penalty function.)

Yeah, but note that the situation I discuss -- refusing more experiments once
you reach the threshold -- is not exactly p-hacking. First, I don't know that
actual p-hacking really yields an evidence-based probability of 0.95. What I
am worried about happens even if you get a genuine bona fide evidence-based
probability of 1-p.  
  
Second, the particular abuse that I mention probably doesn't fall under
p-hacking as one might think it's sound experimental design to set up a
stopping point in advance, and then stop. But given a strictly proper scoring
rule, it's always worth getting more data.

Interesting. But it should be noted the situation is more complicated. For
example, there are professional penalties for being caught p-hacking.

This is not a proper scoring rule. It’s not even close. To make it into a
proper scoring rule, the penalty for being wrong at the threshold would need
to be way higher than the reward for being right. Specifically, if the
threshold is _p_ (say 0.95), then the ratio of reward to penalty needs to be
(1− _p_ ) : _p_. If _p_ = 0.95, the reward to penalty ratio would need to be
1:19. If _p_ = 0.99, it would need to be a staggering 1:99, and if _p_ = 0.9,
it would need to be a still large 1:9. We are very, very far from that. And
when we add the truth-independent reward for publication, things become even
worse.

One of the most problematic aspects of some science practice is a cut-off, say
at 95%, for the evidence-based confidence needed for publication.

If your probability meets the threshold for publication and you’re wrong, at
worst small negative.

But if this is all correct, then by parallel we should not make a significant
distinction in kind between the pursuit of epistemic goods for oneself and the
pursuit of non-epistemic goods for oneself. Hence, norms governing the pursuit
of knowledge and understanding seem to be just a species of prudential norms.

We think highly morally of teachers who put an enormous effort into getting
their students to know and understand the material. Moreover, we think highly
of these teachers regardless of whether they are in a discipline, like some
branches of engineering, where the knowledge and understanding exists
primarily for the sake of non-epistemic goods, as when they are in a
discipline, like cosmology, where the knowledge and understanding is primarily
aimed at epistemic goods.

There is, thus, no significant difference in kind between the pursuit of
epistemic goods and the norms of the pursuit of other goods. We not
infrequently have to weigh one against the other, and it is a mark of the
virtuous person that they do this well.

Does this mean that epistemic norms are just a species of prudential norms?

I don’t think so. Consider that prudentially we also pursue goods of physical
health. However, norms of physical health are not a species of prudential
norms. It is the medical professional who is the expert on the norms of
physical health, not the prudent person as such. Prudential norms apply to
voluntary behavior as such, while the norms of physical health apply to the
body’s state and function. We might say that norms of the voluntary pursuit of
the fulfillment of the norms of physical health are prudential norms, but the
norms of physical health themselves are not prudential norms. Similarly, the
norms of the voluntary pursuit of the fulfillment of epistemic norms are
prudential norms, but the epistemic norms themselves are no more prudential
norms than the health norms are.

The virtues and vices in disseminating epistemic goods are just as much moral
virtues and vices as those in disseminating other goods, such as food,
shelter, friendship, or play, and there need be little difference in kind. The
person who is jealous of another’s knowledge has essentially the same kind of
vice as the one who is jealous of another’s physical strength. The person
generous with their time in teaching exhibits essentially the same virtue as
the one generous with their time in feeding others.

Corrected:  
  
Of course assigning probability x to event P is equivalent to assigning 1-x to
P'.  
So it's natural to assume that  
F(x) = T(1-x).  
If we further assume that F is differentiable, then applying Newton's
criterion for the extremum, i.e. setting the derivative w.r.t. y to zero, we
get  
x T'(y) - (1-x) T'(1-y) = 0  
should hold when y=x, or  
x T'(x) = (1-x) T'(1-x).  
Define  
g(x) = x T'(x),  
then the previous condition is equivalent to  
g(x) = g(1-x),  
i.e. g is symmetric on the interval [0,1].  
  
Given any such symmetric "kernel" g, we can then recover T as  
T(x) = \int g(x)/x dx.  
For example, if g(x)=A a constant, then T(x) = A log(x) + C.  
Choosing the A=+1 so that the extremum is a max rather than a min, and setting
C=0,  
we have the negative of the information content of the probability.  
  
In other words, score the estimate by how much information it would cost.

By the way, I suspect that all single-proposition scoring rules are generated
by the procedure in my post, up to an additive constant, but I don't have a
proof yet. I can show that the differences have to have zero derivative almost
everywhere, but that's not enough.

I wasn't attempting a general theory of scoring functions, just wanted to
provide an easy way to generate them, including the common ones.  
  
I misunderstood your intentions: you want custom scoring rules for a
particular P; I thought you were looking for general scoring rules that could
be used for any P.  
For example, if you want to evaluate your weatherman, you probably want to
evaluate his ability on all kinds of weather not just e.g. snow.  
  
Also, you seem to want to derive scoring rules from a RV V dependent on the
event P.  
The scheme is:  
1\. estimate x as Pr(P);  
2\. use estimate x to calculate the optimal strategy;  
3\. use the actual payoff V as the score.  
  
That should be guaranteed to form a proper scoring rule,  
since the optimization is built in at step 2.  
  
Is that right?

Accuracy scoring rules measure the value of your probability assignment’s
closeness to the truth. A scoring rule for a single proposition _p_ can be
thought of as a pair of functions, _T_ and _F_ on the interval [0,1] where _T_
( _x_ ) tells us the score for assigning _x_ to _p_ when _p_ is true and _F_ (
_x_ ) tells us the score for assigning _x_ to _p_ when _p_ is false. The
scoring rule is proper provided that:

Sure, the possibilities are infinite.  
  
I hadn't heard of the name Brier before, but that rule is equivalent to  
T(x) = -(1-x)^2,  
obtained when the kernel is  
g(x) = x(1-x).

Suppose first that _x_ ≥ _a_. Then the right-hand-side is _x_ (1− _a_ )/ _a_ −
(1− _x_ ). This is non-negative for _x_ ≥ _a_ , and the left-hand-side of (1)
is zero if _y_ < _a_ , so we are done if _y_ < _a_. Since _T_ and _F_ are
constant on [ _a_ ,1], the two sides of (1) are equal for _y_ ≥ _a_.

Thus, _T_ = ((1− _a_ )/ _a_ ) ⋅ 1[ _a_ ,0]. Now let’s check if we have the
propriety condition:

_x_ _T_ ( _x_ ) + (1− _x_ ) _F_ ( _x_ ) ≥ _x_ _T_ ( _y_ ) + (1− _x_ ) _F_ (
_y_ )

Here’s a sketch of the proof of Fact 1. Note first that if _F_ = _c_ is
constant, then _T_ _F_ ( _x_ ) = _c_ − ((1− _x_ )/ _x_ ) _c_ \+ _c_ ( _x_
−1−(1/2)−1) = _c_ \+ _c_ − 2 _c_ = 0 for all _x_. Since the map _F_ ↦ _T_ _F_
is linear, it follows that if _F_ and _H_ differ by a constant, then _T_ _F_
and _T_ _H_ are the same. Thus subtracting a constant from _F_ , we can assume
without loss of generality that _F_ is non-positive.

for all _x_ and _y_. If you assign probability _x_ to _p_ , then _x_ _T_ ( _y_
) + (1− _x_ ) _F_ ( _y_ ) measures your expected value of the score for
someone who assigns _y_. The propriety condition thus says that by your lights
there isn’t a better probability to assign. After all, if there were, wouldn’t
you assign it?

"Second, let T be any monotone increasing function on [0,1] that is finite
except perhaps at − 1."  
  
Perhaps you meant  
"Second, let T be any monotone increasing function on [0,1] that is finite
except perhaps at 1." Is -1 in the domain of T?  
  
Also, if F is assumed to be monotone decreasing, the case F=c is not
consistent with that hypothesis.  
  
Thanks for this post, I found the idea of proposition scoring to be very
interesting.  

I was going to offer a response to Zsolt Nagy, but I think it's just for the
best that you blocked him, Dr. Pruss. Hopefully, you can block his IP from
commenting. You might have to block the IP of VPNs as well. I don't know if
that's possible, though.

Since F is monotone decreasing, we already know that F is differentiable
almost everywhere in [0,1].

In other words, to generate a proper scoring rule, we just need to choose one
of the two functions making up the scoring rule, make sure it is monotone in
the right direction, and then we can generate the other function.

Fact 2 follows from Fact 1 together with the observation that ( _T_ , _F_ ) is
proper if and only if ( _F_ *, _T_ *) is proper, where _T_ * ( _x_ ) = _T_ (1−
_x_ ) and _F_ * ( _x_ ) = _F_ (1− _x_ ).

Btw, please remove my first comment - it was superseded by a correction.

**Fact 2:** The pair ( _T_ , _F_ _T_ ) a proper scoring rule.

Kerry:  
  
I meant 0, not -1. Fixed. Thanks!  
  
And by by "increasing" I meant "non-strictly". I.e., "non-decreasing".  
  
Andrew:  
  
The symmetry assumption is natural, but not inevitable. Here is one way to
construct proper accuracy scoring rules. You are going to be given a choice
between a number of different games, each of which has an outcome that depends
on the proposition q. You choose the game with the highest expected outcome
according to your probability you assigned to q. Your score then is the actual
outcome of the game. (You can get different scoring rules for different tie-
breaking procedures for equal expected outcomes.) This kind of a scoring rule
tends not to satisfy the symmetry condition. For instance, suppose you have a
choice between two games, one which pays $1 on q and $2 on ~q and one which
pays $3 on q and $1 on ~q. Let p be your probability of q. Then you will
choose game 1 if p is less than 1/3 and game 2 if p is more than 1/3. Then
T(p)=1 if p<1/3 and T(p)=3 if p>1/3, while F(p)=2 if p<1/3 and F(p)=1 if
p>1/3. (And what we have at 1/3 depends on the tie-breaking choice.) Then we
don't have your symmetry condition.  
  
Another family of cases of asymmetry is like this. If there is objective
morality (or God or a lawlike universe, etc.), there is a lot of value in
being nearly certain that there is objective morality (etc.) But if there is
no objective morality (or no God or no lawlike universe, etc.), there is
little value in being nearly certain of that.

I’ve been playing with how to construct proper scoring rules for a single
proposition, and I found two nice ways that are probably in the literature but
I haven’t seen explicitly. First, let _F_ be any monotone (not necessarily
strictly) decreasing function on [0,1] that is finite except perhaps at 1.
Then let:

_T_ _F_ ( _x_ ) = _F_ (1/2) − ((1− _x_ )/ _x_ ) _F_ ( _x_ ) − ∫1/2 _x_ _u_ −2
_F_ ( _u_ ) _d_ _u_.

Andrew:  
  
You should make the integral defining F go from something other than 0, as the
integral from 0 to x of 1/x is infinite.

It looks like my method does generate every proper scoring rule. There is an
old paper of Schervish which in Theorem 4.2 and Lemma A.7 gives a
characterization of all scoring rules. One can use the characterization to
show that for every T there is a unique (up to additive constant) F and vice
versa. https://projecteuclid.org/journals/annals-of-
statistics/volume-17/issue-4/A-General-Method-for-Comparing-Probability-
Assessors/10.1214/aos/1176347398.full

**Fact 1:** The pair ( _T_ _F_ , _F_ ) is a proper scoring rule.

We can then approximate _F_ by functions of the form ∑ _i_ _c_ _i_ 1[ _a_ _i_
,1] with _c_ _i_ non-positive (here I have to confess to not having checked
all the details of the approximation) and by linearity we only need to check
propriety for _F_ = − 1[ _a_ ,0]. If _a_ = 0, then _F_ is constant and _T_ _F_
will be zero, and we will trivially have propriety. So suppose _a_ > 0. Let
_T_ ( _x_ ) = − ((1− _x_ )/ _x_ ) _F_ ( _x_ ) − ∫0 _x_ _u_ −2 _F_ ( _u_ ) _d_
_u_. This differs by a constant from _T_ _F_ , so ( _T_ _F_ , _F_ ) will be
proper if and only if ( _T_ , _F_ ) is. Note that _T_ ( _x_ ) = 0 for _x_ <
_a_ and for _x_ ≥ _a_ we have:

_F_ _T_ ( _x_ ) = _T_ (1/2) − ( _x_ /(1− _x_ )) _T_ ( _x_ ) + ∫1/2 _x_ 1/(1−
_u_ )2 _T_ ( _u_ ) _d_ _u_.

_T_ ( _x_ ) = ((1− _x_ )/ _x_ ) + ∫ _a_ _x_ _u_ −2 _d_ _u_ = ((1− _x_ )/ _x_ )
− ( _x_ −1− _a_ −1) = _a_ −1 − 1.

Now suppose that _x_ < _a_. Then the right-hand-side is zero. And the left-
hand-side is zero unless _y_ ≥ _a_. So suppose _y_ ≥ _a_. Since _T_ and _F_
are constant on [ _a_ ,1], we only need to check (1) at _y_ = 1. At _y_ = 1,
the left-hand-side of (1) is _x_ (1− _a_ )/ _a_ − (1− _x_ ) ≤ 0 if _x_ < _a_.

I'm not a philosopher, but causal finitism would seem to be a different beast
from "possibility" finitism.

_x_ _T_ ( _y_ ) + (1− _x_ ) _F_ ( _y_ ) ≤ _x_ _T_ ( _x_ ) + (1− _x_ ) _F_ (
_x_ ).

Second, let _T_ be any monotone increasing function on [0,1] that is finite
except perhaps at 0. Let:

Or maybe we should just get rid of the concept of knowledge and theorize in
terms of credence, justification and truth.

More seriously, we think of knowledge as something stable. But since evidence
comes in degrees, there have got to be cases of knowledge that are quite
unstable—cases where one “just barely knows”. It makes sense to think that if
knowledge has some special value, these cases have rather less of it. Maybe
it’s because knowledge comes in degrees, and these cases have less knowledge.

In the _Meno_ , when Socrates talks about knowledge as something that is tied
down (like the statues of Daedalus), the thing that does the tying down
doesn’t seem to be 20th century ‘justification’ or anything about reaching a
certain credence but “an account of the reason why”:  
  
“True opinions . . . are not willing to remain long, and they escape from a
man’s mind, so that they are not worth much until one ties them down by
(giving) an account of the reason why [τις αὐτὰς δήσῃ αἰτίας λογισμῷ]. . . .
After they are tied down, in the first place they become knowledge, and then
they remain in place” (97e–98a).  
  
I wonder if we get more stability when we do this route than the
justification/credence route.

I expect that is what Socrates is getting at. My thought is that this might be
the reason we think that knowledge is something that is tied down—because
scientia or derivation from essence or something like that is stable, even if
‘having passed the evidential threshold’ isn’t.

This method only works if my credence is slightly above what’s needed for
knowledge. If what’s needed for knowledge is 0.990, then as soon as my
credence rises to 0.995, there is no rational method with reliability better
than 1/2 for making me lose the credence needed for knowledge (this follows
from Proposition 1 [here](https://philpapers.org/archive/PRUBSA.pdf)). So if
you find yourself coming to know something that you don’t want to know, you
should act fast, or you’ll have so much credence you will be beyond rational
help. :-)

Sometimes we know things we wish we didn’t. In some cases, without any
brainwashing, forgetting or other irrational processes, there is a fairly
reliable way to make that wish come true.

Suppose that a necessary condition for knowing is that my evidence yields a
credence of 0.9900, and that I know _p_ with evidence yielding a credence of
0.9910. Then here is how I can rid myself of the knowledge fairly reliably. I
find someone completely trustworthy who would know for sure whether _p_ is
true, and I pay them to do the following:

Then at least 7/8 of the time, they will inform me that the conjunction is
false. That’s a little bit of evidence against _p_. I do a Bayesian update on
this evidence, and my posterior credence will be 0.9897, which is not enough
for knowledge. Thus, with at least 7/8 reliability, I can lose my knowledge.

Maybe Socrates is more talking about understanding than knowledge, something
like medieval scientia or Aristotle's derivation from essence.  
  
If you see someone doing a bizarre thing, you have no idea why they are doing
it, but you can see and hence know that they are doing it. Conversely, one can
give an account of why something is so without knowing the thing: for
instance, suppose the police have a strong suspicion that Alice killed Bob,
and they know exactly what motive Alice had to kill Bob, but they don't have
much evidence to back up the suspicion. In that case, they can give an account
of the reason why, but they don't have knowledge of why Alice killed Bob, even
if in fact she did.

Inform me whether the following conjunction is true: all coins landed heads
and _p_ is true.

Suppose there is a distinctive and significant value to knowledge. What I mean
by that is that if two epistemic are very similar in terms of truth, the level
and type of justification, the subject matter and its relevant to life, the
degree of belief, etc., but one is knowledge and the other is not, then the
one that is knowledge has a significantly higher value because it is
knowledge.

But that’s not quite right. For from Alice’s point of view, because the
threshold for knowledge is not 1, there is a real possibility that _p_ is
false. But it may be that just as there is a discontinuous gain in epistemic
value when your (rational) credence becomes sufficient for knowledge of
something that is in fact true, it may be that there is a discontinuous loss
of epistemic value when your credence becomes sufficient for knowledge of
something false. (Of course, you can’t know anything false, but you can have
evidence-sufficient-for-knowledge with respect to something false.) This is
not implausible, and given this, by looking at the information, by her lights
Alice also has a chance of a significant gain in value due to losing the
illusion of knowledge in something false.

_s_ 1( _x_ , _t_ ) = − _b_ if _r_ < _x_ and _t_ = 0 or if _x_ < 1 − _r_ and
_t_ = 1.

_s_ 1( _x_ , _t_ ) = 0 if 1 − _r_ ≤ _x_ ≤ _r_

If we think that it’s never rational for a rational agent to refuse free
information, then the above argument can be made rigorous to establish that
any discontinuous rise in the epistemic value of credence at the point at
which knowledge of a truth is reached is exactly mirrored by a discontinuous
fall in the epistemic value of a state of credence where seeming-knowledge of
a falsehood is reached. Moreover, the rise and the fall must be in the ratio 1
− _r_ : _r_ where _r_ is the knowledge threshold. Note that for knowledge, _r_
is plausibly pretty large, around 0.95 at least, and so the ratio between the
special value of knowledge of a truth and the special disvalue of evidence-
sufficient-for-knowledge for a falsehood will need to be at most 1:19. This
kind of a ratio seems intuitively implausible to me. It seems unlikely that
the special disvalue of evidence-sufficient-for-knowledge of a falsehood is an
order of magnitude greater than the special value of knowledge. This
contributes to my scepticism that there is a special value of knowledge.

Plausibly, then, if we imagine Alice has some evidence for a truth _p_ that is
insufficient for knowledge, and slowly and continuously her evidence for _p_
mounts up, when the evidence has crossed the threshold needed for knowledge,
the value of Alice’s state with respect to _p_ will have suddenly and
discontinuously increased.

Can we rigorously model this kind of an epistemic value assignment? I think
so. Consider the following discontinuous accuracy scoring rule _s_ 1( _x_ ,
_t_ ), where _x_ is a probability and _t_ is a 0 or 1 truth value:

_s_ 1( _x_ , _t_ ) = _a_ if _r_ < _x_ and _t_ = 1 or if _x_ < 1 − _r_ and _t_
= 0

Suppose that _a_ and _b_ are positive and _a_ / _b_ = (1− _r_ )/ _r_. Then if
my scribbled notes are correct, it is straightforward but annoying to check
that _s_ 1 is proper, and it has a discontinuous reward _a_ for meeting
threshold _r_ with respect to a truth and a discontinuous penalty  − _a_ for
meeting threshold _r_ with respect to a falsehood. To get a strictly proper
scoring rule, just add to it any strictly proper continous accuracy scoring
rule (e.g., Brier).

This hypothesis initially seemed to me to have an unfortunate consequence.
Suppose Alice has just barely exceeded the threshold for knowledge of _p_ ,
and she is offered a cost-free piece of information that may turn out to
slightly increase or slightly decrease her overall evidence with respect to
_p_ , where the decrease would be sufficient to lose her knowledge of _p_
(since she has only “barely” exceeded the evidential threshold for knowledge).
It seems that Alice should refuse to look at the information, since the
benefit of a slight improvement in credence if the evidence is non-misleading
is outweighed by the danger of a significant and discontinuous loss of value
due to loss of knowledge.

(We definitely won’t meet _r_ on a negative test result.) Thus to get (c)
definitely true, we need (3) to hold as well as the probability of a positive
test result to be at least _r_ \+ _α_ :

For suppose we have a credence threshold _r_ and that our intuitions agree
that we can’t have a situation where:

We thus have our joint constraint on the squishiness of our credences: bad
things happen if our credences are so precise as to make (6) true with respect
to a threshold _r_ for which we don’t want (a)–(c) to definitely hold. The
easiest scenario for making (a)–(c) definitely hold will be a binary test with
no false negatives.

Let _α_ > 0 be the “squishiness” of our credences. Let’s say that for one
credence to be definitely bigger than another, their difference has to be at
least _α_ , and that to definitely meet (fail to meet) a threshold, we must be
at least _α_ above (below) it. We assume that our threshold _r_ is definitely
less than one: _r_ \+ _α_ ≤ 1.

_P_ ( _p_ | _E_ ) = _P_ ( _p_ )/ _P_ ( _E_ ) = ( _r_ − _α_ )/( _r_ − _α_ + _β_
(1−( _r_ − _α_ ))).

But what’s a reasonable threshold for belief? Maybe something like 0.9 or
0.95. At _r_ = 0.9, the squishiness needed for paradox is _α_ = 0.046. I
suspect our credences are more precise than that. If we agree that the
squishiness of our credences is less than 4.6%, then we have an argument that
the threshold for belief is _more_ than 0.9. On the other hand, at _r_ = 0.95,
the squishiness needed for paradox is 2.4%. At this point, it becomes more
plausible that our credences lack that kind of precision, but it’s not clear.
At _r_ = 0.98, the squishiness needed for paradox dips below 1%. Depending on
how precise we think our credences are, we get an argument that the threshold
for belief is something like 0.95 or 0.98.

We thus have our joint constraint on the squishiness of our credences: bad
things happen if our credences have a level of precision equal to the right-
hand-side of (6). What exactly that says about _α_ depends on where the
relevant threshold lies. If the threshold _r_ is 1/2, the squishiness _α_ is
0.15. That’s surely higher than the actual squishiness of our credences. So if
we are concerned merely with the threshold being more-likely-than-not, then we
can’t avoid the paradox, because there will be cases where our credence is
definitely below the threshold and it’s definitely above the threshold that
examination of the evidence will push us about the threshold.

Let _z_ = _r_ − _α_ \+ _β_ (1−( _r_ − _α_ )) = (1− _β_ )( _r_ − _α_ ) + _β_.
This is the prior probability of a positive test result. We will definitely
meet _r_ on a positive test result just in case we have ( _r_ − _α_ )/ _z_ =
_P_ ( _p_ | _E_ ) ≥ _r_ \+ _α_ , i.e., just in case

Note that the squishiness of our credences likely varies with where the
credences lie on the line from 0 to 1, i.e., varies with respect to the
relevant threshold. For we _can_ tell the difference between 0.999 and 1.000,
but we probably can’t tell the difference between 0.700 and 0.701. So the
squishiness should probably be counted relative to the threshold. Or perhaps
it should be correlated to log-odds. But I need to get to looking at grad
admissions files now.

In [a recent post](http://alexanderpruss.blogspot.com/2023/01/knowing-you-
will-soon-have-enough.html), I noted that it is possible to cook up a Bayesian
setup where you don’t meet some threshold, say for belief or knowledge, with
respect to some proposition, but you do meet the same threshold with respect
to the claim that after you examine a piece of evidence, then you _will_ meet
the threshold. This is counterintuitive: it seems to imply that you can know
that you will have enough evidence to know something even though you don’t
yet. In a comment, Ian noted that one way out of this is to say that beliefs
do not correspond to sharp credences. It then occurred to me that one could
use the setup to probe the question of how sharp our credences are and what
the thresholds for things like belief and knowledge are, perhaps
complementarily to the considerations in [this
paper](https://philarchive.org/rec/PRUBSA).

What does this tell us about _r_ and _α_? We can actually figure this out.
Consider a test for _p_ that have no false negatives, but has a false positive
rate of _β_. Let _E_ be a positive test result. Our best bet to generating a
counterexample to (a)–(c) will be if the priors for _p_ are as close to _r_ as
possible while yet definitely below, i.e., if the priors for _p_ are _r_ −
_α_. For making the priors be that makes (c) easier to definitely satisfy
while keeping (b) definitely satisfied. Since there are no false negatives,
the posterior for _p_ will be:

It’s in fact not hard to see that (6) is necessary and sufficient for the
existence of a case where (a)–(c) definitely hold.

we meet _r_ with respect to the proposition that we will meet the threshold
with respect to _p_ after we examine evidence _E_.

Note that by appropriate choice of _β_ , we can make _z_ be anything between
_r_ − _α_ and 1, and the right-hand-side of (3) is at least _r_ − _α_ since
_r_ \+ _α_ ≤ 1. Thus we can make (c) definitely hold as long as the right-
hand-side of (3) is bigger than or equal to the right-hand-side of (4), i.e.,
if and only if:

That said, I think a qualification is plausible. Some wrongdoings are minor,
and in those cases the harm to the wrongdoer may be minor as well. But in any
case, to get someone to act against their conscience in a matter that
according to their conscience is major is to do them grave harm, a harm not
that different from death.

In cases where doing wrong and suffering wrong are of roughly the same order
of magnitude, it is very intuitive that we should prevent the suffering of
wrong rather than the doing of wrong. Imagine that Alice is drowning while at
the same time Bob is getting ready to assassinate a politician, but we know
for sure that Bob’s bullets have all been replaced with blanks. If our choice
is whether to try to dissuade Bob from attempting murder or keep Alice from
drowning, we should keep Alice from drowning, evne if on the Socratic view the
harm to Bob from attempting murder will be greater than that to Alice from
drowning. (I am assuming that in this case the two harms are nonetheless of
something like the same order of magnitude.)

One might think that what I said earlier implies that in this difficult case
the state should always allow people to follow their conscience, because after
all it is worse to do wrong—and violating conscience is wrong—than to have
wrong done to one. But that would be absurd and horrible—think of a racist
murderer whose faulty conscience requires them to kill.

The difficult case, however, is when people’s consciences are mistaken to such
a degree that conscience requires them to do something that unjustly harms
others. (A less problematic mistake is when conscience is mistaken to such a
degree that conscience requires them to do something that’s permissible, but
not wrong. In those cases, tolerance is clearly called for. We shouldn’t
pressure vegetarians to eat animals even if their conscientious objection to
eating animals happens to be mistaken.)

When someone’s conscience mistakenly requires something that violates an
objective moral rule, there is a two-fold benefit to that person from a law
incentivizing following the moral rule. The law is a teacher, and the state’s
disapproval may change one’s mind about the matter. And even if it a harm to
one to violate conscience, it is also a harm to one to do something wrong even
inculpably. Thus, the harm of violating conscience is somewhat offset by the
benefit from not doing something else that is wrong.

In some cases the person of mistaken conscience will still do the wrong deed
despite the law’s contrary incentive. In such a case, both the perpetrator and
the victim may be slightly better off for the law. The victim has a dignitary
benefit from the very fact that the state says that the harm was unlawful.
That dignitary benefit may be a cold comfort if the victim suffered a grave
harm, but it is still a benefit. And the perpetrator is slightly better off,
because following one’s conscience _against_ external pressure has an element
of admirability even when the conscience is mistaken.

Nonetheless, there will be cases where these considerations do not suffice,
and the law should be tolerant of mistaken conscience.

Now, acting against one’s conscience _is_ always wrong, and is almost always
_culpably_ wrong. For the most common case when doing something wrong isn’t
culpable is that one is ignorant of the wrongness, but when one acts against
one’s conscience one surely isn’t ignorant that one is acting against
conscience, and that we ought follow our conscience is obvious.

A reasonable optimism says that in most cases most people’s consciences are
correct. Thus typically we would expect that most violators of a legitimate
law will not be acting out of conscience—for a necessarily condition for the
legitimacy of a law is that it does not conflict with a correct conscience.
Thus, even if there is the rare murderer acting from mistaken conscience, most
murderers act against conscience, and by incentivizing abstention from murder,
in most cases the law _helps_ people follow their conscience, and the small
number of other cases can be tolerated as a side effect. Thus the
considerations of conscience favor intolerant laws in such cases. Nonetheless,
there are cases where most violators of a law would likely be acting from
conscience. Thus, if we had a law requiring eating meat, we would expect that
most of the violators would be conscientious. Similarly, a law against
something—say, the wearing of certain clothes or symbols—that is rarely done
except as a religious practice would likely be a law most violators of which
would be conscientious.

Now, the state, just like individuals, should _ceteris paribus_ avoid causing
grave harm. Hence, the state should generally avoid getting people to do
things that violate their conscience in major matters.

In a just defensive war, to refuse to fight to defend one’s fellow citizens
without special reason (perhaps priests and doctors should not kill) is wrong.
But a grave harm is done to a conscientious objector who is gotten to fight by
legal incentives. Let’s think through the five considerations above. The first
mainly applies to laws prohibiting a behavior rather than ones requiring a
behavior. Short of brainwashing, it is impossible to _make_ someone fight. (We
could superglue their hands to a gun, and then administer electric shocks
causing their fingers to spasm and fire a bullet, but that wouldn’t count as
_fighting_.) The second applies somewhat: we do need to weigh the harms to
innocent citizens from enemy invaders, harms that might be prevented if our
conscientious objector fought. But note that there is something rather
speculative about these harms. Someone who fights contrary to conscience is
unlikely to be a very effective fighter, and it is far from clear that their
military activity would actually prevent any actual harm to innocents. Now,
regarding the third consideration, one can design a conscription law with an
exemption that few who aren’t conscientious objectors would take advantage of.
One way to do this is to require evidence of one’s conscience’s objection to
fighting (e.g., prior membership in a pacifist organization). Another way is
to impose non-combat duties on conscientious objectors that are as onerous and
maybe as dangerous as combat would be. Regarding the fourth consideration, it
seems unlikely that a typical conscientious objector’s objections to war would
be changed by legal penalties. And the fifth seems a weak consideration in
general. Putting all these together, we do not outweigh the _prima facie_
considerations against pressuring conscientious objectors to act against their
(mistaken) conscience from the harms in going against conscience.

The harm of violating one’s conscience only happens to one if one _willingly_
violates one’s conscience. If law enforcement physically prevents me from
doing something that conscience requires from me, then I haven’t suffered the
harm. Thus, interestingly, the consideration I sketched against violating
one’s conscience does not apply when one is literally forced (fear of
punishment, unless it is severe enough to suspend one’s freedom of will, does
not actually _force_ , but only incentives).

One of the central insights of Western philosophy, beginning with Socrates,
has been that few if any things are as bad for an individual as culpably doing
wrong. It is better, we are told through much of the Western philosophical
tradition, that it is better to suffer than do injustice.

If the test comes out positive, I will have enough evidence to know that I
have _C_.

In other words, oddly enough, just prior to getting the test results I can
reasonably say:

Ian:  
  
I don't think the worries about sharpness of credences apply. For we can
suppose that the cases we apply the argument to are precisely ones with
sufficiently sharp credences. They might be cases concerning gambling
scenarios, or they could be medical cases where the only relevant data one has
is statistics from the most recent publications on the subject.  
  
I think all we really need for the argument is that in certain kinds of cases
meeting a credence threshold is what makes the difference between not having a
belief (or a justified belief) and having a belief (or a justified belief).  
  
By the way, suppose we think that the argument doesn't work for sharpness
reasons. Let's say that u is a level of sharpness such that it only makes
sense to say that you've met the threshold if you are at r+u or higher and
that you haven't met it if you are at r-u or lower, and if you're between r-u
and r+u it's indeterminate. Then working through the Bayesian details should
provide one with an interesting joint constraint on r and u. And then we can
ask see if it's reasonable to think that r and u actually satisfy the joint
constraint.

I don’t know that I have _C_ but I know that I will know.

As long as the test could be a true negative, you could always be surprised
and update to P of 0.0 if the test is negative. So waiting on the test before
you update is justified.

To see that (2) is true, note that given that the false negative rate is zero,
and the false positive rate is not close to one, I will indeed have non-
negligible evidence for _C_ if the test is positive.

If I am rational, my beliefs will follow the evidence. So if I am rational, in
a situation like the above, I will take myself to have a way of bringing it
about that I believe, and do so rationally, that I have _C_. Moreover, this
way of bringing it about that I believe that I have _C_ will itself be
perfectly rational if the test is free. For of course it’s rational to accept
free information. So I will be in a position where I am rationally able to
bring it about that I rationally believe _C_ , while not yet believing it.

I don’t yet have enough evidence to know that I have _C_ , but I know that in
a moment I will.

Suppose I am just the slightest bit short of the evidence needed for belief
that I have some condition _C_. I consider taking a test for _C_ that has a
zero false negative rate and a middling false positive rate—neither close to
zero nor close to one. On reasonable numerical interpretations of the previous
two sentences:

I’d take this as an argument against the view that belief is (or is justified
by) credence above a threshold.  
  
But that view seems implausible in any case. It’s pretty much only in gambling
setups with well-defined chances that we have sharp credences. But we all have
vast numbers of beliefs (taken in an informal ‘folk’ sense). So it seems that
some other approach is needed.  
  
A thorough Bayesian would be untroubled by this example. She would have her
credences, and that would be enough. She would not care whether they exceeded
some arbitrary threshold for belief. She would be similarly untroubled by the
lottery paradox. But no one in practice is a thorough Bayesian, or even close.
So again, it seems that some other approach is needed.

In fact, the same thing can be said about knowledge, assuming there is
knowledge in lottery situations. For suppose that I am just the slightest bit
short of the evidence needed for _knowledge_ that I have _C_. Then I can set
up the story such that:

If the test comes out positive, it will be another piece of evidence for the
hypothesis that I have _C_ , and it will push me over the edge to belief that
I have _C_.

To see that (1) is true, note that the test is certain to come out positive if
I have _C_ and has a significant probability of coming out positive even if I
don’t have _C_. Hence, the probability of a positive test result will be
significantly higher than the probability that I have _C_. But I am just the
slightest bit short of the evidence needed for belief that I have _C_ , so the
evidence that the test would be positive (let’s suppose a deterministic
setting, so we have no worries about the sense of the subjunctive conditional
here) is sufficient for belief.

I have enough evidence to believe that the test would come out positive.

Still, there is something odd about (5). It’s a bit like the line:

**Appendix:** Suppose the threshold for belief or knowledge is _r_ , where _r_
< 1. Suppose that the false-positive rate for the test is 1/2 and the false-
negative rate is zero. If _E_ is a positive test result, then _P_ ( _C_ | _E_
) = _P_ ( _C_ ) _P_ ( _E_ | _C_ )/ _P_ ( _E_ ) = _P_ ( _C_ )/ _P_ ( _E_ ) = 2
_P_ ( _C_ )/(1+ _P_ ( _C_ )). It follows by a bit of algebra that if my prior
_P_ ( _C_ ) is more than _r_ /(2− _r_ ), then _P_ ( _C_ | _E_ ) is above the
threshold _r_. Since _r_ < 1, we have _r_ /(2− _r_ ) < _r_ , and so the story
(either in the belief or knowledge form) works for the non-empty range of
priors strictly between _r_ /(2− _r_ ) and _r_.

To be clear, I don’t doubt that the example works. It does, and it illustrates
a genuine problem with the view that belief is (or is justified by) credence
above a threshold. Specifically, this violates an apparently natural
reflection principle, that if I rationally believe that I will rationally come
to believe, then I should believe now. Issues like this seem unavoidable with
a threshold (other than 1). The lottery paradox illustrates a different one:
violation of logical closure. Of course, you may be prepared to live with such
issues - maybe beliefs don’t have to be logically consistent.  
  
  
My problem with the threshold approach is more basic. If I don’t have a
credence, I can’t apply the threshold test. If I do have a credence, then
noting whether it is above or below a threshold adds nothing useful. One
response to this is straight Bayesianism, which rejects any role for belief
(beyond credence 1). Another is that credence and belief are separate and
related, but not straightforwardly. In the example, I would say that I
_believe_ the relevant medical information about the chance that I have C and
about the reliability of the test, and set my _credences_ accordingly.

But (6) is absurd: if I know that I will know something, then I am in a
position to know that the matter is so, since that I will know _p_ entails
that _p_ is true (assuming that _p_ doesn’t concern an open future). However,
there is no similar absurdity in (5). I may know that I will have enough
evidence to know _C_ , but that’s not the same as knowing that I will _know_
_C_ or even be in a position to know _C_. For it is possible to have enough
evidence to know something without being in a position to know it (namely,
when the thing isn’t true or when one is Gettiered).

I have enough evidence to _know_ that the test would come out positive,

Ian,  
  
I hadn't thought about this in the context of reflection. But technically
there is no counterexample here to van Fraassen's reflection principle, which
only says that P(H | credence at t will be p) = p. My examples satisfy that,
assuming full transparency about my own credences and full certainty that I
will conditionalize (cf.: https://jonathanweisberg.org/pdf/C_R_and_SKv2.SP.pdf
).  
  
The original reflection principle implies that my credence should be the
expected value of my future credences (at a fixed future time; we might also
generalize to any martingale stopping time). This is not a problem in my
cases, because although it is likely that my future credence will be bigger
than it is now, there is a small chance that my future credence will plummet
to zero due to a negative test result, as William notes.  
  
Upon further thought, I think it can be quite reasonable to say: "I know that
tomorrow I will have knowledge-level evidence for p, but I don't yet." And if
so, then my subsequent post on squishiness, even if technically correct, is
moot.

Trevor:  
  
I don't know why we need to say that the relation is related to the relata.
The relation relates the relata.

I have a problem with (2). (2) only follows if a complete explanation is
possible, which is kind of what you want to prove. So, it seems to me (2) begs
the question.  
If the 'water' is a brute fact, 'the earth floats on water' is as far as an
explanation can get. In a way it is merely partial but it isn't actually part
of something complete because there isn't something complete.  
That's why (1) is 'kind of right' but it isn't completely right.  
  
  
  

When you say grounding should not be thought of as a real relation do you mean
that the way grounding connects things and the way relations connects things
are different? Or do you mean that grounded things are not really distinct
from their grounds in extra mental reality? I apologize if I am
misunderstanding something, it just seems to me that grounding must be a real
relation in extra mental reality. For example God seems to ground our
existence in some way but we are really related to and therefore really
distinct from God. Also substances seem to ground their accidents but are
really distinct from and therefore really related to their accidents. But then
if grounding is in fact a real relation then the monism argument seems to
work.

A partial explanation is one that is a part of a complete explanation.

I was thinking of an argument I had seen by Della Rocca where he attempts to
show that relations cannot exist and therefore we should accept monism. If I
understand it correctly the idea is that relations depend on their relata for
their existence and therefore are grounded in one or more of their relata.
Then by asking what this grounding relation depends on it would start an
infinite regress of relations.

So, if any event _E_ has an explanation, it has an explanation going all the
way back to something self-explanatory. (1,2)

At least in some cases there seems to be no relation between x and y when x
grounds y. For in some cases of where x grounds y, there is nothing to y other
than x.  
  
Consider the fact that the moon is round or cubical. This is grounded in the
fact that the moon is round. But are there two facts here *and* a relation
between them? I doubt it.

I am not sure I buy (1). But it sounds kind of right to me now. Additionally,
(3) kind of sounds correct on its own. If _A_ causes _B_ and _B_ causes _C_
but there is no explanation of _A_ , then it seems that _B_ and _C_ are really
unexplained. Aristotle notes that there was a presocratic philosopher who
explained why the earth doesn’t fall down by saying that it floats on water,
and he notes that the philosopher failed to ask the same question about the
water. I think one lesson of Aristotle’s critique is that if it is unexplained
why the water doesn’t fall down it is unexplained why the earth falls down.

Any explanation for an event _E_ that does not go all the way back to
something self-explanatory is merely partial.

An explanation going back to something self-explanatory involves the activity
of a necessary being.

Dr Pruss, do you think that Bradley type regresses must also stop in some sort
of self explanatory fact? For example instead of having an infinite regress of
relations being related to their relata maybe it is somehow self explanatory
that the relation is related to its relata?

What am I? A herring-eater, a husband, a badminton player, a philosopher, a
father, a Canadian, a six-footer and a human are all correct answers. There is
an ancient form of the “What is _x_?” question where we are looking for a
“central” answer, and of course “a human” is usually taken to be that one.
What makes for that answer being central?

is not _generally_ logically valid. But some instances of a schema can be
logically valid even if the schema is not logically valid in general.

Necessarily, I am good and a virtuous human if and only if I am a good
virtuous human.

Necessarily, I am good and a human if and only if I am a good human.

Maybe, but can you say: What it is for you to be good is for you to be a good
man?  
  
Maybe the issue is that there is an ambiguity. A man (in the relevant sense)
is an adult male human. But "good adult male human" is a bit ambiguous: is one
good at the adulting, the maleness or the humanness, or at all three?  
  
I would say that what makes you be good is that you are good at being human,
and given that you are a man, this may have certain implications that it
wouldn't have if you weren't a man. (E.g., that you are a man makes it bad for
you to say "I am not a man".)

**Objection 2:** “Good” is attributive, and hence there is no such thing as
being good _simpliciter_.

In other words, that which I am centrally, really, deep-down and essentially
is that which sets the norms for me to be good _simpliciter_.

**Response:** I deny that it’s absurd. It’s just that all the electrons we
meet _are_ good at electronicity.

Andrew:  
  
Nice paper!  
  
But I think classification is not what I am getting at. I am aptly classified
as a male human, a human, a mammal, a vertebrate, an animal, an organism, and
an enmattered thing. However, I think only "human" is the answer to the
question I am after, the "what am I really" question. I am looking for a
particular "deep" classification. This is neither the most general
classification nor the most specific one, but the one that captures what most
deeply I am.  
  
On Athanasian theology, what makes me be saved is that the second person of
the Trinity became human just as I am human. He also became a male human just
as I am a male human and he became a vertebrate just as I am a vertebrate. But
these things do not save me. What saves me is his co-humanity, not his co-
vertebricity or co-animality or co-maleness. There is something deeper about
humanity than about any of these other things.  
  
This may be ethically important, too. Should I care more for a squid or a
mouse, if their intelligence is equal? The mouse is a fellow vertebrate, but
so what? It makes no difference. Similarly, that someone is a fellow male
matters not a whit. However, that someone is a fellow human, that seems to
matter.

_x_ is good and _x_ is _F_ if and only if _x_ is a good _F_

So our initial account of what is being asked for is that we are asking for an
attribute _F_ such that:

**Objection 1:** Some things are “centrally” electrons, but something’s being
good at electronicity is absurd.

But the same is not true for any other attribute besides “human” among those
of the first paragraph. I can be good and a badminton player while not being a
good badminton player, and I can be a good herring-eater without being good
and a herring-eater. And I have no idea what it is to be a good six-footer,
but perhaps the fact that I am a quarter of an inch short of six feet right
now makes me not be one. (In some cases one direction may hold. It may be that
if I am good and a human, then I am a good human.)

And yet “a virtuous human” would not be the answer to the ancient “What am I
centrally, really, deep-down, essentially?” question even if I were in fact a
virtuous human.

Sometimes the word “essential” is thrown in: I am _essentially_ human. But
what does that mean? In contemporary analytic jargon, it just means that I
cannot exist without being human. But the “central” answer to “What am I?” is
not just an answer that states a property that I cannot lack. There are, after
all, many such properties essential in such a way that do not answer the
question. “Someone conceived in 1972” and “Someone in a world where 2+2=4”
attribute properties I cannot lack, but are not the central answers.

Necessarily, if I am human, what it is for me to be good is for me to be a
good human.

So the sense of the “What am I centrally, really, deep-down, essentially?”
question isn’t just modal. What is it?

In other words, if I am a human, being a good human explains my being good. On
the other hand, even if I were a virtuous human, my being a good virtuous
human would not explain my being good. For redundancy is to be avoided in
explanation, and “good virtuous human” is redundant.

Necessarily, _x_ is a good _F_ if and only if _x_ is good.

_x_ is “centrally, really, deep-down, essentially” _F_ just in case what it is
for _x_ to be good is for _x_ to be a good _F_.

Another interpretation of the question of what we are is that it is asking for
*classification*. Generics can be useful here, because they (a) are apt
answers to classification questions and (b) still allow for exceptions. The
former feature makes them substantive. The latter makes them modest and immune
to certain styles of counterexample. For more on this (with respect to the
question of what *we* are, but the tools at play are widely applicable), see:
https://andrewmbailey.com/GenericAnimalism.pdf

On the "male human" point: it does seem like I can say "I am good and a man
iff I am a good man." But then (if we take the above approach) it seems like I
am centrally (really, deep-down) a man, not merely a human. This seems to sit
less comfortably with the Athanasian point. Perhaps this depends on the notion
of gender-specific norms? I'm not enough of a philosopher of gender to know
the literature on things like that.  
  

But perhaps we can do better. The necessary biconditional (2) holds in the
case “virtuous human”, but in a kind of trivial way: “a good virtuous human”
is repetition. I think that, as often, we need to pass from a modal to a
hyperintensional characterization. Consider that not only is (1) true, but
also:

Ian:  
  
I am somewhat inclined to say that a high credence just *is* what a belief is.  
  
THToD:  
  
Like you and Alston, I am inclined to agree that "belief" is at best an
awkward way to describe God's cognition. However, I think a major part of the
reason why God doesn't fundamentally have beliefs is that there is no room for
a gap in God between cognition and reality. Whether an open theist can have
the "no gap" view of God depends on the variety of open theism. If the open
theist is an open futurist, who thinks there are no facts in reality about
future contingents, then they can have the "no gap" view. But an open theist
like Swinburne or van Inwagen who thinks there are facts about future
contingents and God doesn't know them thinks there is a gap between God's
knowledge and reality, and now God is sounding very much like us -- there is
stuff that God is uncertain about.

A rational being believes anything that they take to have probability bigger
than 1 − (1/10100) given their evidence.

I suppose the best way out is for the open theist to deny (1).

Alex  
  
As Ian says, (1) applies to finite creatures, it doesn't apply to an
omniscient being. On open theism, God knows everything that can be known, but
libertarian free choices cannot be known in advance. If God were to "guess"
what I will do tomorrow, of course on open theism, he could turn out to be
wrong, but it would be a guess and not a belief.

I’d reject (1) in any case. Perfectly rational beings would have no use for
belief. They would have consistent credences (which could be 0 or 1) for
everything. On this view, beliefs are a shortcut that we, with our limited
mental capacity, use to reduce processing load.

Speaking of "God's beliefs" seems about as coherent as discussing what God ate
for lunch. He's not a finite creature.

Consider worlds created by God that contain four hundred people, each of whom
has an independent 1/2 chance of freely choosing to eat an orange tomorrow
(they love their oranges). Let _p_ be the proposition that at least one of
these 400 people will freely choose to eat an orange tomorrow. The chance of
not- _p_ in any such world will be (1/2)400 < 1/10100. Assuming open theism,
so God doesn’t just directly know whether _p_ is true or not, God will take
the probability of _p_ in any such world to be bigger than 1 − (1/10100) and
by (1) God will believe _p_ in these worlds. But in some of these worlds, that
belief will turn out to be false—no one will freely eat the orange. And this
violates (3).

These three theses, together with some auxiliary assumptions, yield a serious
problem for open theism.

But there is a problem here. For even if there is a “success value” in
accomplishing a self-set goal, the strength of the reasons for pursuing the
follow-through is also proportioned to facts independent of this exercise of
normative power. Rather, the reasons for pursuing the follow-through will
include the internal and external goods of victory (winning as such, prizes,
adulation, etc.), and these are independent of one’s setting follow-through as
one’s goal.

As far as I know, in all racquet sports players are told to follow-through: to
continue the racquet swing after the ball or shuttle have left the racquet.
But of course the ball or shuttle doesn’t care what the racquet is doing at
that point. So what’s the point of follow-through? The usual story is this: by
aiming to follow-through, one hits the ball or shuttle better. If one weren’t
trying to follow-through, the swing’s direction would be wrong or the swing
might slow down.

Yet it seems that whatever is intended is intended as a means or an end. One
might reject this principle, taking follow-through to be a counterexample.

Clearly the follow-through is _intended_ —it’s consciously planned, aimed at,
etc. But it need not be a means to anything one cares about in the game
(though, of course, in some cases it can be a means to impressing the
spectators or intimidating an opponent). But is it an end? It seems pointless
as an end!

This is interesting action-theoretically. The follow-through appears
pointless, because the agent’s interest is in what happens _before_ the
follow-through, the impact’s having the right physical properties, and yet
there is surely no backwards causation here. But there not appear to be an
effective way to reliably secure these physical properties of the impact
except by trying for the follow-through. So the follow-through itself is
pointless, but one’s _aiming at_ or _trying for_ the follow-through very much
has a point. And here the order of causality is respected: one swings aiming
at the follow-through, which causes an impact with the right physical
properties, and the swing then continues on to the “pointless” follow-through.

Maybe we should say this. Even if all intentional action is end-directed,
there are two kinds of reasons for an action: the reasons that come from the
value of the end and the reasons that come from the value of the pursuit of
that end. In the case of follow-through, there may be a fairly trivial success
value in the follow-through—a success value that comes from one’s exercise of
normative power in adopting the follow-through as one’s end—but that success
value provides only fairly trivial reasons. However, there can be
significantly non-trivial reasons for one’s pursuing that end, reasons
independent of that end.

Another move is this. We actually have a normative power to make something
_be_ an end. And then it becomes genuinely worth pursuing, because we have
adopted it as an end. So the player first exercises the normative power to
make follow-through be an end, and then pursues that end as an end.

I'm not sure that the creaturely changes are necessarily "destroying" (or
creating) parts of God (beliefs) on the open theists' assumption. I think they
are determining them. I will assume open theists who believe God is maximally
knowledgeable, that is, knows all that can be known, which does not include
perfect knowledge of the future according to them.  
  
On this set of assumptions God will still, however, have perfect foreknowledge
of all possible outcomes as he can understand all possible cause/effect
branches as First Cause. He just doesn't know which path along the branching
options will eventuate before the time that they do. This means his knowledge
grows only in the sense that he is able to pick out which of each fully
foreseen branch is actualised. In other words, the only way his knowledge
grows is by the steady lengthening of a "track" through a foreknown "space" of
possible world-states over time. In such a scenario, creatures with free will
would determine the direction of the track and thus the precise determination
of God’s knowledge of it, but they would not be adding to or subtracting from
God's beliefs in any quantitative sense. The addition that would occur comes
about automatically as time flows, no matter what finite agents decide.  
  
Please note, I do not hold this position, but it seems to me it is not guilty
of the exact absurdity alleged here, whatever its other flaws.  

How? Easy. I am now sitting, and God knows that. So, a part of God is the
belief that I am sitting. But I can destroy that belief of God’s by standing
up! For as soon as I stand up, the belief that I am sitting will no longer
exist. But on the view in question, God’s beliefs are parts of him. So by
standing up, I would bring it about that a part of God doesn’t exist.

Alex  
  
How does it follow from the idea that God is not simple that God’s beliefs
change as the reality they are about changes?  
  
If (2) is true, isn't that a problem for divine simplicity as well? For on
divine simplicity, God id identical to God's beliefs, so if (2) is true it
seems that God changes as the reality changes, which conradicts divine
simplicity as well as divine immutability.

I agree that one could hold 1 without 2. But my argument is against people who
hold the package of 1 and 2. Think here of process theologians, and their more
orthodox cousins.

And of course by standing up, I bring it about that a new divine belief
exists. So:

I agree with Walter. It seems to me that 2 is more directly incompatible with
God's atemporality than with his lack of complexity as such. An atemporal God
who sees Creation across its history in toto as a block, whether He is simple
or complex, has only true beliefs about everything that has been or will be,
and about when each fact will be true for finite creatures who are temporal.
But his beliefs don't change.  
  
Although a God who changed with and experienced time such that his beliefs
also changed might be thus considered complex across time by having temporal
parts, that would seem to be assuming an A-theory of time for all (including
God), then importing a B perspective at the end, incoherently. So, simplicity
of essence (at any point in time) and atemporality could be distinguished I
suppose, such that a person could affirm both simplicity and temporality of
God. I'm not saying that any open theist holds this opinion, or should, but it
seems a logical possibility from within that framework.

God is not simple, and in particular God’s beliefs are proper parts of God.

It depends on the details of how they think beliefs work. But suppose they
hold to this picture: for each proposition p that God knows, there is God's
act of believing p. Moreover, these opponents of simplicity think that God's
act of believing p is a different act for each different p that God believes.
These different acts are on that view something like accidents of God. Thus,
by raising one's arm one brings it about that God's belief that one's arm is
in a lowered state does not exist and that God's belief that one's arm is in a
raised state does exist, and so one destroys a part of God and creates a new
one. (It is a part of the package that I am criticizing that propositions are
tensed, and so it's not enough for God to eternally know that at t1 the arm is
lowered and at t2 the arm is raised.)

I think one can hold that God's beliefs are accidents of his which are not
identical to him (rejecting a strong doctrine of divine simplicity) without
holding that they are "parts" of him in any sense that makes (3) or (4)
absurd. Even for created beings like us, the accident-substance relationship
does not appear to be at all the same as the part-whole relationship as common
sense conceives it. (I would find it weird to say that my beliefs are a part
of me in the same sense that my arm is a part of my body, for instance.)

There is, however, a difference between the cases. Many great ordinary goods
that dwarf trivial evils, like the courage of a Socrates, are known to us. Few
if any finite goods that dwarf horrendous evils are known to us. Nonetheless,
if theism is true, it is very likely that such goods are possible. And since
the argument from evil is addressed against the theist, it seems fair for the
theist to invoke that hierarchy.

Perhaps not negatively, but certainly 'without love'. It seems to me that God
did not have to create you and thus God did not have to love you. If God were
forced to love you or loved you randomly, I wouldn't consider God to be a
moral being. I also wouldn't consider love to be a moral good if it were
forced. Therefore, it seems possible that evil actions are merely privations
of good actions, and thus one cannot choose to do "nothing" because "nothing"
is a lack of good, which seems to be some sort of evil. Perhaps one could make
a distinction between evils, but it seems that this might be a better
distinction that the one I previously drew.

The reason why few people seriously run an argument from trivial evils is
simply because the case is much more obvious in the case of horrendous evil.  
But a truly opnipotent being should be able to accomplish Evert good without
any evil, unless this is logically impossible  
So, unless there is a convincing argument for why a certain good cannot
posdibly be accomplished without permitting evil, the default position should
be that it is possible.  

Moreover, we might ask whether our ignorance of goods higher up in the
hierarchy of goods beyond the ordinary goods is not itself evidence against
the existence of such goods. Here, I think the answer is that it is very
little evidence. We would expect any particular finite being to be able to
recognize only a finite number of types of good, and thus the fact that there
are only a finite number of goods that we recognize is very little evidence
against the hypothesis of the upward hierarchy of goods.

There seems to be a legitimate reason to allow evil choices to exist and to
allow those evil choices to affect other people. Seemingly, actions wouldn't
mean much (such as freely loving God) if we were all in cubicles and couldn't
actually interact with people negatively. Free love and free hatred seems to
be two sides of the same coin. I would also say that there are legitimate
reasons for suffering to exist. Perhaps the world would have less good in it
if there is no suffering. Would God permit suffering to allow more good? I
don't see a reason as to why He wouldn't. Theodicies and defenses are very
interesting, though it seems to me that, intuitively at least, most theodicies
seem to deal with the problem of trivial evils.

So if God exists, we would expect there to be unknown possible finite goods
that are related to the known horrendous evils in something like the
proportion in which the known great finite goods are related to the known
trivial evils. Thus, if God exists, there very likely is  
an upward hierarchy of possible goods to which the horrendous evils of this
life stand like a mosquito bite to the courage of a Socrates. If we believe in
this hierarchy of goods, then it seems we should be no more impressed by the
atheological evidential force of horrendous evils than the ordinary person is
by the atheological evidential force of trivial evils.

Nobody seriously runs an argument against the existence of God from _trivial_
evils, like hangnails or mosquito bites.

Unknown  
  
So, God's actions do not mean much if He can't interact with people
negatively?

On the other hand, if we think of _horrendous_ evils, like the torture of
children, it is difficult to think of much greater goods. Maybe with
difficulty we can come up with one or two possibilities, but not enough to
make it _easy_ to suppose a justification for God’s permission of the evil in
terms of the goods.

However, if God exists, we would _expect_ there to be an unbounded upward
qualitative hierarchy of possible finite goods. God is infinitely good, and
finite goods are participations in God, so we would expect a hierarchy of
qualitatively greater and greater types of good that reflect God’s infinite
goodness better and better.

Unknown  
  
I wouldn't say "forced" is the correct word here, but it is true that God
_necessarily_ loves you. So, in a sense, God _did/does_ have to love you.

Why not? Here is a hypothesis. There are so very many possible much greater
goods—goods qualitatively and not just quantitatively much greter—that it
would be easy to suppose that God’s permitting the trivial evil could promote
or enhance one of these goods to a degree sufficient to yield justification.

